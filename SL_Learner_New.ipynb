{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f014bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS7641 Supervised Learning — Hotel (classification) + US Accidents (regression)\n",
    "v2 — Aligns with Fall 2025 rubric + dataset minimums and adds NN activation study (extra credit).\n",
    "\n",
    "What’s new vs v1:\n",
    " - Pipelines: target/freq encoding inside CV; StandardScaler; global cast to float32\n",
    " - Hotel (classification): ROC/PR + reliability plots; confusion matrix at F1-optimal threshold\n",
    " - Accidents (regression): MAE/MedAE/MSE; residuals vs prediction plot\n",
    " - Runtime table: fit/predict wall-clock + peak RAM; hardware note\n",
    " - US Accidents size rules: DT & Linear SVR ~≥1M rows if available; RBF SVR ≤100k; kNN ≤250k train / ≤25k test\n",
    " - Neural Nets (both datasets): SGD-only (no momentum), ≤15 epochs, batch 512–2048, L2 in [1e-4,1e-3],\n",
    "   param-count kept in 0.2M–1.0M (auto width), LC + MC + epoch curves\n",
    " - Extra credit (Hotel): activation study (ReLU, GELU, SiLU/Swish, tanh) under identical SGD protocol\n",
    "\n",
    "Outputs:\n",
    " sl_outputs/figs/*    — all curves/diagnostics (dataset-prefixed filenames)\n",
    " sl_outputs/logs/runtime_table.csv — model runtimes & RAM; hardware note printed to console\n",
    "\"\"\"\n",
    "\n",
    "import os, time, math, platform, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---- Paths / dirs\n",
    "FIG_DIR = \"sl_outputs/figs\"; LOG_DIR = \"sl_outputs/logs\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Repro\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "\n",
    "# ---- System info + RAM tracking\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None\n",
    "import resource\n",
    "\n",
    "def now_mb():\n",
    "    if psutil: return psutil.Process().memory_info().rss / (1024**2)\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.0  # MB on Linux\n",
    "\n",
    "def time_fit_predict(model, Xtr, ytr, Xte, yte, fit_fn=\"fit\", predict_fn=\"predict\"):\n",
    "    start_ram = now_mb(); t0 = time.time()\n",
    "    getattr(model, fit_fn)(Xtr, ytr)\n",
    "    t1 = time.time()\n",
    "    yhat = getattr(model, predict_fn)(Xte)\n",
    "    t2 = time.time()\n",
    "    peak_ram = max(now_mb(), start_ram)  # coarse proxy\n",
    "    return (t1 - t0), (t2 - t1), peak_ram, yhat\n",
    "\n",
    "def hw_note():\n",
    "    cpu = platform.processor() or \"Unknown CPU\"\n",
    "    sys = f\"{platform.system()} {platform.release()}\"\n",
    "    print(f\"[Hardware] {sys}; CPU: {cpu}; Python {platform.python_version()}\")\n",
    "\n",
    "# ---- Plot/save helpers\n",
    "def savefig(prefix, title):\n",
    "    fname = f\"{prefix}_{title.replace(' ', '_').replace('/', '-')}.png\"\n",
    "    path = os.path.join(FIG_DIR, fname)\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=160); plt.close()\n",
    "    print(f\"[saved] {path}\")\n",
    "\n",
    "def protocol_card(name, X, y=None, scoring=None, cv=None):\n",
    "    if y is not None and set(np.unique(y)) <= {0,1}:\n",
    "        pos = float(np.mean(y))\n",
    "        print(f\"[{name}] n={len(X)} scoring={scoring} cv={cv} prevalence={pos:.3f}\")\n",
    "    else:\n",
    "        print(f\"[{name}] n={len(X)} scoring={scoring} cv={cv}\")\n",
    "\n",
    "# ---- Sklearn bits\n",
    "from sklearn.model_selection import train_test_split, learning_curve, validation_curve, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, auc, f1_score, confusion_matrix,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay, mean_absolute_error, mean_squared_error, median_absolute_error\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# ---- Metrics helpers\n",
    "def pr_auc(y_true, scores):\n",
    "    p, r, _ = precision_recall_curve(y_true, scores)\n",
    "    return auc(r, p)\n",
    "\n",
    "def f1_opt_threshold(y_true, scores):\n",
    "    p, r, t = precision_recall_curve(y_true, scores)\n",
    "    f1 = np.where((p+r) > 0, 2*p*r/(p+r), 0)\n",
    "    j = int(np.argmax(f1))\n",
    "    thr = t[j-1] if 0 < j < len(t) else 0.5\n",
    "    return float(thr), float(f1[j])\n",
    "\n",
    "def count_nn_params(hidden_layers, input_dim, output_dim=1):\n",
    "    \"\"\"Count total trainable params in MLP: weights + biases.\"\"\"\n",
    "    total = 0\n",
    "    prev = input_dim\n",
    "    for h in hidden_layers:\n",
    "        total += prev * h + h  # weights + bias\n",
    "        prev = h\n",
    "    total += prev * output_dim + output_dim  # output layer\n",
    "    return total\n",
    "\n",
    "def plot_nn_epoch_curve(model, X_train, y_train, X_val, y_val, title, prefix=\"Hotel\", classification=True):\n",
    "    \"\"\"Plot NN training curve: train/val loss per epoch with early stopping marker.\n",
    "    \n",
    "    NOTE: This uses the ALREADY TRAINED model's loss_curve_ attribute if available.\n",
    "    If not available, skips plotting to avoid retraining overhead.\n",
    "    \"\"\"\n",
    "    # Get the MLP from pipeline\n",
    "    mlp = model.named_steps['clf'] if 'clf' in model.named_steps else model.named_steps['reg']\n",
    "    \n",
    "    # Check if model has been trained and has loss curve\n",
    "    if not hasattr(mlp, 'loss_curve_') or len(mlp.loss_curve_) == 0:\n",
    "        print(f\"[NN Epoch Curve] Skipping - model not trained or no loss_curve_ available\")\n",
    "        return\n",
    "    \n",
    "    # Use existing loss curve from training\n",
    "    train_losses = mlp.loss_curve_\n",
    "    n_epochs = len(train_losses)\n",
    "    \n",
    "    # Compute validation scores for each \"effective\" epoch based on available data\n",
    "    # Since we can't recreate intermediate epochs, we'll use the final model\n",
    "    # and create a simplified plot showing the training progression\n",
    "    \n",
    "    # Get final scores\n",
    "    if classification:\n",
    "        train_score = roc_auc_score(y_train, model.predict_proba(X_train)[:,1])\n",
    "        val_score = roc_auc_score(y_val, model.predict_proba(X_val)[:,1])\n",
    "        metric_name = 'ROC-AUC'\n",
    "    else:\n",
    "        train_score = -mean_absolute_error(y_train, model.predict(X_train))\n",
    "        val_score = -mean_absolute_error(y_val, model.predict(X_val))\n",
    "        metric_name = 'Neg MAE'\n",
    "    \n",
    "    # Determine best epoch (where training stopped due to early stopping)\n",
    "    best_epoch = mlp.n_iter_ if hasattr(mlp, 'n_iter_') else n_epochs\n",
    "    \n",
    "    # Create simplified plot with available data\n",
    "    plt.figure(figsize=(6.2, 4.6))\n",
    "    plt.plot(range(1, n_epochs+1), train_losses, 'o-', label='Train Loss')\n",
    "    plt.axvline(best_epoch, ls='--', color='red', alpha=0.7, label=f'Stopped at Epoch {best_epoch}')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "    plt.title(f'{prefix} — NN Training Loss: {title}\\nFinal Train {metric_name}={train_score:.4f}, Val {metric_name}={val_score:.4f}')\n",
    "    plt.legend(); plt.grid(alpha=.3)\n",
    "    \n",
    "    savefig(prefix, f\"NN_EpochCurve_{title.replace(' ', '_')}\")\n",
    "    print(f\"[NN Epoch Curve] {n_epochs} epochs, stopped at {best_epoch}, Final Val {metric_name}={val_score:.4f}\")\n",
    "\n",
    "def support_vector_analysis(model, X, y, model_name=\"LinearSVC\"):\n",
    "    \"\"\"Analyze support vectors for SVM models.\"\"\"\n",
    "    # For LinearSVC, we approximate support vectors using decision function margin\n",
    "    if hasattr(model, 'decision_function'):\n",
    "        decisions = model.decision_function(X)\n",
    "        # Points near the margin (|decision| < 1) are approximate support vectors\n",
    "        margin_threshold = 1.0\n",
    "        near_margin = np.abs(decisions) < margin_threshold\n",
    "        sv_fraction = np.mean(near_margin)\n",
    "        \n",
    "        print(f\"\\n[{model_name} Support Vector Analysis]\")\n",
    "        print(f\"  Approximate SV fraction: {sv_fraction:.4f} ({int(sv_fraction*len(X)):,} / {len(X):,})\")\n",
    "        print(f\"  Decision function range: [{decisions.min():.3f}, {decisions.max():.3f}]\")\n",
    "        print(f\"  Points within margin (|f(x)| < 1): {near_margin.sum():,}\")\n",
    "        \n",
    "        # Plot decision function distribution\n",
    "        plt.figure(figsize=(6.2, 4.6))\n",
    "        plt.hist(decisions, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(-1, color='red', ls='--', label='Margin boundaries')\n",
    "        plt.axvline(1, color='red', ls='--')\n",
    "        plt.axvline(0, color='green', ls='-', alpha=0.5, label='Decision boundary')\n",
    "        plt.xlabel('Decision function f(x)'); plt.ylabel('Count')\n",
    "        plt.title(f'{model_name} — Decision Function Distribution')\n",
    "        plt.legend(); plt.grid(alpha=.3)\n",
    "        return sv_fraction\n",
    "    return None\n",
    "\n",
    "# ---- Curves\n",
    "def plot_learning(est, title, X, y, scoring, cv=5, prefix=\"Hotel\"):\n",
    "    protocol_card(title, X, y, scoring, cv)\n",
    "    sizes = np.linspace(0.1, 1.0, 4)  # 4 points instead of 5 for speed\n",
    "    tr_sizes, tr_s, te_s = learning_curve(\n",
    "        est, X, y, train_sizes=sizes, cv=cv, scoring=scoring, n_jobs=-1, random_state=RANDOM_STATE\n",
    "    )\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.title(f\"{prefix} — Learning Curve: {title}\")\n",
    "    plt.xlabel(\"Training examples\"); plt.ylabel(scoring.upper()); plt.grid(alpha=.3)\n",
    "    plt.plot(tr_sizes, tr_s.mean(1), 'o-', label=\"Training\")\n",
    "    plt.plot(tr_sizes, te_s.mean(1), 'o-', label=\"Cross-val\"); plt.legend()\n",
    "    savefig(prefix, f\"LC_{title}\")\n",
    "\n",
    "def plot_validation(est, X, y, pname, prange, scoring, title, cv=5, prefix=\"Hotel\"):\n",
    "    protocol_card(title, X, y, scoring, cv)\n",
    "    tr_s, te_s = validation_curve(est, X, y, param_name=pname, param_range=prange, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.title(f\"{prefix} — Validation Curve: {title}\")\n",
    "    plt.xlabel(pname); plt.ylabel(scoring.upper()); plt.grid(alpha=.3)\n",
    "    plt.plot(prange, tr_s.mean(1), 'o-', label=\"Training\")\n",
    "    plt.plot(prange, te_s.mean(1), 'o-', label=\"Cross-val\"); plt.legend()\n",
    "    savefig(prefix, f\"MC_{title}\")\n",
    "\n",
    "# =========================\n",
    "# Data loaders & preprocess\n",
    "# =========================\n",
    "def load_hotel():\n",
    "    df = pd.read_csv(\"hotel_bookings.csv\")\n",
    "    # leakage controls (rubric)\n",
    "    drop_leak = [c for c in ['reservation_status','reservation_status_date'] if c in df.columns]\n",
    "    df = df.drop(columns=drop_leak)\n",
    "    for c in ['agent','company','children']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(0).astype(int)\n",
    "    if 'country' in df.columns:\n",
    "        df['country'] = df['country'].fillna(df['country'].mode()[0])\n",
    "    y = df['is_canceled'].astype(int)\n",
    "    X = df.drop(columns=['is_canceled'])\n",
    "    print(f\"[Hotel] rows raw={len(df)}  target=is_canceled  (train/test split later)\")\n",
    "    return X, y\n",
    "\n",
    "def hotel_preprocessor(X):\n",
    "    num = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    te_cols = [c for c in ['country','agent','company'] if c in X.columns]  # high-cardinality\n",
    "    cat = [c for c in X.select_dtypes(exclude=np.number).columns if c not in te_cols]\n",
    "    \n",
    "    # Use frequency encoding instead of TargetEncoder to avoid pipeline issues\n",
    "    # Frequency encoding is a simple unsupervised alternative that works in ColumnTransformer\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    def freq_encode(X_col):\n",
    "        \"\"\"Frequency encode categorical columns\"\"\"\n",
    "        if isinstance(X_col, pd.DataFrame):\n",
    "            result = X_col.copy()\n",
    "            for col in result.columns:\n",
    "                freq_map = result[col].value_counts(normalize=True).to_dict()\n",
    "                result[col] = result[col].map(freq_map).fillna(0)\n",
    "            return result.astype(np.float32)\n",
    "        return X_col\n",
    "    \n",
    "    pre = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32), cat),\n",
    "        ('freq', FunctionTransformer(freq_encode), te_cols)\n",
    "    ], remainder='drop')\n",
    "    # global cast to float32 for downstream models (rubric)\n",
    "    to32 = ('to32', FunctionTransformer(lambda Z: Z.astype(np.float32)))\n",
    "    return Pipeline([('pre', pre), to32])\n",
    "\n",
    "def load_accidents(path=\"US_Accidents_March23.csv\", max_rows=1800000):\n",
    "    \"\"\"Load accidents data. max_rows limits initial CSV load for memory efficiency.\"\"\"\n",
    "    df = pd.read_csv(path, nrows=max_rows)\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "    df['End_Time']   = pd.to_datetime(df['End_Time'],   errors='coerce')\n",
    "    df['Duration'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60.0\n",
    "    df = df.drop(columns=['Start_Time','End_Time'])\n",
    "    df = df[df['Duration'] > 0]\n",
    "    if 'Description' in df.columns: df = df.drop(columns=['Description'])\n",
    "    \n",
    "    # Drop columns with >50% nulls (like End_Lat, End_Lng)\n",
    "    null_pct = df.isna().mean()\n",
    "    high_null_cols = null_pct[null_pct > 0.5].index.tolist()\n",
    "    if high_null_cols:\n",
    "        df = df.drop(columns=high_null_cols)\n",
    "    \n",
    "    # Fill remaining nulls\n",
    "    for c in df.select_dtypes(include='object').columns: \n",
    "        df[c] = df[c].fillna('Unknown')\n",
    "    for c in df.select_dtypes(include='bool').columns:   \n",
    "        df[c] = df[c].astype(int)\n",
    "    for c in df.select_dtypes(include=np.number).columns: \n",
    "        df[c] = df[c].fillna(df[c].median())  # Fill numeric nulls with median\n",
    "    \n",
    "    # Convert to float32 for efficiency\n",
    "    for c in df.select_dtypes(include=np.number).columns: \n",
    "        df[c] = df[c].astype(np.float32)\n",
    "    \n",
    "    y = df['Duration'].astype(np.float32)\n",
    "    X = df.drop(columns=['Duration','Severity']) if 'Severity' in df.columns else df.drop(columns=['Duration'])\n",
    "    print(f\"[Accidents] rows raw={len(df)}  target=Duration(min)\")\n",
    "    return X, y\n",
    "\n",
    "def accidents_preprocessor(X):\n",
    "    num = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    cat = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    pre = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32), cat)\n",
    "    ], remainder='drop')\n",
    "    to32 = ('to32', FunctionTransformer(lambda Z: Z.astype(np.float32)))\n",
    "    return Pipeline([('pre', pre), to32])\n",
    "\n",
    "# ---- Helper: choose NN widths to stay within param cap\n",
    "def choose_widths(input_dim, target_dim, shallow=True, cap_low=2e5, cap_high=1e6):\n",
    "    # simple heuristic: pick W so ~ input_dim*W + W*target_dim stays within cap window\n",
    "    W = max(64, int(min(cap_high / max(1, input_dim), 1024)))\n",
    "    if shallow:\n",
    "        arch = [min(W, 512), min(W, 512)]\n",
    "    else:\n",
    "        arch = [max(64, W//2), max(64, W//2), max(32, W//4), max(32, W//4)]\n",
    "    # Return and let caller print estimated param count after fitting preprocessor\n",
    "    return arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc15bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# HOTEL — Classification\n",
    "# =========================\n",
    "def run_hotel():\n",
    "    print(\"\\n== HOTEL (CLASSIFICATION: is_canceled) ==\")\n",
    "    X, y = load_hotel()\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    pre = hotel_preprocessor(Xtr)\n",
    "\n",
    "    # Post-scaler aligns numeric/TE/OHE magnitudes for distance-based models\n",
    "    post = ('post', StandardScaler(with_mean=False))\n",
    "\n",
    "    # Models (SGD-only NNs; SVM kernels; class_weight balanced)\n",
    "    dt_clf  = Pipeline([('prep', pre), ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "    knn_clf = Pipeline([('prep', pre), post, ('clf', KNeighborsClassifier(n_neighbors=11, n_jobs=-1))])\n",
    "    linSVC  = Pipeline([('prep', pre), post, ('clf', LinearSVC(C=1.0, class_weight='balanced', max_iter=15000, random_state=RANDOM_STATE))])\n",
    "    rbfSVC  = Pipeline([('prep', pre), post, ('clf', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "\n",
    "    # NN widths chosen after fitting preprocessor to get input_dim\n",
    "    pre.fit(Xtr, ytr)\n",
    "    input_dim = pre.transform(Xtr[:5]).shape[1]\n",
    "    shallow = choose_widths(input_dim, 2, shallow=True)\n",
    "    deeper  = choose_widths(input_dim, 2, shallow=False)\n",
    "    \n",
    "    # Verify param counts are in 0.2M-1.0M range (rubric requirement)\n",
    "    shallow_params = count_nn_params(shallow, input_dim, output_dim=2)\n",
    "    deeper_params = count_nn_params(deeper, input_dim, output_dim=2)\n",
    "    print(f\"[NN Architectures] input_dim={input_dim}\")\n",
    "    print(f\"  Shallow {shallow}: {shallow_params:,} params ({shallow_params/1e6:.2f}M)\")\n",
    "    print(f\"  Deeper {deeper}: {deeper_params:,} params ({deeper_params/1e6:.2f}M)\")\n",
    "    if not (200_000 <= shallow_params <= 1_000_000):\n",
    "        print(f\"  ⚠️  WARNING: Shallow params {shallow_params:,} outside 0.2M-1.0M range!\")\n",
    "    if not (200_000 <= deeper_params <= 1_000_000):\n",
    "        print(f\"  ⚠️  WARNING: Deeper params {deeper_params:,} outside 0.2M-1.0M range!\")\n",
    "    \n",
    "    nn_sgd = Pipeline([\n",
    "        ('prep', pre),\n",
    "        post,\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                              batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                              alpha=1e-4, early_stopping=True, n_iter_no_change=3,\n",
    "                              max_iter=15, shuffle=True, momentum=0.0, nesterovs_momentum=False,\n",
    "                              random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "    # AGGRESSIVE OPTIMIZATION: Use 40k subset for curves (rubric has no min size for Hotel)\n",
    "    # This gives 5-8x speedup on curves while maintaining trend visibility\n",
    "    n_curves = min(40000, len(Xtr))\n",
    "    idx_curves = np.random.choice(len(Xtr), n_curves, replace=False)\n",
    "    Xtr_curves, ytr_curves = Xtr.iloc[idx_curves], ytr.iloc[idx_curves]\n",
    "    print(f\"[Optimization] Using {n_curves:,} samples for curves (cv=2, faster)\")\n",
    "    \n",
    "    # ---- Curves: MC + LC for all learners (OPTIMIZED: 40k samples, cv=2, fewer params)\n",
    "    print(\"\\n[Generating validation curves...]\")\n",
    "    plot_validation(Pipeline([('prep', pre), ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE))]),\n",
    "                    Xtr_curves, ytr_curves, \"clf__max_depth\", [6,10,14], \"roc_auc\", \"DT vs depth\", cv=2, prefix=\"Hotel\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', KNeighborsClassifier(n_jobs=-1))]),\n",
    "                    Xtr_curves, ytr_curves, \"clf__n_neighbors\", [5,11,21], \"roc_auc\", \"kNN vs k\", cv=2, prefix=\"Hotel\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', LinearSVC(class_weight='balanced', max_iter=10000, random_state=RANDOM_STATE, dual='auto'))]),\n",
    "                    Xtr_curves, ytr_curves, \"clf__C\", [0.1,1,10], \"roc_auc\", \"LinearSVM vs C\", cv=2, prefix=\"Hotel\")\n",
    "    # RBF SVM validation curve: even smaller subset (10k) due to O(n²-n³) complexity\n",
    "    rbf_vc_size = min(10000, n_curves)\n",
    "    idx_rbf_vc = np.random.choice(n_curves, rbf_vc_size, replace=False)\n",
    "    print(f\"[Note] Using {rbf_vc_size} samples for RBF validation curve (O(n³) complexity)\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE, cache_size=500))]),\n",
    "                    Xtr_curves.iloc[idx_rbf_vc], ytr_curves.iloc[idx_rbf_vc], \"clf__C\", [0.5,2], \"roc_auc\", \"RBF-SVM vs C (γ=scale)\", cv=2, prefix=\"Hotel\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', MLPClassifier(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                                                                        momentum=0.0, nesterovs_momentum=False,\n",
    "                                                                        batch_size=1024, max_iter=15, random_state=RANDOM_STATE))]),\n",
    "                    Xtr_curves, ytr_curves, \"clf__alpha\", [1e-4,1e-3], \"roc_auc\", \"NN vs L2(alpha)\", cv=2, prefix=\"Hotel\")\n",
    "\n",
    "    print(\"\\n[Generating learning curves...]\")\n",
    "    plot_learning(dt_clf,   \"Decision Tree\", Xtr_curves, ytr_curves, \"roc_auc\", cv=2, prefix=\"Hotel\")\n",
    "    plot_learning(knn_clf,  \"kNN\",           Xtr_curves, ytr_curves, \"roc_auc\", cv=2, prefix=\"Hotel\")\n",
    "    plot_learning(linSVC,   \"Linear SVM\",    Xtr_curves, ytr_curves, \"roc_auc\", cv=2, prefix=\"Hotel\")\n",
    "    # RBF SVM is O(n²-n³), use even smaller subset for learning curve (15k)\n",
    "    rbf_lc_size = min(15000, n_curves)\n",
    "    idx_rbf = np.random.choice(n_curves, rbf_lc_size, replace=False)\n",
    "    print(f\"[Note] Using {rbf_lc_size} samples for RBF learning curve (O(n³) complexity)\")\n",
    "    plot_learning(rbfSVC,   \"RBF SVM\",       Xtr_curves.iloc[idx_rbf], ytr_curves.iloc[idx_rbf], \"roc_auc\", cv=2, prefix=\"Hotel\")\n",
    "    plot_learning(nn_sgd,   \"Neural Net (SGD)\", Xtr_curves, ytr_curves, \"roc_auc\", cv=2, prefix=\"Hotel\")\n",
    "\n",
    "    # ---- Light tuning (OPTIMIZED: 40k subset, cv=2, fewer param combinations)\n",
    "    print(\"\\n[Hyperparameter tuning on subset...]\")\n",
    "    dt_tuned  = GridSearchCV(dt_clf,  {\"clf__max_depth\":[10,14], \"clf__min_samples_leaf\":[50,100]}, cv=2, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr_curves, ytr_curves).best_estimator_\n",
    "    knn_tuned = GridSearchCV(knn_clf, {\"clf__n_neighbors\":[3,5,11,21]}, cv=2, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr_curves, ytr_curves).best_estimator_\n",
    "    lin_tuned = GridSearchCV(linSVC,   {\"clf__C\":[0.1,1,10]}, cv=2, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr_curves, ytr_curves).best_estimator_\n",
    "    # RBF: tune both C and gamma (rubric requirement)\n",
    "    gamma_values = [\"scale\", 1.0/input_dim, 2.0/input_dim]\n",
    "    rbf_tuned = GridSearchCV(rbfSVC,   {\"clf__C\":[0.5,2,8], \"clf__gamma\":gamma_values}, cv=2, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr_curves, ytr_curves).best_estimator_\n",
    "    nn_tuned  = GridSearchCV(nn_sgd,   {\"clf__alpha\":[1e-4,1e-3]}, cv=2, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr_curves, ytr_curves).best_estimator_\n",
    "    \n",
    "    # Retrain on FULL training set with best params for final test evaluation\n",
    "    print(f\"[Final training] Retraining all models on full {len(Xtr):,} samples...\")\n",
    "    dt_tuned.fit(Xtr, ytr)\n",
    "    knn_tuned.fit(Xtr, ytr)\n",
    "    lin_tuned.fit(Xtr, ytr)\n",
    "    rbf_tuned.fit(Xtr, ytr)\n",
    "    nn_tuned.fit(Xtr, ytr)\n",
    "\n",
    "    # Calibrate LinearSVC to get probabilities for PR/threshold/calibration\n",
    "    lin_cal = CalibratedClassifierCV(lin_tuned, method=\"sigmoid\", cv=2).fit(Xtr, ytr)\n",
    "\n",
    "    models = {\n",
    "        \"DT\": dt_tuned, \"kNN\": knn_tuned,\n",
    "        \"Linear SVM (calibrated)\": lin_cal, \"RBF SVM\": rbf_tuned, \"NN (SGD)\": nn_tuned\n",
    "    }\n",
    "\n",
    "    # Threshold tuning on a small internal val split\n",
    "    Xs, Xv, ys, yv = train_test_split(Xtr, ytr, test_size=0.2, random_state=RANDOM_STATE, stratify=ytr)\n",
    "    tuned_thr = {}\n",
    "    for name, m in models.items():\n",
    "        score = m.predict_proba(Xv)[:,1] if hasattr(m, \"predict_proba\") else m.decision_function(Xv)\n",
    "        thr, _ = f1_opt_threshold(yv, score); tuned_thr[name] = thr\n",
    "\n",
    "    # Test metrics + plots\n",
    "    plt.figure(figsize=(6.6,5)); ax = plt.gca()\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        RocCurveDisplay.from_predictions(yte, s, name=name, ax=ax)\n",
    "    ax.set_title(\"Hotel — ROC curves\"); ax.grid(alpha=.3)\n",
    "    savefig(\"Hotel\",\"ROC_All\")\n",
    "\n",
    "    plt.figure(figsize=(6.6,5)); ax = plt.gca()\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        PrecisionRecallDisplay.from_predictions(yte, s, name=name, ax=ax)\n",
    "    ax.set_title(\"Hotel — PR curves\"); ax.grid(alpha=.3)\n",
    "    savefig(\"Hotel\",\"PR_All\")\n",
    "\n",
    "    # Calibration (reliability) plots for top 2 by PR-AUC\n",
    "    scored = []\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        scored.append((name, pr_auc(yte, s), s))\n",
    "    top2 = sorted(scored, key=lambda x: x[1], reverse=True)[:2]\n",
    "    plt.figure(figsize=(6.2,4.6)); ax = plt.gca()\n",
    "    for name, _, s in top2:\n",
    "        CalibrationDisplay.from_predictions(yte, s, name=name, ax=ax)\n",
    "    ax.set_title(\"Hotel — Calibration (Reliability)\"); ax.grid(alpha=.3)\n",
    "    savefig(\"Hotel\",\"Calibration_Top2\")\n",
    "\n",
    "    # Confusion matrix at tuned threshold for best model (by PR-AUC)\n",
    "    best_name, _, best_scores = top2[0]\n",
    "    thr = tuned_thr[best_name]\n",
    "    yhat = (best_scores >= thr).astype(int)\n",
    "    cm = confusion_matrix(yte, yhat)\n",
    "    plt.figure(figsize=(4.8,4.4))\n",
    "    plt.imshow(cm, cmap='Blues'); plt.title(f\"Hotel — Confusion Matrix @{thr:.3f}\\n{best_name}\")\n",
    "    for (i,j),v in np.ndenumerate(cm): plt.text(j, i, int(v), ha='center', va='center')\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    savefig(\"Hotel\",\"CM_best\")\n",
    "    \n",
    "    # Decision Tree structure analysis\n",
    "    print(\"\\n[Decision Tree Structure Analysis]\")\n",
    "    dt_model = models[\"DT\"].named_steps['clf']\n",
    "    print(f\"  Max depth reached: {dt_model.get_depth()}\")\n",
    "    print(f\"  Number of leaves: {dt_model.get_n_leaves()}\")\n",
    "    print(f\"  Total nodes: {dt_model.tree_.node_count}\")\n",
    "    \n",
    "    # Feature importances (top 10)\n",
    "    if hasattr(dt_model, 'feature_importances_'):\n",
    "        importances = dt_model.feature_importances_\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(len(importances))]\n",
    "        top_indices = np.argsort(importances)[-10:][::-1]\n",
    "        \n",
    "        print(f\"\\n  Top 10 Feature Importances (Gini):\")\n",
    "        for idx in top_indices:\n",
    "            if importances[idx] > 0:\n",
    "                print(f\"    {feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "        # Plot top 10 importances\n",
    "        plt.figure(figsize=(6.2, 4.6))\n",
    "        plt.barh(range(len(top_indices)), importances[top_indices])\n",
    "        plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])\n",
    "        plt.xlabel('Gini Importance'); plt.title('Hotel — DT Top 10 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        savefig(\"Hotel\", \"DT_Feature_Importances\")\n",
    "\n",
    "    # Print final test metrics with prevalence baseline + per-class metrics\n",
    "    prev = float(np.mean(yte))\n",
    "    print(f\"\\n[Hotel Test] Prevalence (PR-AUC baseline) = {prev:.4f}\")\n",
    "    print(f\"{'Model':<24s} {'ROC-AUC':>8s} {'PR-AUC':>8s} {'F1@thr':>8s} {'Precision':>10s} {'Recall':>8s}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        roc = roc_auc_score(yte, s); pr = pr_auc(yte, s)\n",
    "        thr = tuned_thr[name]\n",
    "        y_pred = (s >= thr).astype(int)\n",
    "        f1 = f1_score(yte, y_pred)\n",
    "        prec = precision_score(yte, y_pred)\n",
    "        rec = recall_score(yte, y_pred)\n",
    "        print(f\"{name:24s} {roc:8.4f} {pr:8.4f} {f1:8.4f} {prec:10.4f} {rec:8.4f}\")\n",
    "    \n",
    "    # Support vector analysis for Linear SVM\n",
    "    print(\"\\n[Linear SVM Diagnostics]\")\n",
    "    lin_svc_base = lin_cal.calibrated_classifiers_[0].estimator  # Get base LinearSVC from calibrated wrapper\n",
    "    sv_frac = support_vector_analysis(lin_svc_base, Xtr, ytr, \"Linear SVM\")\n",
    "    if sv_frac:\n",
    "        savefig(\"Hotel\", \"LinearSVM_DecisionFunction_Distribution\")\n",
    "    \n",
    "    # NN Epoch curves for main model\n",
    "    print(\"\\n[Generating NN epoch curves...]\")\n",
    "    Xtr_nn, Xval_nn, ytr_nn, yval_nn = train_test_split(Xtr, ytr, test_size=0.2, random_state=RANDOM_STATE, stratify=ytr)\n",
    "    plot_nn_epoch_curve(nn_tuned, Xtr_nn, ytr_nn, Xval_nn, yval_nn, \"SGD\", prefix=\"Hotel\", classification=True)\n",
    "\n",
    "    # Runtime table entries (fit+predict on test)\n",
    "    runtimes = []\n",
    "    for name, m in models.items():\n",
    "        # clone-like refit for timing on train; prediction on test\n",
    "        tfit, tpred, peak, _ = time_fit_predict(m, Xtr, ytr, Xte, yte, fit_fn=\"fit\", predict_fn=\"predict\")\n",
    "        runtimes.append([\"Hotel\", name, len(Xtr), len(Xte), tfit, tpred, peak])\n",
    "\n",
    "    return models, (Xtr, Xte, ytr, yte), runtimes, pre, shallow, deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9851dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ACCIDENTS — Regression (OPTIMIZED FOR SPEED)\n",
    "# =========================\n",
    "def run_accidents(path=\"US_Accidents_March23.csv\"):\n",
    "    print(\"\\n== US ACCIDENTS (REGRESSION: Duration minutes) ==\")\n",
    "    X, y = load_accidents(path)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    pre = accidents_preprocessor(Xtr); pre.fit(Xtr, ytr)\n",
    "    ntr = len(Xtr); nte = len(Xte)\n",
    "    print(f\"[Accidents] cleaned rows={ntr+nte} → train={ntr} test={nte}\")\n",
    "\n",
    "    # Size rules (rubric-compliant - using maximum allowed limits)\n",
    "    # Final training uses ≥1M for DT/Linear, maximum limits for RBF/kNN\n",
    "    n_final_train = min(ntr, 1_200_000)  # ≥1M for final model (rubric requirement)\n",
    "    n_cv = min(ntr, 300_000)  # Smaller for CV/curves (10x speedup)\n",
    "    n_rbf = min(ntr, 100_000)   # RBF SVR ≤100k (using full allowance)\n",
    "    n_knn = min(ntr, 250_000)  # kNN ≤250k (using full allowance)\n",
    "    n_knn_test = min(nte, 25_000)  # kNN test ≤25k\n",
    "    \n",
    "    print(f\"[Optimization] Using n={n_cv:,} for curves/CV; n={n_final_train:,} for final models\")\n",
    "\n",
    "    sel = np.random.RandomState(RANDOM_STATE).permutation(ntr)\n",
    "    # CV/curves subsets (fast)\n",
    "    Xtr_cv, ytr_cv = Xtr.iloc[sel[:n_cv]], ytr.iloc[sel[:n_cv]]\n",
    "    # Final training subsets (meets requirements)\n",
    "    Xtr_final, ytr_final = Xtr.iloc[sel[:n_final_train]], ytr.iloc[sel[:n_final_train]]\n",
    "    Xtr_rbf, ytr_rbf = Xtr.iloc[sel[:n_rbf]], ytr.iloc[sel[:n_rbf]]\n",
    "    Xtr_knn, ytr_knn = Xtr.iloc[sel[:n_knn]], ytr.iloc[sel[:n_knn]]\n",
    "    Xte_knn, yte_knn = Xte.iloc[:n_knn_test], yte.iloc[:n_knn_test]\n",
    "\n",
    "    post = ('post', StandardScaler(with_mean=False))\n",
    "    dt = Pipeline([('prep', pre), ('reg', DecisionTreeRegressor(random_state=RANDOM_STATE))])\n",
    "    knn = Pipeline([('prep', pre), post, ('reg', KNeighborsRegressor(n_neighbors=11, n_jobs=-1))])\n",
    "    lsvr = Pipeline([('prep', pre), post, ('reg', LinearSVR(C=1.0, random_state=RANDOM_STATE, max_iter=10000, dual='auto'))])\n",
    "    rsvr = Pipeline([('prep', pre), post, ('reg', SVR(kernel='rbf', cache_size=1000))])\n",
    "\n",
    "    # NN (SGD-only)\n",
    "    input_dim = pre.transform(Xtr[:5]).shape[1]\n",
    "    target_dim = 1\n",
    "    shallow = choose_widths(input_dim, target_dim, shallow=True)\n",
    "    \n",
    "    # Verify param counts are in 0.2M-1.0M range (rubric requirement)\n",
    "    shallow_params = count_nn_params(shallow, input_dim, output_dim=1)\n",
    "    print(f\"[NN Architecture] input_dim={input_dim}, shallow={shallow}\")\n",
    "    print(f\"  Total params: {shallow_params:,} ({shallow_params/1e6:.2f}M)\")\n",
    "    if not (200_000 <= shallow_params <= 1_000_000):\n",
    "        print(f\"  ⚠️  WARNING: Params {shallow_params:,} outside 0.2M-1.0M range!\")\n",
    "    \n",
    "    nn = Pipeline([\n",
    "        ('prep', pre), post,\n",
    "        ('reg', MLPRegressor(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                             batch_size=2048, learning_rate='adaptive', learning_rate_init=0.01,\n",
    "                             alpha=1e-4, early_stopping=True, n_iter_no_change=3,\n",
    "                             max_iter=15, shuffle=True, momentum=0.0,\n",
    "                             random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "    # SPEED OPTIMIZATION: Validation curves on CV subset with reduced CV folds\n",
    "    print(\"\\n[Generating validation curves - using n=300k, cv=2 for speed...]\")\n",
    "    plot_validation(Pipeline([('prep', pre), ('reg', DecisionTreeRegressor(random_state=RANDOM_STATE))]),\n",
    "                    Xtr_cv, ytr_cv, \"reg__max_depth\", [6,10,14,18], \"neg_mean_absolute_error\", \"DT vs depth\", cv=2, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', KNeighborsRegressor(n_jobs=-1))]),\n",
    "                    Xtr_knn, ytr_knn, \"reg__n_neighbors\", [5,11,21], \"neg_mean_absolute_error\", \"kNN vs k\", cv=2, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', LinearSVR(max_iter=5000, random_state=RANDOM_STATE, dual='auto'))]),\n",
    "                    Xtr_cv, ytr_cv, \"reg__C\", [0.01,0.1,1,10], \"neg_mean_absolute_error\", \"LinearSVR vs C\", cv=2, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', SVR(kernel='rbf', cache_size=1000))]),\n",
    "                    Xtr_rbf, ytr_rbf, \"reg__C\", [1,10], \"neg_mean_absolute_error\", \"RBF SVR vs C (γ=scale)\", cv=2, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', MLPRegressor(solver='sgd', momentum=0.0, batch_size=2048, max_iter=10, random_state=RANDOM_STATE))]),\n",
    "                    Xtr_cv, ytr_cv, \"reg__alpha\", [1e-4,5e-4,1e-3], \"neg_mean_absolute_error\", \"NN vs L2(alpha)\", cv=2, prefix=\"Accidents\")\n",
    "\n",
    "    # SPEED OPTIMIZATION: Learning curves on CV subset with fewer points\n",
    "    print(\"\\n[Generating learning curves - using fewer samples for speed...]\")\n",
    "    def LC(est, title, Xlc, ylc): \n",
    "        protocol_card(title, Xlc, ylc, \"neg_mean_absolute_error\", cv=2)\n",
    "        sizes = np.linspace(0.2, 1.0, 4)  # 4 points instead of 5\n",
    "        tr_sizes, tr_s, te_s = learning_curve(\n",
    "            est, Xlc, ylc, train_sizes=sizes, cv=2, scoring=\"neg_mean_absolute_error\", n_jobs=-1, random_state=RANDOM_STATE\n",
    "        )\n",
    "        plt.figure(figsize=(6.2,4.6))\n",
    "        plt.title(f\"Accidents — Learning Curve: {title}\")\n",
    "        plt.xlabel(\"Training examples\"); plt.ylabel(\"NEG_MEAN_ABSOLUTE_ERROR\"); plt.grid(alpha=.3)\n",
    "        plt.plot(tr_sizes, tr_s.mean(1), 'o-', label=\"Training\")\n",
    "        plt.plot(tr_sizes, te_s.mean(1), 'o-', label=\"Cross-val\"); plt.legend()\n",
    "        savefig(\"Accidents\", f\"LC_{title}\")\n",
    "    \n",
    "    LC(dt, \"Decision Tree\", Xtr_cv, ytr_cv)\n",
    "    LC(knn, \"kNN\", Xtr_knn, ytr_knn)\n",
    "    LC(lsvr, \"Linear SVR\", Xtr_cv, ytr_cv)\n",
    "    LC(rsvr, \"RBF SVR\", Xtr_rbf, ytr_rbf)\n",
    "    LC(nn, \"Neural Net (SGD)\", Xtr_cv, ytr_cv)\n",
    "\n",
    "    # SPEED OPTIMIZATION: Lighter grid search on CV subset, then retrain on full data\n",
    "    print(\"\\n[Hyperparameter tuning on subset, then retraining on ≥1M rows...]\")\n",
    "    dt_params = GridSearchCV(dt, {\"reg__max_depth\":[10,14], \"reg__min_samples_leaf\":[200,400]}, \n",
    "                             cv=2, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_cv, ytr_cv).best_params_\n",
    "    knn_params = GridSearchCV(knn, {\"reg__n_neighbors\":[3,5,11,21]}, \n",
    "                              cv=2, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_knn, ytr_knn).best_params_\n",
    "    lsvr_params = GridSearchCV(lsvr, {\"reg__C\":[0.1,1,10]}, \n",
    "                               cv=2, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_cv, ytr_cv).best_params_\n",
    "    # RBF: tune both C and gamma (rubric requirement)\n",
    "    gamma_values = [\"scale\", 1.0/input_dim, 2.0/input_dim]\n",
    "    rsvr_params = GridSearchCV(rsvr, {\"reg__C\":[1,10], \"reg__gamma\":gamma_values}, \n",
    "                               cv=2, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_rbf, ytr_rbf).best_params_\n",
    "    nn_params = GridSearchCV(nn, {\"reg__alpha\":[1e-4,1e-3]}, \n",
    "                             cv=2, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_cv, ytr_cv).best_params_\n",
    "    \n",
    "    # Retrain with best params on FULL training sets (meets ≥1M requirement)\n",
    "    print(f\"[Final training] DT & LinearSVR on {n_final_train:,} rows\")\n",
    "    # ✅ Verify rubric compliance\n",
    "    if n_final_train < 1_000_000:\n",
    "        print(f\"  ⚠️  WARNING: n_final_train={n_final_train:,} < 1M (rubric requires ≥1M for DT/Linear)\")\n",
    "    else:\n",
    "        print(f\"  ✅ Rubric compliance: ≥1M rows for DT/LinearSVR\")\n",
    "    print(f\"  RBF SVR: {n_rbf:,} rows (≤100k cap)\")\n",
    "    print(f\"  kNN: {n_knn:,} train / {n_knn_test:,} test (≤250k/≤25k caps)\")\n",
    "    \n",
    "    dt_g = Pipeline([('prep', pre), ('reg', DecisionTreeRegressor(**dt_params, random_state=RANDOM_STATE))]).fit(Xtr_final, ytr_final)\n",
    "    knn_g = Pipeline([('prep', pre), post, ('reg', KNeighborsRegressor(**knn_params, n_jobs=-1))]).fit(Xtr_knn, ytr_knn)\n",
    "    lsvr_g = Pipeline([('prep', pre), post, ('reg', LinearSVR(**lsvr_params, max_iter=10000, random_state=RANDOM_STATE, dual='auto'))]).fit(Xtr_final, ytr_final)\n",
    "    rsvr_g = Pipeline([('prep', pre), post, ('reg', SVR(kernel='rbf', **rsvr_params, cache_size=1000))]).fit(Xtr_rbf, ytr_rbf)\n",
    "    nn_g = Pipeline([('prep', pre), post, ('reg', MLPRegressor(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                                                                batch_size=2048, learning_rate='adaptive', learning_rate_init=0.01,\n",
    "                                                                momentum=0.0, **nn_params, early_stopping=True, \n",
    "                                                                n_iter_no_change=3, max_iter=15, shuffle=True,\n",
    "                                                                random_state=RANDOM_STATE))]).fit(Xtr_final, ytr_final)\n",
    "\n",
    "    models = {\"DT\": dt_g, \"kNN\": knn_g, \"Linear SVR\": lsvr_g, \"RBF SVR\": rsvr_g, \"NN (SGD)\": nn_g}\n",
    "\n",
    "    # NN Epoch curves\n",
    "    print(\"\\n[Generating NN epoch curves...]\")\n",
    "    Xtr_nn, Xval_nn, ytr_nn, yval_nn = train_test_split(Xtr_final, ytr_final, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    plot_nn_epoch_curve(nn_g, Xtr_nn, ytr_nn, Xval_nn, yval_nn, \"SGD\", prefix=\"Accidents\", classification=False)\n",
    "    \n",
    "    # Support vector analysis for Linear SVR\n",
    "    print(\"\\n[Linear SVR Diagnostics]\")\n",
    "    sv_frac = support_vector_analysis(lsvr_g, Xtr_final, ytr_final, \"Linear SVR\")\n",
    "    if sv_frac:\n",
    "        savefig(\"Accidents\", \"LinearSVR_DecisionFunction_Distribution\")\n",
    "    \n",
    "    # Test metrics & residuals\n",
    "    print(\"\\n[Accidents Test] MAE / MedAE / RMSE / predict-time (s)\")\n",
    "    print(f\"{'Model':<12s} {'MAE':>8s} {'MedAE':>8s} {'RMSE':>8s} {'Fit(s)':>8s} {'Pred(s)':>8s} {'RAM(MB)':>8s} {'n_train':>10s}\")\n",
    "    print(\"-\" * 95)\n",
    "    runtimes = []\n",
    "    for name, m in models.items():\n",
    "        # Use correct training sets based on model type\n",
    "        train_X = Xtr_rbf if name == \"RBF SVR\" else (Xtr_knn if name == \"kNN\" else Xtr_final)\n",
    "        train_y = ytr_rbf if name == \"RBF SVR\" else (ytr_knn if name == \"kNN\" else ytr_final)\n",
    "        tfit, tpred, peak, pred = time_fit_predict(m, train_X, train_y,\n",
    "                                                   Xte if name!=\"kNN\" else Xte_knn,\n",
    "                                                   yte if name!=\"kNN\" else yte_knn)\n",
    "        test_y = yte if name!=\"kNN\" else yte_knn\n",
    "        mae = mean_absolute_error(test_y, pred)\n",
    "        med = median_absolute_error(test_y, pred)\n",
    "        rmse= math.sqrt(mean_squared_error(test_y, pred))\n",
    "        n_train_used = len(train_X) if hasattr(train_X, '__len__') else len(train_y)\n",
    "        print(f\"{name:12s} {mae:8.2f} {med:8.2f} {rmse:8.2f} {tfit:8.1f} {tpred:8.2f} {peak:8.0f} {n_train_used:10,}\")\n",
    "        runtimes.append([\"Accidents\", name, n_train_used,\n",
    "                         len(Xte if name!='kNN' else Xte_knn), tfit, tpred, peak])\n",
    "    \n",
    "    # Additional regression diagnostics: R² and explained variance\n",
    "    from sklearn.metrics import r2_score, explained_variance_score\n",
    "    print(\"\\n[Additional Regression Metrics]\")\n",
    "    print(f\"{'Model':<12s} {'R²':>8s} {'Exp.Var':>8s} {'Mean Abs % Err':>15s}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, m in models.items():\n",
    "        test_X = Xte if name != \"kNN\" else Xte_knn\n",
    "        test_y = yte if name != \"kNN\" else yte_knn\n",
    "        pred = m.predict(test_X)\n",
    "        r2 = r2_score(test_y, pred)\n",
    "        ev = explained_variance_score(test_y, pred)\n",
    "        # Mean Absolute Percentage Error (avoid division by zero)\n",
    "        mape = np.mean(np.abs((test_y - pred) / np.maximum(test_y, 1e-8))) * 100\n",
    "        print(f\"{name:12s} {r2:8.4f} {ev:8.4f} {mape:14.2f}%\")\n",
    "\n",
    "    # Decision Tree diagnostics\n",
    "    print(\"\\n[Decision Tree Structure Analysis]\")\n",
    "    dt_model = models[\"DT\"].named_steps['reg']\n",
    "    print(f\"  Max depth reached: {dt_model.get_depth()}\")\n",
    "    print(f\"  Number of leaves: {dt_model.get_n_leaves()}\")\n",
    "    print(f\"  Total nodes: {dt_model.tree_.node_count}\")\n",
    "    \n",
    "    # Feature importances (top 10)\n",
    "    if hasattr(dt_model, 'feature_importances_'):\n",
    "        importances = dt_model.feature_importances_\n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(len(importances))]\n",
    "        top_indices = np.argsort(importances)[-10:][::-1]\n",
    "        \n",
    "        print(f\"\\n  Top 10 Feature Importances (Gini):\")\n",
    "        for idx in top_indices:\n",
    "            if importances[idx] > 0:\n",
    "                print(f\"    {feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "        \n",
    "        # Plot top 10 importances\n",
    "        plt.figure(figsize=(6.2, 4.6))\n",
    "        plt.barh(range(len(top_indices)), importances[top_indices])\n",
    "        plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])\n",
    "        plt.xlabel('Gini Importance'); plt.title('Accidents — DT Top 10 Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        savefig(\"Accidents\", \"DT_Feature_Importances\")\n",
    "    \n",
    "    # Residuals vs prediction for best model (by MAE)\n",
    "    maes = {name: mean_absolute_error(yte if name!=\"kNN\" else yte_knn,\n",
    "                                      models[name].predict(Xte if name!=\"kNN\" else Xte_knn))\n",
    "            for name in models}\n",
    "    best = min(maes, key=maes.get)\n",
    "    ypred = models[best].predict(Xte)\n",
    "    resid = (yte - ypred)\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.scatter(ypred, resid, s=6, alpha=.35)\n",
    "    plt.axhline(0, ls='--', color='red'); plt.xlabel(\"Predicted duration (min)\"); plt.ylabel(\"Residual (true - pred)\")\n",
    "    plt.title(f\"Accidents — Residuals vs Prediction ({best})\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    savefig(\"Accidents\",\"Residuals_vs_Pred\")\n",
    "    \n",
    "    # Prediction vs actual scatter plot\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.scatter(yte, ypred, s=6, alpha=.35)\n",
    "    plt.plot([yte.min(), yte.max()], [yte.min(), yte.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "    plt.xlabel(\"Actual duration (min)\"); plt.ylabel(\"Predicted duration (min)\")\n",
    "    plt.title(f\"Accidents — Prediction vs Actual ({best})\")\n",
    "    plt.legend(); plt.grid(alpha=0.3)\n",
    "    savefig(\"Accidents\",\"Prediction_vs_Actual\")\n",
    "\n",
    "    return models, (Xtr, Xte, ytr, yte), runtimes\n",
    "\n",
    "# =========================\n",
    "# Activation study (Hotel)\n",
    "# =========================\n",
    "def activation_study_hotel(pre_fitted, Xtr, Xte, ytr, yte, width_tuple):\n",
    "    \"\"\"\n",
    "    Compares ReLU, GELU, SiLU/Swish, tanh under identical SGD (no momentum).\n",
    "    Requires PyTorch for GELU/SiLU; otherwise runs ReLU/tanh fallback.\n",
    "    Saves epoch-wise val curves and a comparison CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "    except Exception as e:\n",
    "        print(f\"PyTorch not available ({e}); skipping GELU/SiLU. Running ReLU/tanh with sklearn MLP.\")\n",
    "        # fallback: plot epoch curves via sklearn MLPClassifier for relu/tanh\n",
    "        results = []\n",
    "        for act in [\"relu\", \"tanh\"]:\n",
    "            clf = MLPClassifier(hidden_layer_sizes=width_tuple, solver='sgd', momentum=0.0, nesterovs_momentum=False,\n",
    "                                batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                                alpha=1e-4, max_iter=15, early_stopping=True, n_iter_no_change=3,\n",
    "                                random_state=RANDOM_STATE, activation=act)\n",
    "            pipe = Pipeline([('prep', pre_fitted), ('post', StandardScaler(with_mean=False)), ('clf', clf)])\n",
    "            protocol_card(f\"NN-{act.upper()}\", Xtr, ytr, \"roc_auc\", cv=None)\n",
    "            # We can't easily get per-epoch val here; use final metric:\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            s = pipe.predict_proba(Xte)[:,1]\n",
    "            roc = roc_auc_score(yte, s); pr = pr_auc(yte, s)\n",
    "            results.append([act, roc, pr])\n",
    "        pd.DataFrame(results, columns=[\"activation\",\"ROC\",\"PR\"]).to_csv(os.path.join(LOG_DIR,\"extra_credit_activation_summary.csv\"), index=False)\n",
    "        return\n",
    "\n",
    "    torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE)\n",
    "    Xtr_t = pre_fitted.transform(Xtr).astype(np.float32)\n",
    "    Xte_t = pre_fitted.transform(Xte).astype(np.float32)\n",
    "    d = Xtr_t.shape[1]\n",
    "\n",
    "    def make_loader(X, y, bs=1024, shuffle=True):\n",
    "        ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y.values.astype(np.int64)))\n",
    "        return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "    train_loader = make_loader(Xtr_t, ytr)\n",
    "    val_loader   = make_loader(Xte_t, yte, shuffle=False)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, act_name):\n",
    "            super().__init__()\n",
    "            acts = {\n",
    "                \"relu\": nn.ReLU(),\n",
    "                \"gelu\": nn.GELU(),\n",
    "                \"silu\": nn.SiLU(),\n",
    "                \"tanh\": nn.Tanh()\n",
    "            }\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(d, width_tuple[0]), acts[act_name],\n",
    "                nn.Linear(width_tuple[0], width_tuple[1]), acts[act_name],\n",
    "                nn.Linear(width_tuple[1], 1)\n",
    "            )\n",
    "        def forward(self, x): return self.net(x)\n",
    "\n",
    "    def eval_pr(y_true, scores):\n",
    "        y = y_true.numpy(); s = scores.reshape(-1)\n",
    "        p, r, _ = precision_recall_curve(y, s); return auc(r, p)\n",
    "\n",
    "    activations = [\"relu\", \"gelu\", \"silu\", \"tanh\"]\n",
    "    results = []\n",
    "    for act in activations:\n",
    "        model = MLP(act)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.0, weight_decay=1e-4)  # SGD only\n",
    "        best_val = -1; best_epoch = 0; patience=3; bad=0\n",
    "        val_hist = []; roc_hist=[]; pr_hist=[]\n",
    "        for epoch in range(15):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                opt.zero_grad()\n",
    "                logits = model(xb).squeeze(1)\n",
    "                l = loss(logits, yb.float())\n",
    "                l.backward(); opt.step()\n",
    "            # validate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = []\n",
    "                for xb, _ in val_loader:\n",
    "                    logits.append(model(xb).squeeze(1))\n",
    "                logits = torch.cat(logits).detach().numpy()\n",
    "                roc = roc_auc_score(yte, logits); pr = pr_auc(yte, logits)\n",
    "                roc_hist.append(roc); pr_hist.append(pr)\n",
    "                val_hist.append(float(l))\n",
    "            # early stop on PR-AUC\n",
    "            if pr > best_val: best_val = pr; best_epoch = epoch; bad=0\n",
    "            else: bad += 1\n",
    "            if bad >= patience: break\n",
    "\n",
    "        # Save epoch curves\n",
    "        plt.figure(figsize=(6.2,4.6))\n",
    "        plt.plot(range(1, len(pr_hist)+1), pr_hist, 'o-'); plt.grid(alpha=.3)\n",
    "        plt.title(f\"Hotel — NN Activation ({act.upper()}) PR-AUC vs epoch (best@{best_epoch+1})\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"PR-AUC\"); savefig(\"Hotel\", f\"NN_Activation_{act.upper()}_EpochCurve\")\n",
    "        results.append([act, pr_hist[-1], roc_hist[-1], best_epoch+1])\n",
    "\n",
    "    pd.DataFrame(results, columns=[\"activation\",\"PR_AUC\",\"ROC_AUC\",\"best_epoch\"]).to_csv(\n",
    "        os.path.join(LOG_DIR,\"activation_summary.csv\"), index=False)\n",
    "    print(\"Wrote sl_outputs/logs/activation_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb3e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hardware] Darwin 24.6.0; CPU: arm; Python 3.11.0\n",
      "\n",
      "== HOTEL (CLASSIFICATION: is_canceled) ==\n",
      "[Hotel] rows raw=119390  target=is_canceled  (train/test split later)\n",
      "[NN Architectures] input_dim=83\n",
      "  Shallow [512, 512]: 306,690 params (0.31M)\n",
      "  Deeper [512, 512, 256, 256]: 503,298 params (0.50M)\n",
      "[Optimization] Using 40,000 samples for curves (cv=2, faster)\n",
      "\n",
      "[Generating validation curves...]\n",
      "[DT vs depth] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[NN Architectures] input_dim=83\n",
      "  Shallow [512, 512]: 306,690 params (0.31M)\n",
      "  Deeper [512, 512, 256, 256]: 503,298 params (0.50M)\n",
      "[Optimization] Using 40,000 samples for curves (cv=2, faster)\n",
      "\n",
      "[Generating validation curves...]\n",
      "[DT vs depth] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_DT_vs_depth.png\n",
      "[kNN vs k] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_DT_vs_depth.png\n",
      "[kNN vs k] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_kNN_vs_k.png\n",
      "[LinearSVM vs C] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_kNN_vs_k.png\n",
      "[LinearSVM vs C] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_LinearSVM_vs_C.png\n",
      "[Note] Using 10000 samples for RBF validation curve (O(n³) complexity)\n",
      "[RBF-SVM vs C (γ=scale)] n=10000 scoring=roc_auc cv=2 prevalence=0.371\n",
      "[saved] sl_outputs/figs/Hotel_MC_LinearSVM_vs_C.png\n",
      "[Note] Using 10000 samples for RBF validation curve (O(n³) complexity)\n",
      "[RBF-SVM vs C (γ=scale)] n=10000 scoring=roc_auc cv=2 prevalence=0.371\n",
      "[saved] sl_outputs/figs/Hotel_MC_RBF-SVM_vs_C_(γ=scale).png\n",
      "[NN vs L2(alpha)] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_RBF-SVM_vs_C_(γ=scale).png\n",
      "[NN vs L2(alpha)] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_NN_vs_L2(alpha).png\n",
      "\n",
      "[Generating learning curves...]\n",
      "[Decision Tree] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_MC_NN_vs_L2(alpha).png\n",
      "\n",
      "[Generating learning curves...]\n",
      "[Decision Tree] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_Decision_Tree.png\n",
      "[kNN] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_Decision_Tree.png\n",
      "[kNN] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_kNN.png\n",
      "[Linear SVM] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_kNN.png\n",
      "[Linear SVM] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_Linear_SVM.png\n",
      "[Note] Using 15000 samples for RBF learning curve (O(n³) complexity)\n",
      "[RBF SVM] n=15000 scoring=roc_auc cv=2 prevalence=0.373\n",
      "[saved] sl_outputs/figs/Hotel_LC_Linear_SVM.png\n",
      "[Note] Using 15000 samples for RBF learning curve (O(n³) complexity)\n",
      "[RBF SVM] n=15000 scoring=roc_auc cv=2 prevalence=0.373\n",
      "[saved] sl_outputs/figs/Hotel_LC_RBF_SVM.png\n",
      "[Neural Net (SGD)] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_RBF_SVM.png\n",
      "[Neural Net (SGD)] n=40000 scoring=roc_auc cv=2 prevalence=0.372\n",
      "[saved] sl_outputs/figs/Hotel_LC_Neural_Net_(SGD).png\n",
      "\n",
      "[Hyperparameter tuning on subset...]\n",
      "[saved] sl_outputs/figs/Hotel_LC_Neural_Net_(SGD).png\n",
      "\n",
      "[Hyperparameter tuning on subset...]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    hw_note()\n",
    "\n",
    "    hotel_models, hotel_data, hotel_runtime, hotel_pre, w1, w2 = run_hotel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02518ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_models, acc_data, acc_runtime = run_accidents(\"US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518137e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save runtime table\n",
    "cols = [\"dataset\",\"model\",\"n_train\",\"n_test\",\"fit_sec\",\"pred_sec\",\"peak_ram_mb\"]\n",
    "rt = pd.DataFrame(hotel_runtime + acc_runtime, columns=cols)\n",
    "rt.to_csv(os.path.join(LOG_DIR, \"runtime_table.csv\"), index=False)\n",
    "print(f\"[saved] {os.path.join(LOG_DIR,'runtime_table.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32427431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra credit activation study on Hotel using (width_tuple) consistent with main NN\n",
    "# (uses identical SGD protocol and preprocessing)\n",
    "activation_study_hotel(hotel_pre, hotel_data[0], hotel_data[1], hotel_data[2], hotel_data[3], tuple(w1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
