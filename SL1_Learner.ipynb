{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588f7efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hotel] Loading with Polars…\n",
      "[Hotel] Loaded rows=119,390, cols=32\n",
      "[Hotel] Converting to pandas and casting dtypes…\n",
      "[Hotel] Stratified splits (70/15/15)…\n",
      "[Hotel] Counts: {'tag': 'Hotel', 'raw_rows': 119390, 'cleaned_rows': 119390, 'train_rows': 83573, 'val_rows': 17908, 'test_rows': 17909}\n",
      "[Hotel/DT] Fitting DecisionTreeClassifier…\n",
      "[Profiler] START: Hotel_DT_fit\n",
      "[Profiler] END: Hotel_DT_fit | sec=0.33 | peakGB=2.149\n",
      "[Hotel/DT] Predicting & scoring…\n",
      "[Plot] Learning curve: Hotel: DT Learning Curve\n",
      "[Plot] Reliability: Hotel: DT Reliability\n",
      "[Hotel/DT] Selecting threshold by val F1…\n",
      "[Hotel/DT] Model complexity sweep (max_depth)…\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supervised Learning — Hotel (classification) + US Accidents (regression)\n",
    "\n",
    "References (used in comments citations):\n",
    "[R1] Mitchell, T. M., \"Machine Learning\", McGraw-Hill, 1997. (DT/kNN/ANN overview)\n",
    "[R2] SL Report spec v6-1 (metrics/plots/splits/compute logs; deliverables)\n",
    "[R3] scikit-learn docs (APIs; implementation reference)\n",
    "[R4] Quinlan, \"Induction of Decision Trees\", Machine Learning, 1986. (DT)\n",
    "[R5] Cover & Hart, \"Nearest Neighbor Pattern Classification\", IEEE TIT, 1967. (kNN)\n",
    "[R6] Cortes & Vapnik, \"Support-Vector Networks\", Machine Learning, 1995. (SVM)\n",
    "[R7] Rumelhart et al., \"Learning representations by back-propagating errors\", Nature, 1986. (NN)\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# CHUNK 0 — imports, constants\n",
    "# ===============================\n",
    "\n",
    "import os, json, time, platform, warnings\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl  # fast CSV & memory-friendly transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, KFold, learning_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_curve, auc,\n",
    "    precision_recall_curve, confusion_matrix, classification_report,\n",
    "    mean_absolute_error, mean_squared_error, r2_score, log_loss\n",
    ")\n",
    "from sklearn.metrics import median_absolute_error as medae\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "try:\n",
    "    import psutil  # [R2] runtime/RAM logging\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "# Silence benign warnings; keep real errors visible\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=r\"sklearn\\.neural_network\")\n",
    "\n",
    "# Output roots\n",
    "OUT_HOTEL = \"outputs/hotel_cls\"   # Hotel = classification (is_canceled) [R2]\n",
    "OUT_ACC   = \"outputs/acc_reg\"     # Accidents = regression (duration_minutes) [R2]\n",
    "os.makedirs(OUT_HOTEL, exist_ok=True)\n",
    "os.makedirs(OUT_ACC, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42  # [R2] reproducibility\n",
    "\n",
    "# ========== YOUR FILE PATHS ==========\n",
    "HOTEL_CSV = \"hotel_bookings.csv\"\n",
    "ACC_CSV   = \"US_Accidents_March23.csv\"\n",
    "# =====================================\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# CHUNK 1 — tiny profiler + hardware descriptor\n",
    "# ================================================\n",
    "\n",
    "def hw_info():\n",
    "    info = dict(platform=platform.platform(), python=platform.python_version())\n",
    "    if psutil:\n",
    "        info[\"cpu_count\"] = psutil.cpu_count(logical=True)\n",
    "        info[\"ram_gb\"] = round(psutil.virtual_memory().total / 1e9, 2)\n",
    "    return info\n",
    "\n",
    "class Profiler:\n",
    "    \"\"\"Wall-clock + peak RSS logger for a code section. [R2]\"\"\"\n",
    "    def __init__(self, tag=\"run\"): self.tag = tag; self.t0 = None; self.max_rss = 0\n",
    "    def __enter__(self):\n",
    "        self.t0 = time.perf_counter()\n",
    "        self._sample()\n",
    "        print(f\"[Profiler] START: {self.tag}\")\n",
    "        return self\n",
    "    def _sample(self):\n",
    "        if psutil:\n",
    "            rss = psutil.Process().memory_info().rss\n",
    "            self.max_rss = max(self.max_rss, rss)\n",
    "    def tick(self): self._sample()\n",
    "    def __exit__(self, *exc):\n",
    "        self.seconds_fit = time.perf_counter() - self.t0\n",
    "        self._sample()\n",
    "        self.peak_gb = round(self.max_rss / 1e9, 3) if self.max_rss else None\n",
    "        print(f\"[Profiler] END: {self.tag} | sec={self.seconds_fit:.2f} | peakGB={self.peak_gb}\")\n",
    "\n",
    "def save_json_safe(path, obj):\n",
    "    \"\"\"Ensure json-serializable (cast numpy types to native Python).\"\"\"\n",
    "    def cast(o):\n",
    "        if isinstance(o, (np.integer,)): return int(o)\n",
    "        if isinstance(o, (np.floating,)): return float(o)\n",
    "        if isinstance(o, (np.ndarray,)): return o.tolist()\n",
    "        return o\n",
    "    if isinstance(obj, dict):\n",
    "        obj = {k: cast(v) for k,v in obj.items()}\n",
    "    json.dump(obj, open(path,\"w\"), indent=2)\n",
    "\n",
    "def save_profile(path, **kwargs):\n",
    "    save_json_safe(path, kwargs)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CHUNK 2 — plotting helpers [R2]\n",
    "# ========================================\n",
    "\n",
    "def plot_confusion(cm: np.ndarray, labels: List[str], title: str, outpath: str):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\", aspect=\"auto\")\n",
    "    plt.title(title)\n",
    "    ticks = np.arange(len(labels))\n",
    "    plt.xticks(ticks, labels, rotation=45)\n",
    "    plt.yticks(ticks, labels)\n",
    "    thr = cm.max()/2 if cm.max()>0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{int(cm[i,j])}\", ha=\"center\",\n",
    "                     color=\"white\" if cm[i,j]>thr else \"black\")\n",
    "    plt.ylabel(\"True\"); plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout(); plt.savefig(outpath, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_line(x, y, title, xlabel, ylabel, outpath):\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, label=title)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(outpath, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_learning_curve_single(\n",
    "    estimator, X, y, title, outpath, cv,\n",
    "    train_sizes=np.linspace(0.2, 1.0, 5),\n",
    "    scoring=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    "):\n",
    "    \"\"\"Learning curves diagnose bias/variance; required per model [R1][R2].\"\"\"\n",
    "    print(f\"[Plot] Learning curve: {title}\")\n",
    "    sizes, tr, va = learning_curve(\n",
    "        estimator, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv, shuffle=True, random_state=random_state,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        return_times=False\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.plot(sizes, tr.mean(axis=1), marker=\"o\", label=\"Training\")\n",
    "    plt.plot(sizes, va.mean(axis=1), marker=\"s\", label=\"Validation\")\n",
    "    plt.xlabel(\"Training examples\"); plt.ylabel(\"Score\"); plt.title(title); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(outpath, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_parity(y_true, y_pred, title, outpath):\n",
    "    \"\"\"Regression: parity (ŷ vs y) [R2].\"\"\"\n",
    "    print(f\"[Plot] Parity: {title}\")\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=6, alpha=0.35)\n",
    "    lo, hi = float(min(np.min(y_true), np.min(y_pred))), float(max(np.max(y_true), np.max(y_pred)))\n",
    "    plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
    "    plt.title(title); plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
    "    plt.tight_layout(); plt.savefig(outpath, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title, outpath):\n",
    "    \"\"\"Residuals (ŷ − y) vs ŷ exposes bias/heteroscedasticity [R2].\"\"\"\n",
    "    print(f\"[Plot] Residuals: {title}\")\n",
    "    res = y_pred - y_true\n",
    "    plt.figure()\n",
    "    plt.scatter(y_pred, res, s=6, alpha=0.35)\n",
    "    plt.axhline(0.0, linestyle=\"--\")\n",
    "    plt.title(title); plt.xlabel(\"Predicted\"); plt.ylabel(\"Residual (ŷ − y)\")\n",
    "    plt.tight_layout(); plt.savefig(outpath, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_reliability(y_true, y_proba, title, outpath, n_bins=10):\n",
    "    \"\"\"Probability calibration (reliability curve) for classification [R2].\"\"\"\n",
    "    print(f\"[Plot] Reliability: {title}\")\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, y_proba, n_bins=n_bins, strategy=\"uniform\")\n",
    "    plt.figure()\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\", label=\"Reliability\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\", label=\"Perfect\")\n",
    "    plt.title(title); plt.xlabel(\"Mean predicted probability\"); plt.ylabel(\"Fraction positives\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(outpath, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# CHUNK 3 — metrics & helpers [R2]\n",
    "# =========================================\n",
    "\n",
    "def summarize_classification(y_true, y_proba, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prc, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    prec, reca, _ = precision_recall_curve(y_true, y_proba)\n",
    "    pr_auc = auc(reca, prec)\n",
    "    return ({\"accuracy\":float(acc),\"precision\":float(prc),\"recall\":float(rec),\"f1\":float(f1),\n",
    "             \"roc_auc\":float(roc_auc),\"pr_auc\":float(pr_auc)},\n",
    "            (fpr,tpr), (reca,prec))\n",
    "\n",
    "def summarize_regression(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\":   float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"MedAE\": float(medae(y_true, y_pred)),\n",
    "        \"RMSE\":  float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"R2\":    float(r2_score(y_true, y_pred))\n",
    "    }\n",
    "\n",
    "def prevalence_baseline(y_true):\n",
    "    \"\"\"PR-AUC baseline equals positive class prevalence [R2].\"\"\"\n",
    "    return float(np.mean(y_true))\n",
    "\n",
    "def f1_optimal_threshold(y_true_val, y_proba_val):\n",
    "    \"\"\"Pick threshold on validation to maximize F1 (document rule in report) [R2].\"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "    thr_grid = np.linspace(0.05, 0.95, 19)\n",
    "    scores = [(thr, f1_score(y_true_val, (y_proba_val >= thr).astype(int))) for thr in thr_grid]\n",
    "    best_thr, best_f1 = max(scores, key=lambda t: t[1])\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "# ==== Extra helpers: permutation importances + NN budget audit ====\n",
    "\n",
    "def permutation_importance_topk(pipeline, X, y, scoring, k=10, n_repeats=10, random_state=RANDOM_STATE):\n",
    "    \"\"\"Top-k permutation importances on raw columns (permutes pre-transform inputs).\"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    r = permutation_importance(pipeline, X, y, scoring=scoring, n_repeats=n_repeats, random_state=random_state)\n",
    "    imp = pd.DataFrame({\"feature\": X.columns, \"importance\": r.importances_mean})\n",
    "    return imp.sort_values(\"importance\", ascending=False).head(k)\n",
    "\n",
    "def _estimate_mlp_params(n_in, hidden, n_out):\n",
    "    sizes = [n_in] + list(hidden) + [n_out]\n",
    "    params = 0\n",
    "    for i in range(len(sizes) - 1):\n",
    "        params += sizes[i] * sizes[i+1] + sizes[i+1]  # weights + biases\n",
    "    return int(params)\n",
    "\n",
    "def _estimate_input_dim(preproc, X_sample):\n",
    "    # tiny fit on a small slice to infer transformed width (safe & quick)\n",
    "    Xt = preproc.fit_transform(X_sample.head(64))\n",
    "    return Xt.shape[1]\n",
    "\n",
    "def finite_clip(X, clip=1_000.0):\n",
    "    # Ensure finite values and tame outliers before scaling\n",
    "    X = np.asarray(X, dtype=np.float32, order=\"C\")\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=clip, neginf=-clip)\n",
    "    return np.clip(X, -clip, clip)\n",
    "\n",
    "def to_dense32(X):\n",
    "    # ColumnTransformer may return sparse; MLP needs dense\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        X = X.toarray()\n",
    "    return np.asarray(X, dtype=np.float32, order=\"C\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CHUNK 4 — encoders & preprocessor (float32, high-card) [R2]\n",
    "# ==========================================================\n",
    "\n",
    "def make_ohe():\n",
    "    \"\"\"Create OneHotEncoder with best available arg (compat shim).\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "def detect_high_cardinality(df, cols, min_unique=100, min_ratio=0.05):\n",
    "    hi, lo = [], []\n",
    "    n = len(df)\n",
    "    for c in cols:\n",
    "        u = df[c].nunique(dropna=True)\n",
    "        if (u >= min_unique) or (u / max(n,1) >= min_ratio): hi.append(c)\n",
    "        else: lo.append(c)\n",
    "    return hi, lo\n",
    "\n",
    "def to_float32(X):\n",
    "    try:    return X.astype(np.float32)\n",
    "    except: return np.asarray(X, dtype=np.float32)\n",
    "\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Frequency-encode high-card categorical columns (leak-safe when used inside CV/Pipeline).\n",
    "    Clone-safe: init stores params verbatim (no mutation).\n",
    "    Works for DataFrame or ndarray slices (ColumnTransformer passes only selected columns).\n",
    "    \"\"\"\n",
    "    def __init__(self, cols=None, min_samples=5):\n",
    "        self.cols = cols\n",
    "        self.min_samples = min_samples\n",
    "        self._maps_by_pos = None\n",
    "        self._n_features_in_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        Xc = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        self._n_features_in_ = Xc.shape[1]\n",
    "        self._maps_by_pos = {}\n",
    "        for j in range(self._n_features_in_):\n",
    "            s = Xc.iloc[:, j]\n",
    "            vc = s.value_counts(dropna=True)\n",
    "            if (self.min_samples is not None) and (self.min_samples > 1):\n",
    "                vc = vc[vc >= self.min_samples]\n",
    "            total = float(vc.sum()) if vc.sum() > 0 else 1.0\n",
    "            freq = (vc / total).astype(\"float32\")\n",
    "            # prior fallback for unseen\n",
    "            freq = pd.concat([freq, pd.Series({\"__PRIOR__\": np.float32(0.0)}, dtype=\"float32\")])\n",
    "            self._maps_by_pos[j] = freq\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xc = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        k = Xc.shape[1]\n",
    "        out = np.zeros((len(Xc), k), dtype=\"float32\")\n",
    "        for j in range(k):\n",
    "            s = Xc.iloc[:, j]\n",
    "            freq = self._maps_by_pos.get(j)\n",
    "            if freq is None:\n",
    "                continue\n",
    "            mapped = s.map(freq).fillna(freq.get(\"__PRIOR__\", np.float32(0.0))).astype(\"float32\")\n",
    "            out[:, j] = mapped.to_numpy(copy=False)\n",
    "        return out\n",
    "\n",
    "def make_preprocessor(df, target, task=\"classification\"):\n",
    "    \"\"\"\n",
    "    Mixed encoding per [R2]:\n",
    "      - numerics: median impute + StandardScaler + float32\n",
    "      - low-card categoricals: one-hot (cast to float32)\n",
    "      - high-card categoricals: frequency encoding (inside pipeline)\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target])\n",
    "    num = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat = X.select_dtypes(include=[\"object\", \"category\", \"bool\", \"boolean\", \"string\"]).columns.tolist()\n",
    "\n",
    "    hi, lo = detect_high_cardinality(X, cat, min_unique=100, min_ratio=0.05)\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"finite\", FunctionTransformer(finite_clip, accept_sparse=True)),\n",
    "        (\"scale\",  StandardScaler()),                 # requirement: StandardScaler for numerics\n",
    "        (\"to32\",   FunctionTransformer(to_float32, accept_sparse=True))\n",
    "    ])\n",
    "\n",
    "    transformers = [(\"num\", num_pipe, num)]\n",
    "\n",
    "    if lo:\n",
    "        transformers.append((\"ohe\",\n",
    "            Pipeline([\n",
    "                (\"impute\", SimpleImputer(strategy=\"most_frequent\", missing_values=np.nan)),\n",
    "                (\"onehot\", make_ohe()),\n",
    "                (\"to32\",   FunctionTransformer(to_float32, accept_sparse=True)),\n",
    "            ]),\n",
    "            lo))\n",
    "\n",
    "    if hi:\n",
    "        transformers.append((\"freq_hi\",\n",
    "            Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\", missing_values=np.nan)),\n",
    "                      (\"freq\",   FrequencyEncoder(cols=hi, min_samples=5)),\n",
    "                      (\"to32\",   FunctionTransformer(to_float32))]),\n",
    "            hi))\n",
    "\n",
    "    return ColumnTransformer(transformers=transformers, sparse_threshold=0.3)\n",
    "\n",
    "# ===================================================\n",
    "# CHUNK 5 — HOTEL load/clean/split + preprocessor\n",
    "# ===================================================\n",
    "\n",
    "print(\"[Hotel] Loading with Polars…\")\n",
    "\n",
    "# Treat these strings as nulls; avoids parse errors like 'NA' in integer columns\n",
    "CSV_NULLS = [\"NA\", \"NaN\", \"NULL\", \"null\", \"\", \"None\"]\n",
    "\n",
    "# Read directly (file is small enough)\n",
    "hotel_df = pl.read_csv(\n",
    "    HOTEL_CSV,\n",
    "    infer_schema_length=20000,\n",
    "    null_values=CSV_NULLS,\n",
    "    schema_overrides={\n",
    "        \"children\": pl.Float64,\n",
    "        \"babies\": pl.Float64,\n",
    "        \"adults\": pl.Float64,\n",
    "    },\n",
    ")\n",
    "# Safety cast in case the columns are missing in your version\n",
    "for col in (\"children\", \"babies\", \"adults\"):\n",
    "    if col in hotel_df.columns:\n",
    "        hotel_df = hotel_df.with_columns(pl.col(col).cast(pl.Float64, strict=False))\n",
    "print(f\"[Hotel] Loaded rows={hotel_df.height:,}, cols={hotel_df.width}\")\n",
    "\n",
    "print(\"[Hotel] Converting to pandas and casting dtypes…\")\n",
    "hotel = hotel_df.to_pandas(use_pyarrow_extension_array=False)\n",
    "try:\n",
    "    hotel = hotel.replace({pd.NA: np.nan})\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Drop known leakage column if present\n",
    "if \"reservation_status\" in hotel.columns:\n",
    "    hotel = hotel.drop(columns=[\"reservation_status\"])\n",
    "\n",
    "# Cast boolean-like strings to category (cleaner OHE)\n",
    "for c in hotel.columns:\n",
    "    if str(hotel[c].dtype) in (\"string[pyarrow]\", \"object\"):\n",
    "        vals = set(pd.Series(hotel[c]).dropna().unique())\n",
    "        if vals.issubset({\"True\",\"False\",\"Yes\",\"No\",\"TRUE\",\"FALSE\"}):\n",
    "            hotel[c] = hotel[c].astype(\"category\")\n",
    "\n",
    "assert \"is_canceled\" in hotel.columns, \"Expected 'is_canceled' in hotel dataset.\"\n",
    "y_h = hotel[\"is_canceled\"].astype(\"int32\")\n",
    "X_h = hotel.drop(columns=[\"is_canceled\"])\n",
    "\n",
    "print(\"[Hotel] Stratified splits (70/15/15)…\")\n",
    "Xh_tr, Xh_tmp, yh_tr, yh_tmp = train_test_split(\n",
    "    X_h, y_h, test_size=0.30, stratify=y_h, random_state=RANDOM_STATE\n",
    ")\n",
    "Xh_va, Xh_te, yh_va, yh_te = train_test_split(\n",
    "    Xh_tmp, yh_tmp, test_size=0.50, stratify=yh_tmp, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pre_hotel = make_preprocessor(\n",
    "    pd.concat([X_h, y_h.rename(\"is_canceled\")], axis=1),\n",
    "    target=\"is_canceled\",\n",
    "    task=\"classification\"\n",
    ")\n",
    "\n",
    "hotel_counts = dict(\n",
    "    tag=\"Hotel\",\n",
    "    raw_rows=int(len(hotel)), cleaned_rows=int(len(X_h)),\n",
    "    train_rows=int(len(Xh_tr)), val_rows=int(len(Xh_va)), test_rows=int(len(Xh_te))\n",
    ")\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"counts.json\"), hotel_counts)\n",
    "print(f\"[Hotel] Counts: {hotel_counts}\")\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# CHUNK 6 — HOTEL: Decision Tree (DT) [R1][R4][R2]\n",
    "# =================================================\n",
    "\n",
    "cv_h = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"[Hotel/DT] Fitting DecisionTreeClassifier…\")\n",
    "dt_h = Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                 (\"clf\", DecisionTreeClassifier(\n",
    "                     criterion=\"gini\",\n",
    "                     max_depth=14,              # guardrails {6,10,14,18} [R2]\n",
    "                     min_samples_leaf=50,       # {50,100,200}\n",
    "                     min_samples_split=200,     # {100,200,400}\n",
    "                     max_features=\"sqrt\",       # {\"sqrt\",\"log2\",0.5}\n",
    "                     random_state=RANDOM_STATE\n",
    "                 ))])\n",
    "\n",
    "with Profiler(\"Hotel_DT_fit\") as prof:\n",
    "    dt_h.fit(Xh_tr, yh_tr)\n",
    "\n",
    "print(\"[Hotel/DT] Predicting & scoring…\")\n",
    "t0 = time.perf_counter()\n",
    "yh_prob_dt = dt_h.predict_proba(Xh_te)[:,1]\n",
    "yh_hat_dt  = dt_h.predict(Xh_te)\n",
    "pred_s = time.perf_counter()-t0\n",
    "\n",
    "m_dt, (fpr_dt, tpr_dt), (rec_dt, prec_dt) = summarize_classification(yh_te, yh_prob_dt, yh_hat_dt)\n",
    "pd.DataFrame([m_dt]).to_csv(os.path.join(OUT_HOTEL, \"dt_metrics.csv\"), index=False)\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_dt), [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: DT Confusion @0.5\", os.path.join(OUT_HOTEL, \"dt_confusion.png\"))\n",
    "plot_line(fpr_dt, tpr_dt, \"Hotel: DT ROC\", \"FPR\", \"TPR\", os.path.join(OUT_HOTEL, \"dt_roc.png\"))\n",
    "plot_line(rec_dt, prec_dt, \"Hotel: DT PR\", \"Recall\", \"Precision\", os.path.join(OUT_HOTEL, \"dt_pr.png\"))\n",
    "plot_learning_curve_single(dt_h, Xh_tr, yh_tr, \"Hotel: DT Learning Curve\",\n",
    "                           os.path.join(OUT_HOTEL, \"dt_learning_curve.png\"), cv=cv_h)\n",
    "plot_reliability(yh_te, yh_prob_dt, \"Hotel: DT Reliability\", os.path.join(OUT_HOTEL, \"dt_reliability.png\"))\n",
    "\n",
    "# Threshold via validation F1\n",
    "print(\"[Hotel/DT] Selecting threshold by val F1…\")\n",
    "yh_prob_val = dt_h.predict_proba(Xh_va)[:,1]\n",
    "thr_dt, bestf1_dt = f1_optimal_threshold(yh_va, yh_prob_val)\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"dt_val_threshold.json\"),\n",
    "               {\"best_val_threshold\": thr_dt, \"val_f1_at_thr\": bestf1_dt})\n",
    "yh_hat_thr = (yh_prob_dt >= thr_dt).astype(int)\n",
    "cm_thr = confusion_matrix(yh_te, yh_hat_thr)\n",
    "plot_confusion(cm_thr, [\"NotCanceled\",\"Canceled\"], \"Hotel: DT Confusion @F1-threshold\",\n",
    "               os.path.join(OUT_HOTEL, \"dt_confusion_f1thr.png\"))\n",
    "\n",
    "# Realized tree stats [R2]\n",
    "tree = dt_h.named_steps[\"clf\"]\n",
    "realized = {\n",
    "    \"depth\": int(tree.get_depth()),\n",
    "    \"leaves\": int(tree.get_n_leaves()),\n",
    "    \"nodes\": int(tree.tree_.node_count),\n",
    "}\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"dt_realized_stats.json\"), realized)\n",
    "\n",
    "# PR-AUC prevalence baseline\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"baseline.json\"),\n",
    "               {\"prevalence_baseline_pr_auc\": prevalence_baseline(yh_te)})\n",
    "\n",
    "# Full classification report (threshold 0.5)\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"dt_classification_report.json\"),\n",
    "               classification_report(yh_te, yh_hat_dt, output_dict=True, zero_division=0))\n",
    "\n",
    "# Runtime record (fit + predict + hardware)\n",
    "save_profile(os.path.join(OUT_HOTEL, \"dt_profile.json\"),\n",
    "             tag=\"Hotel_DT_fit+predict\", seconds_fit=prof.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof.peak_gb, hardware=hw_info())\n",
    "\n",
    "# Model-Complexity curve (vary max_depth) [R2]\n",
    "print(\"[Hotel/DT] Model complexity sweep (max_depth)…\")\n",
    "def model_complexity_curve(pipe_maker, param_name, param_grid, X, y, cv, scorer, out_csv, title, out_png):\n",
    "    rows = []\n",
    "    for val in param_grid:\n",
    "        clf = pipe_maker(val)\n",
    "        scores, tr_scores = [], []\n",
    "        for tr, va in cv.split(X, y):\n",
    "            clf.fit(X.iloc[tr], y.iloc[tr])\n",
    "            yhat_va = clf.predict(X.iloc[va]); scores.append(scorer(y.iloc[va], yhat_va))\n",
    "            yhat_tr = clf.predict(X.iloc[tr]); tr_scores.append(scorer(y.iloc[tr], yhat_tr))\n",
    "        rows.append(dict(param=val, val_score=float(np.mean(scores)), tr_score=float(np.mean(tr_scores))))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    # robust plotting: index on x, labels as strings\n",
    "    x = np.arange(len(df))\n",
    "    labels = df[\"param\"].astype(str).tolist()\n",
    "    plt.figure()\n",
    "    plt.plot(x, df[\"val_score\"], marker=\"o\", label=\"Validation\")\n",
    "    plt.plot(x, df[\"tr_score\"], marker=\"s\", label=\"Training\")\n",
    "    plt.title(title); plt.xlabel(param_name); plt.ylabel(\"Accuracy\"); plt.legend()\n",
    "    plt.xticks(x, labels, rotation=0)\n",
    "    plt.tight_layout(); plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def dt_maker(max_depth):\n",
    "    return Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                     (\"clf\", DecisionTreeClassifier(max_depth=int(max_depth),\n",
    "                                                   min_samples_leaf=50, min_samples_split=200,\n",
    "                                                   random_state=RANDOM_STATE))])\n",
    "model_complexity_curve(dt_maker, \"max_depth\", [6,10,14,18], Xh_tr, yh_tr, cv_h,\n",
    "                       scorer=accuracy_score,\n",
    "                       out_csv=os.path.join(OUT_HOTEL,\"mc_dt.csv\"),\n",
    "                       title=\"Hotel: DT Model-Complexity (max_depth)\",\n",
    "                       out_png=os.path.join(OUT_HOTEL,\"mc_dt.png\"))\n",
    "\n",
    "# Permutation importances (top-10 by F1)\n",
    "imp_dt = permutation_importance_topk(dt_h, Xh_te, yh_te, scoring=\"f1\", k=10, n_repeats=10)\n",
    "imp_dt.to_csv(os.path.join(OUT_HOTEL, \"dt_perm_importance_top10.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a08cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hotel/kNN] Fitting exact kNN (brute)…\n",
      "[Profiler] START: Hotel_kNN_fit\n",
      "[Profiler] END: Hotel_kNN_fit | sec=0.27 | peakGB=1.031\n",
      "[Plot] Learning curve: Hotel: kNN Learning Curve\n",
      "[Plot] Reliability: Hotel: kNN Reliability\n",
      "[Hotel/kNN] Model complexity sweep (k)…\n",
      "[Hotel/SVM-Lin] Fitting LinearSVC (+calibration)…\n",
      "[Profiler] START: Hotel_LinSVM_fit\n",
      "[Profiler] END: Hotel_LinSVM_fit | sec=1.83 | peakGB=1.144\n",
      "[Plot] Learning curve: Hotel: Linear SVM Learning Curve\n",
      "[Plot] Reliability: Hotel: Linear SVM Reliability\n",
      "[Hotel/SVM-Lin] Selecting threshold by val F1 (calibrated probs)…\n",
      "[Hotel/SVM-Lin] Model complexity sweep (C)…\n",
      "[Hotel/SVM-Lin] Margin diagnostic (LinearSVC on full train)…\n",
      "[Hotel/SVM-RBF] Subsampling train to ≤25k for RBF (nonlinear check)…\n",
      "[Profiler] START: Hotel_RBFSVM_fit\n",
      "[Profiler] END: Hotel_RBFSVM_fit | sec=12.50 | peakGB=2.223\n",
      "[Hotel/SVM-RBF] Scoring using decision_function (no probability calibration)…\n",
      "[Hotel/SVM-RBF] Learning curve on 25k subset…\n",
      "[Plot] Learning curve: Hotel: RBF SVM Learning Curve (25k subset)\n",
      "[Hotel/SVM-RBF] Model complexity sweep (gamma)…\n",
      "[Profiler] START: Hotel_MLP_fit\n",
      "[Hotel/MLP] Epoch-by-epoch (SGD-only, ≤15 epochs, early stopping)…\n",
      "[Profiler] END: Hotel_MLP_fit | sec=20.56 | peakGB=1.837\n",
      "[Hotel/MLP] Selecting threshold by val F1…\n",
      "[Plot] Learning curve: Hotel: MLP Learning Curve\n",
      "[Plot] Reliability: Hotel: MLP Reliability\n",
      "[NN-Act] Starting activation study on Hotel (SGD only)…\n",
      "[NN-Act] Training activation='relu' with SGD for 15 epochs…\n",
      "[NN-Act] relu: epoch  1 | val_logloss=0.6133 | val_acc=0.6621\n",
      "[NN-Act] relu: epoch  5 | val_logloss=0.5126 | val_acc=0.7657\n",
      "[NN-Act] relu: epoch 10 | val_logloss=0.4580 | val_acc=0.7958\n",
      "[NN-Act] relu: epoch 15 | val_logloss=0.4335 | val_acc=0.8072\n",
      "[NN-Act] Saved curves for relu → outputs/hotel_cls/nn_activation_study/epochs_relu.csv / outputs/hotel_cls/nn_activation_study/epochs_relu.png\n",
      "[NN-Act] Training activation='tanh' with SGD for 15 epochs…\n",
      "[NN-Act] tanh: epoch  1 | val_logloss=0.5387 | val_acc=0.7506\n",
      "[NN-Act] tanh: epoch  5 | val_logloss=0.4533 | val_acc=0.7999\n",
      "[NN-Act] tanh: epoch 10 | val_logloss=0.4284 | val_acc=0.8092\n",
      "[NN-Act] tanh: epoch 15 | val_logloss=0.4153 | val_acc=0.8120\n",
      "[NN-Act] Saved curves for tanh → outputs/hotel_cls/nn_activation_study/epochs_tanh.csv / outputs/hotel_cls/nn_activation_study/epochs_tanh.png\n",
      "[NN-Act] Training activation='logistic' with SGD for 15 epochs…\n",
      "[NN-Act] logistic: epoch  1 | val_logloss=0.6588 | val_acc=0.6296\n",
      "[NN-Act] logistic: epoch  5 | val_logloss=0.6581 | val_acc=0.6296\n",
      "[NN-Act] logistic: epoch 10 | val_logloss=0.6572 | val_acc=0.6296\n",
      "[NN-Act] logistic: epoch 15 | val_logloss=0.6563 | val_acc=0.6296\n",
      "[NN-Act] Saved curves for logistic → outputs/hotel_cls/nn_activation_study/epochs_logistic.csv / outputs/hotel_cls/nn_activation_study/epochs_logistic.png\n",
      "[NN-Act] Training activation='identity' with SGD for 15 epochs…\n",
      "[NN-Act] identity: epoch  1 | val_logloss=0.5183 | val_acc=0.7636\n",
      "[NN-Act] identity: epoch  5 | val_logloss=0.4435 | val_acc=0.8058\n",
      "[NN-Act] identity: epoch 10 | val_logloss=0.4226 | val_acc=0.8110\n",
      "[NN-Act] identity: epoch 15 | val_logloss=0.4116 | val_acc=0.8134\n",
      "[NN-Act] Saved curves for identity → outputs/hotel_cls/nn_activation_study/epochs_identity.csv / outputs/hotel_cls/nn_activation_study/epochs_identity.png\n",
      "[NN-Act] Wrote comparison table → outputs/hotel_cls/nn_activation_study/activation_comparison.csv\n",
      "[NN-Act] Activation study complete.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CHUNK 7 — HOTEL: kNN + Linear/RBF SVM [R5][R6][R2]\n",
    "# =========================================================\n",
    "\n",
    "# kNN (exact, brute) + learning curve + MC(k)\n",
    "print(\"[Hotel/kNN] Fitting exact kNN (brute)…\")\n",
    "knn_h = Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                  (\"clf\", KNeighborsClassifier(n_neighbors=11, algorithm=\"brute\", n_jobs=-1))])\n",
    "\n",
    "with Profiler(\"Hotel_kNN_fit\") as prof_knn:\n",
    "    knn_h.fit(Xh_tr, yh_tr)\n",
    "t0 = time.perf_counter()\n",
    "yh_prob_knn = knn_h.predict_proba(Xh_te)[:,1]\n",
    "yh_hat_knn  = knn_h.predict(Xh_te)\n",
    "pred_s = time.perf_counter()-t0\n",
    "m_knn, (fpr_knn, tpr_knn), (rec_knn, prec_knn) = summarize_classification(yh_te, yh_prob_knn, yh_hat_knn)\n",
    "pd.DataFrame([m_knn]).to_csv(os.path.join(OUT_HOTEL, \"knn_metrics.csv\"), index=False)\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_knn), [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: kNN Confusion @0.5\", os.path.join(OUT_HOTEL, \"knn_confusion.png\"))\n",
    "plot_line(fpr_knn, tpr_knn, \"Hotel: kNN ROC\", \"FPR\", \"TPR\", os.path.join(OUT_HOTEL, \"knn_roc.png\"))\n",
    "plot_line(rec_knn, prec_knn, \"Hotel: kNN PR\", \"Recall\", \"Precision\", os.path.join(OUT_HOTEL, \"knn_pr.png\"))\n",
    "plot_learning_curve_single(knn_h, Xh_tr, yh_tr, \"Hotel: kNN Learning Curve\",\n",
    "                           os.path.join(OUT_HOTEL, \"knn_learning_curve.png\"), cv=cv_h)\n",
    "plot_reliability(yh_te, yh_prob_knn, \"Hotel: kNN Reliability\", os.path.join(OUT_HOTEL, \"knn_reliability.png\"))\n",
    "save_profile(os.path.join(OUT_HOTEL, \"knn_profile.json\"),\n",
    "             tag=\"Hotel_kNN_fit+predict\", seconds_fit=prof_knn.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_knn.peak_gb, hardware=hw_info())\n",
    "\n",
    "# F1-thresholding for kNN\n",
    "yh_prob_knn_val = knn_h.predict_proba(Xh_va)[:, 1]\n",
    "thr_knn, bestf1_knn = f1_optimal_threshold(yh_va, yh_prob_knn_val)\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"knn_val_threshold.json\"),\n",
    "               {\"best_val_threshold\": thr_knn, \"val_f1_at_thr\": bestf1_knn})\n",
    "yh_hat_knn_thr = (yh_prob_knn >= thr_knn).astype(int)\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_knn_thr),\n",
    "               [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: kNN Confusion @F1-threshold\",\n",
    "               os.path.join(OUT_HOTEL, \"knn_confusion_f1thr.png\"))\n",
    "\n",
    "# MC for kNN: vary k\n",
    "print(\"[Hotel/kNN] Model complexity sweep (k)…\")\n",
    "def knn_maker(k):\n",
    "    return Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                     (\"clf\", KNeighborsClassifier(n_neighbors=int(k), algorithm=\"brute\", n_jobs=-1))])\n",
    "model_complexity_curve(knn_maker, \"k\", [3,5,11,21], Xh_tr, yh_tr, cv_h,\n",
    "                       scorer=accuracy_score,\n",
    "                       out_csv=os.path.join(OUT_HOTEL,\"mc_knn.csv\"),\n",
    "                       title=\"Hotel: kNN Model-Complexity (k)\",\n",
    "                       out_png=os.path.join(OUT_HOTEL,\"mc_knn.png\"))\n",
    "\n",
    "# Linear SVM (+ calibrated probabilities)\n",
    "print(\"[Hotel/SVM-Lin] Fitting LinearSVC (+calibration)…\")\n",
    "lin_base = LinearSVC(C=1.0, dual=False, max_iter=20000, tol=1e-3, random_state=RANDOM_STATE)\n",
    "lin_svm  = Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                     (\"cal\", CalibratedClassifierCV(lin_base, cv=3, n_jobs=-1))])\n",
    "\n",
    "with Profiler(\"Hotel_LinSVM_fit\") as prof_lsvm:\n",
    "    lin_svm.fit(Xh_tr, yh_tr)\n",
    "t0 = time.perf_counter()\n",
    "yh_prob_lsvm = lin_svm.predict_proba(Xh_te)[:,1]\n",
    "yh_hat_lsvm  = lin_svm.predict(Xh_te)\n",
    "pred_s = time.perf_counter()-t0\n",
    "m_lsvm, (fpr_lsvm, tpr_lsvm), (rec_lsvm, prec_lsvm) = summarize_classification(yh_te, yh_prob_lsvm, yh_hat_lsvm)\n",
    "pd.DataFrame([m_lsvm]).to_csv(os.path.join(OUT_HOTEL, \"linsvm_metrics.csv\"), index=False)\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_lsvm), [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: Linear SVM Confusion @0.5\", os.path.join(OUT_HOTEL, \"linsvm_confusion.png\"))\n",
    "plot_line(fpr_lsvm, tpr_lsvm, \"Hotel: Linear SVM ROC\", \"FPR\", \"TPR\", os.path.join(OUT_HOTEL, \"linsvm_roc.png\"))\n",
    "plot_line(rec_lsvm, prec_lsvm, \"Hotel: Linear SVM PR\", \"Recall\", \"Precision\", os.path.join(OUT_HOTEL, \"linsvm_pr.png\"))\n",
    "plot_learning_curve_single(lin_svm, Xh_tr, yh_tr, \"Hotel: Linear SVM Learning Curve\",\n",
    "                           os.path.join(OUT_HOTEL, \"linsvm_learning_curve.png\"), cv=cv_h)\n",
    "plot_reliability(yh_te, yh_prob_lsvm, \"Hotel: Linear SVM Reliability\", os.path.join(OUT_HOTEL, \"linsvm_reliability.png\"))\n",
    "save_profile(os.path.join(OUT_HOTEL, \"linsvm_profile.json\"),\n",
    "             tag=\"Hotel_LinSVM_fit+predict\", seconds_fit=prof_lsvm.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_lsvm.peak_gb, hardware=hw_info())\n",
    "\n",
    "# Threshold by validation F1\n",
    "print(\"[Hotel/SVM-Lin] Selecting threshold by val F1 (calibrated probs)…\")\n",
    "yh_prob_lsvm_val = lin_svm.predict_proba(Xh_va)[:, 1]\n",
    "thr_lsvm, bestf1_lsvm = f1_optimal_threshold(yh_va, yh_prob_lsvm_val)\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"linsvm_val_threshold.json\"),\n",
    "               {\"best_val_threshold\": thr_lsvm, \"val_f1_at_thr\": bestf1_lsvm})\n",
    "yh_hat_lsvm_thr = (yh_prob_lsvm >= thr_lsvm).astype(int)\n",
    "cm_lsvm_thr = confusion_matrix(yh_te, yh_hat_lsvm_thr)\n",
    "plot_confusion(cm_lsvm_thr, [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: Linear SVM Confusion @F1-threshold\",\n",
    "               os.path.join(OUT_HOTEL, \"linsvm_confusion_f1thr.png\"))\n",
    "\n",
    "# Model-complexity sweep over C\n",
    "print(\"[Hotel/SVM-Lin] Model complexity sweep (C)…\")\n",
    "def linsvm_maker(Cval):\n",
    "    base = LinearSVC(C=float(Cval), dual=False, max_iter=20000, tol=1e-3, random_state=RANDOM_STATE)\n",
    "    return Pipeline([(\"pre\", clone(pre_hotel)), (\"lin\", base)])\n",
    "\n",
    "model_complexity_curve(\n",
    "    pipe_maker=linsvm_maker,\n",
    "    param_name=\"C\",\n",
    "    param_grid=[0.1, 1.0, 10.0],\n",
    "    X=Xh_tr, y=yh_tr, cv=cv_h, scorer=accuracy_score,\n",
    "    out_csv=os.path.join(OUT_HOTEL, \"mc_linsvm.csv\"),\n",
    "    title=\"Hotel: Linear SVM Model-Complexity (C)\",\n",
    "    out_png=os.path.join(OUT_HOTEL, \"mc_linsvm.png\")\n",
    ")\n",
    "\n",
    "# Support-vector margin diagnostic\n",
    "print(\"[Hotel/SVM-Lin] Margin diagnostic (LinearSVC on full train)…\")\n",
    "lin_svm_raw = Pipeline([(\"pre\", clone(pre_hotel)), (\"lin\", LinearSVC(C=1.0, dual=\"auto\", max_iter=50000, random_state=RANDOM_STATE))])\n",
    "lin_svm_raw.fit(Xh_tr, yh_tr)\n",
    "margins = lin_svm_raw.decision_function(Xh_te)\n",
    "np.save(os.path.join(OUT_HOTEL,\"linsvm_margins.npy\"), margins)\n",
    "\n",
    "# RBF SVM (cap ≤25k)\n",
    "print(\"[Hotel/SVM-RBF] Subsampling train to ≤25k for RBF (nonlinear check)…\")\n",
    "cap = min(25000, len(Xh_tr))\n",
    "sub_idx = np.random.RandomState(RANDOM_STATE).choice(len(Xh_tr), size=cap, replace=False)\n",
    "Xh_tr_rbf, yh_tr_rbf = Xh_tr.iloc[sub_idx], yh_tr.iloc[sub_idx]\n",
    "\n",
    "rbf_svm = Pipeline([\n",
    "    (\"pre\", clone(pre_hotel)),\n",
    "    (\"clf\", SVC(kernel=\"rbf\", C=2.0, gamma=\"scale\", probability=False,\n",
    "                cache_size=1000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "with Profiler(\"Hotel_RBFSVM_fit\") as prof_rsvm:\n",
    "    rbf_svm.fit(Xh_tr_rbf, yh_tr_rbf)\n",
    "\n",
    "print(\"[Hotel/SVM-RBF] Scoring using decision_function (no probability calibration)…\")\n",
    "t0 = time.perf_counter()\n",
    "scores_rbf = rbf_svm.decision_function(Xh_te)     # continuous scores\n",
    "yh_hat_rbf = (scores_rbf >= 0).astype(int)        # 0-threshold for labels\n",
    "pred_s = time.perf_counter() - t0\n",
    "\n",
    "fpr_rbf, tpr_rbf, _ = roc_curve(yh_te, scores_rbf)\n",
    "roc_auc_rbf = auc(fpr_rbf, tpr_rbf)\n",
    "prec_rbf, reca_rbf, _ = precision_recall_curve(yh_te, scores_rbf)\n",
    "pr_auc_rbf = auc(reca_rbf, prec_rbf)\n",
    "acc_rbf = accuracy_score(yh_te, yh_hat_rbf)\n",
    "prc_rbf, rec_rbf, f1_rbf, _ = precision_recall_fscore_support(yh_te, yh_hat_rbf, average=\"binary\", zero_division=0)\n",
    "\n",
    "m_rbf = {\"accuracy\": float(acc_rbf), \"precision\": float(prc_rbf), \"recall\": float(rec_rbf), \"f1\": float(f1_rbf),\n",
    "         \"roc_auc\": float(roc_auc_rbf), \"pr_auc\": float(pr_auc_rbf)}\n",
    "pd.DataFrame([m_rbf]).to_csv(os.path.join(OUT_HOTEL, \"rbfsvm_metrics.csv\"), index=False)\n",
    "\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_rbf), [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: RBF SVM Confusion @0 threshold\", os.path.join(OUT_HOTEL, \"rbfsvm_confusion.png\"))\n",
    "plot_line(fpr_rbf, tpr_rbf, \"Hotel: RBF SVM ROC\", \"FPR\", \"TPR\", os.path.join(OUT_HOTEL, \"rbfsvm_roc.png\"))\n",
    "plot_line(reca_rbf, prec_rbf, \"Hotel: RBF SVM PR\", \"Recall\", \"Precision\", os.path.join(OUT_HOTEL, \"rbfsvm_pr.png\"))\n",
    "save_profile(os.path.join(OUT_HOTEL, \"rbfsvm_profile.json\"),\n",
    "             tag=\"Hotel_RBFSVM_fit+predict\", seconds_fit=prof_rsvm.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_rsvm.peak_gb, hardware=hw_info())\n",
    "\n",
    "# F1-thresholding on decision_function\n",
    "scores_rbf_val = rbf_svm.decision_function(Xh_va)\n",
    "qs = np.linspace(0.05, 0.95, 19)\n",
    "thr_grid = np.quantile(scores_rbf_val, qs)\n",
    "from sklearn.metrics import f1_score\n",
    "thr_rbf, bestf1_rbf = max(((thr, f1_score(yh_va, (scores_rbf_val >= thr).astype(int)))\n",
    "                            for thr in thr_grid), key=lambda t: t[1])\n",
    "thr_rbf, bestf1_rbf = float(thr_rbf), float(bestf1_rbf)\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"rbfsvm_val_threshold.json\"),\n",
    "               {\"best_val_threshold_on_scores\": thr_rbf, \"val_f1_at_thr\": bestf1_rbf})\n",
    "yh_hat_rbf_thr = (scores_rbf >= thr_rbf).astype(int)\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_rbf_thr),\n",
    "               [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: RBF SVM Confusion @F1-threshold\",\n",
    "               os.path.join(OUT_HOTEL, \"rbfsvm_confusion_f1thr.png\"))\n",
    "\n",
    "# Learning curve on 25k subset\n",
    "print(\"[Hotel/SVM-RBF] Learning curve on 25k subset…\")\n",
    "plot_learning_curve_single(rbf_svm, Xh_tr_rbf, yh_tr_rbf,\n",
    "                           \"Hotel: RBF SVM Learning Curve (25k subset)\",\n",
    "                           os.path.join(OUT_HOTEL, \"rbfsvm_learning_curve.png\"),\n",
    "                           cv=cv_h)\n",
    "\n",
    "# RBF MC sweep (gamma)\n",
    "print(\"[Hotel/SVM-RBF] Model complexity sweep (gamma)…\")\n",
    "def rbf_maker(gamma):\n",
    "    return Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                     (\"clf\", SVC(kernel=\"rbf\", C=2.0, gamma=gamma, probability=True, random_state=RANDOM_STATE))])\n",
    "d = max(1, Xh_tr.shape[1])\n",
    "gammas = [\"scale\", 1/d, 2/d]\n",
    "model_complexity_curve(rbf_maker, \"gamma\", gammas, Xh_tr_rbf, yh_tr_rbf, cv=cv_h,\n",
    "                       scorer=accuracy_score,\n",
    "                       out_csv=os.path.join(OUT_HOTEL,\"mc_rbf.csv\"),\n",
    "                       title=\"Hotel: RBF SVM MC (gamma)\",\n",
    "                       out_png=os.path.join(OUT_HOTEL,\"mc_rbf.png\"))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CHUNK 8a — HOTEL: MLP (SGD-only) + epoch curve [R7][R2]\n",
    "# ==========================================================\n",
    "\n",
    "def train_mlp_sgd_with_epoch_curve(X_tr, y_tr, X_va, y_va, out_csv, out_png, patience=3):\n",
    "    \"\"\"MLPClassifier with SGD only; ≤15 epochs; early stopping marker; param budget log.\"\"\"\n",
    "    print(\"[Hotel/MLP] Epoch-by-epoch (SGD-only, ≤15 epochs, early stopping)…\")\n",
    "    clf = Pipeline([(\"pre\", clone(pre_hotel)),\n",
    "                    (\"clf\", MLPClassifier(hidden_layer_sizes=(512,512),  # shallow–wide\n",
    "                                          solver=\"sgd\", learning_rate_init=0.01,\n",
    "                                          momentum=0.0, nesterovs_momentum=False,\n",
    "                                          batch_size=1024, max_iter=1, random_state=RANDOM_STATE,\n",
    "                                          warm_start=True, alpha=1e-4))])\n",
    "    rows, best = [], {\"epoch\": 0, \"val_logloss\": np.inf}\n",
    "    no_improve = 0\n",
    "    for epoch in range(1, 16):  # ≤15 epochs\n",
    "        clf.fit(X_tr, y_tr)  # one epoch\n",
    "        y_tr_proba = clf.predict_proba(X_tr)[:,1]\n",
    "        y_va_proba = clf.predict_proba(X_va)[:,1]\n",
    "        tr_ll = log_loss(y_tr, y_tr_proba, labels=[0,1])\n",
    "        va_ll = log_loss(y_va, y_va_proba, labels=[0,1])\n",
    "        rows.append({\"epoch\": epoch, \"train_logloss\": float(tr_ll), \"val_logloss\": float(va_ll)})\n",
    "        if va_ll + 1e-6 < best[\"val_logloss\"]:\n",
    "            best.update({\"epoch\": epoch, \"val_logloss\": float(va_ll)}); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"[Hotel/MLP] Early stopping at epoch {epoch} (best={best['epoch']})\")\n",
    "                break\n",
    "    df = pd.DataFrame(rows); df.to_csv(out_csv, index=False)\n",
    "    plt.figure(); plt.plot(df[\"epoch\"], df[\"train_logloss\"], marker=\"o\", label=\"Train\")\n",
    "    plt.plot(df[\"epoch\"], df[\"val_logloss\"], marker=\"s\", label=\"Validation\")\n",
    "    plt.axvline(best[\"epoch\"], linestyle=\"--\", label=f\"best@{best['epoch']}\")\n",
    "    plt.title(\"Hotel: MLP (SGD) Epoch Curve\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"LogLoss\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n",
    "    # param budget audit (should be in 0.2M–1.0M)\n",
    "    n_in = _estimate_input_dim(pre_hotel, X_tr); n_params = _estimate_mlp_params(n_in, (512,512), 1)\n",
    "    save_json_safe(os.path.join(OUT_HOTEL, \"mlp_param_budget.json\"),\n",
    "                   {\"estimated_input_dim\": n_in, \"param_count\": n_params})\n",
    "    return clf\n",
    "\n",
    "with Profiler(\"Hotel_MLP_fit\") as prof_mlp:\n",
    "    mlp_h = train_mlp_sgd_with_epoch_curve(Xh_tr, yh_tr, Xh_va, yh_va,\n",
    "                                           os.path.join(OUT_HOTEL,\"mlp_epochs.csv\"),\n",
    "                                           os.path.join(OUT_HOTEL,\"mlp_epochs.png\"))\n",
    "t0 = time.perf_counter()\n",
    "yh_prob_mlp = mlp_h.predict_proba(Xh_te)[:,1]\n",
    "yh_hat_mlp  = mlp_h.predict(Xh_te)\n",
    "print(\"[Hotel/MLP] Selecting threshold by val F1…\")\n",
    "yh_prob_mlp_val = mlp_h.predict_proba(Xh_va)[:, 1]\n",
    "thr_mlp, bestf1_mlp = f1_optimal_threshold(yh_va, yh_prob_mlp_val)\n",
    "save_json_safe(os.path.join(OUT_HOTEL, \"mlp_val_threshold.json\"),\n",
    "               {\"best_val_threshold\": thr_mlp, \"val_f1_at_thr\": bestf1_mlp})\n",
    "yh_hat_mlp_thr = (yh_prob_mlp >= thr_mlp).astype(int)\n",
    "cm_mlp_thr = confusion_matrix(yh_te, yh_hat_mlp_thr)\n",
    "plot_confusion(cm_mlp_thr, [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: MLP Confusion @F1-threshold\",\n",
    "               os.path.join(OUT_HOTEL, \"mlp_confusion_f1thr.png\"))\n",
    "\n",
    "pred_s = time.perf_counter()-t0\n",
    "m_mlp, (fpr_mlp, tpr_mlp), (rec_mlp, prec_mlp) = summarize_classification(yh_te, yh_prob_mlp, yh_hat_mlp)\n",
    "pd.DataFrame([m_mlp]).to_csv(os.path.join(OUT_HOTEL, \"mlp_metrics.csv\"), index=False)\n",
    "plot_confusion(confusion_matrix(yh_te, yh_hat_mlp), [\"NotCanceled\",\"Canceled\"],\n",
    "               \"Hotel: MLP Confusion @0.5\", os.path.join(OUT_HOTEL, \"mlp_confusion.png\"))\n",
    "plot_line(fpr_mlp, tpr_mlp, \"Hotel: MLP ROC\", \"FPR\", \"TPR\", os.path.join(OUT_HOTEL, \"mlp_roc.png\"))\n",
    "plot_line(rec_mlp, prec_mlp, \"Hotel: MLP PR\", \"Recall\", \"Precision\", os.path.join(OUT_HOTEL, \"mlp_pr.png\"))\n",
    "plot_learning_curve_single(mlp_h, Xh_tr, yh_tr, \"Hotel: MLP Learning Curve\",\n",
    "                           os.path.join(OUT_HOTEL, \"mlp_learning_curve.png\"), cv=cv_h)\n",
    "plot_reliability(yh_te, yh_prob_mlp, \"Hotel: MLP Reliability\", os.path.join(OUT_HOTEL, \"mlp_reliability.png\"))\n",
    "save_profile(os.path.join(OUT_HOTEL, \"mlp_profile.json\"),\n",
    "             tag=\"Hotel_MLP_fit+predict\", seconds_fit=prof_mlp.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_mlp.peak_gb, hardware=hw_info())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# CHUNK 8b — NN Activation Study (Hotel; SGD only)\n",
    "# =========================================================\n",
    "\n",
    "ACT_DIR = os.path.join(OUT_HOTEL, \"nn_activation_study\")\n",
    "os.makedirs(ACT_DIR, exist_ok=True)\n",
    "\n",
    "def train_mlp_sgd_epoch_curve_activation(\n",
    "    activation: str,\n",
    "    X_tr, y_tr, X_va, y_va,\n",
    "    hidden=(256, 256),        # fixed architecture across activations (shallow–wide)\n",
    "    alpha=1e-4,               # fixed regularization\n",
    "    lr=0.01,                  # fixed learning rate\n",
    "    batch_size=1024,          # fixed batch size\n",
    "    max_epochs=15,            # ≤15\n",
    "    random_state=RANDOM_STATE\n",
    "):\n",
    "    print(f\"[NN-Act] Training activation='{activation}' with SGD for {max_epochs} epochs…\")\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", clone(pre_hotel)),\n",
    "        (\"clf\", MLPClassifier(\n",
    "            hidden_layer_sizes=hidden,\n",
    "            activation=activation,\n",
    "            solver=\"sgd\",\n",
    "            learning_rate_init=lr,\n",
    "            learning_rate=\"constant\",\n",
    "            momentum=0.0, nesterovs_momentum=False,\n",
    "            alpha=alpha,\n",
    "            batch_size=batch_size,\n",
    "            max_iter=1,\n",
    "            warm_start=True,\n",
    "            random_state=random_state,\n",
    "            early_stopping=False\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    rows = []\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        pipe.fit(X_tr, y_tr)  # one epoch\n",
    "        p_tr = pipe.predict_proba(X_tr)[:, 1]\n",
    "        p_va = pipe.predict_proba(X_va)[:, 1]\n",
    "        yhat_tr = (p_tr >= 0.5).astype(int)\n",
    "        yhat_va = (p_va >= 0.5).astype(int)\n",
    "        tr_ll = log_loss(y_tr, p_tr, labels=[0, 1])\n",
    "        va_ll = log_loss(y_va, p_va, labels=[0, 1])\n",
    "        tr_acc = accuracy_score(y_tr, yhat_tr)\n",
    "        va_acc = accuracy_score(y_va, yhat_va)\n",
    "        rows.append({\"epoch\": epoch, \"activation\": activation,\n",
    "                     \"train_logloss\": float(tr_ll), \"val_logloss\": float(va_ll),\n",
    "                     \"train_acc\": float(tr_acc), \"val_acc\": float(va_acc)})\n",
    "        if epoch in (1, 5, 10, 15):\n",
    "            print(f\"[NN-Act] {activation}: epoch {epoch:>2} | val_logloss={va_ll:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.join(ACT_DIR, f\"epochs_{activation}.csv\")\n",
    "    png_path = os.path.join(ACT_DIR, f\"epochs_{activation}.png\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df[\"epoch\"], df[\"val_logloss\"], marker=\"o\", label=f\"{activation} (val logloss)\")\n",
    "    plt.plot(df[\"epoch\"], df[\"train_logloss\"], marker=\"s\", label=f\"{activation} (train logloss)\")\n",
    "    plt.title(f\"Hotel NN (SGD) — Epoch Curve — {activation}\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"LogLoss\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(png_path, bbox_inches=\"tight\"); plt.close()\n",
    "    print(f\"[NN-Act] Saved curves for {activation} → {csv_path} / {png_path}\")\n",
    "    return pipe, df\n",
    "\n",
    "def activation_study_hotel(X_tr, y_tr, X_va, y_va, X_te, y_te):\n",
    "    activations = [\"relu\", \"tanh\", \"logistic\", \"identity\"]\n",
    "    results = []\n",
    "    for act in activations:\n",
    "        model, _df = train_mlp_sgd_epoch_curve_activation(\n",
    "            activation=act, X_tr=X_tr, y_tr=y_tr, X_va=X_va, y_va=y_va,\n",
    "            hidden=(256, 256), alpha=1e-4, lr=0.01, batch_size=1024, max_epochs=15\n",
    "        )\n",
    "        p_te = model.predict_proba(X_te)[:, 1]\n",
    "        yhat_te = (p_te >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_te, yhat_te)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_te, yhat_te, average=\"binary\", zero_division=0)\n",
    "        fpr, tpr, _ = roc_curve(y_te, p_te); roc_auc = auc(fpr, tpr)\n",
    "        prec_curve, reca_curve, _ = precision_recall_curve(y_te, p_te); pr_auc = auc(reca_curve, prec_curve)\n",
    "        results.append({\"activation\": act, \"test_accuracy\": float(acc), \"test_precision\": float(prec),\n",
    "                        \"test_recall\": float(rec), \"test_f1\": float(f1),\n",
    "                        \"test_roc_auc\": float(roc_auc), \"test_pr_auc\": float(pr_auc)})\n",
    "\n",
    "        plt.figure(); plt.plot(fpr, tpr, label=f\"{act} (AUC={roc_auc:.3f})\")\n",
    "        plt.title(f\"Hotel NN — ROC ({act})\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(ACT_DIR, f\"roc_{act}.png\"), bbox_inches=\"tight\"); plt.close()\n",
    "        plt.figure(); plt.plot(reca_curve, prec_curve, label=f\"{act} (PR AUC={pr_auc:.3f})\")\n",
    "        plt.title(f\"Hotel NN — PR ({act})\"); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(ACT_DIR, f\"pr_{act}.png\"), bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    comp = pd.DataFrame(results).sort_values(\"test_roc_auc\", ascending=False)\n",
    "    comp_path = os.path.join(ACT_DIR, \"activation_comparison.csv\")\n",
    "    comp.to_csv(comp_path, index=False)\n",
    "    print(f\"[NN-Act] Wrote comparison table → {comp_path}\")\n",
    "\n",
    "    # overlays\n",
    "    plt.figure()\n",
    "    for act in activations:\n",
    "        df = pd.read_csv(os.path.join(ACT_DIR, f\"epochs_{act}.csv\"))\n",
    "        plt.plot(df[\"epoch\"], df[\"val_logloss\"], marker=\"o\", label=f\"{act}\")\n",
    "    plt.title(\"Hotel NN (SGD) — Validation LogLoss by Activation\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Val LogLoss\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(ACT_DIR, \"epochs_overlay_val_logloss.png\"), bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    for act in activations:\n",
    "        df = pd.read_csv(os.path.join(ACT_DIR, f\"epochs_{act}.csv\"))\n",
    "        plt.plot(df[\"epoch\"], df[\"val_acc\"], marker=\"s\", label=f\"{act}\")\n",
    "    plt.title(\"Hotel NN (SGD) — Validation Accuracy by Activation\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Accuracy\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(ACT_DIR, \"epochs_overlay_val_acc.png\"), bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"[NN-Act] Starting activation study on Hotel (SGD only)…\")\n",
    "activation_study_hotel(Xh_tr, yh_tr, Xh_va, yh_va, Xh_te, yh_te)\n",
    "print(\"[NN-Act] Activation study complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d41483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN-MC] Candidate widths within 0.2M–1.0M params:\n",
      "  w= 448 → ~232,065 params (n_in≈67)\n",
      "  w= 512 → ~297,985 params (n_in≈67)\n",
      "  w= 640 → ~454,401 params (n_in≈67)\n",
      "  w= 768 → ~643,585 params (n_in≈67)\n",
      "  w= 896 → ~865,537 params (n_in≈67)\n",
      "[NN-MC] Wrote: outputs/hotel_cls/nn_width_sweep/mc_mlp_width.csv\n",
      "[NN-MC] Saved plot → outputs/hotel_cls/nn_width_sweep/mc_mlp_width.png\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ==========================================================\n",
    "# CHUNK 8c — HOTEL: NN (SGD) model‑complexity curve — width sweep\n",
    "# Keeps ≤15 epochs; annotates param count; uses same preprocessing.\n",
    "# ==========================================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "WIDTH_DIR = os.path.join(OUT_HOTEL, \"nn_width_sweep\")\n",
    "os.makedirs(WIDTH_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Pick candidate widths for a 2‑layer MLP [w, w], then filter by param budget.\n",
    "n_in = _estimate_input_dim(pre_hotel, Xh_tr)\n",
    "PARAM_MIN, PARAM_MAX = 200_000, 1_000_000  # spec budget (0.2M–1.0M)\n",
    "\n",
    "candidates = [128, 192, 256, 320, 384, 448, 512, 640, 768, 896, 1024]\n",
    "width_grid, param_map = [], {}\n",
    "for w in candidates:\n",
    "    p = _estimate_mlp_params(n_in, (w, w), 1)  # binary head\n",
    "    if PARAM_MIN <= p <= PARAM_MAX:\n",
    "        width_grid.append(w)\n",
    "        param_map[w] = p\n",
    "\n",
    "print(\"[NN-MC] Candidate widths within 0.2M–1.0M params:\")\n",
    "for w in width_grid:\n",
    "    print(f\"  w={w:>4} → ~{param_map[w]:,} params (n_in≈{n_in})\")\n",
    "\n",
    "# 2) Factory for the pipeline at a given width.\n",
    "#    Activation: choose one and hold it fixed for the sweep.\n",
    "#    (Use the best from your activation study; 'tanh' is a good default for tabular.)\n",
    "def mlp_width_maker(w, activation=\"tanh\"):\n",
    "    return Pipeline([\n",
    "        (\"pre\", clone(pre_hotel)),\n",
    "        (\"dense32\", FunctionTransformer(to_dense32, accept_sparse=True)),\n",
    "        (\"clf\", MLPClassifier(\n",
    "            hidden_layer_sizes=(int(w), int(w)),\n",
    "            activation=activation,\n",
    "            solver=\"sgd\", learning_rate=\"constant\", learning_rate_init=0.01,\n",
    "            momentum=0.0, nesterovs_momentum=False, alpha=1e-4,\n",
    "            batch_size=1024,\n",
    "            max_iter=15,                # ≤ 15 epochs per spec\n",
    "            early_stopping=False,       # we do CV outside\n",
    "            tol=1e-4,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=False\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# 3) Evaluate each width with 3‑fold StratifiedKFold (cv_h defined earlier).\n",
    "rows = []\n",
    "for w in width_grid:\n",
    "    pipe = mlp_width_maker(w, activation=\"tanh\")  # or \"identity\"/\"relu\"/\"logistic\" if you prefer\n",
    "    tr_scores, va_scores = [], []\n",
    "    for tr_idx, va_idx in cv_h.split(Xh_tr, yh_tr):\n",
    "        Xtr, Xva = Xh_tr.iloc[tr_idx], Xh_tr.iloc[va_idx]\n",
    "        ytr, yva = yh_tr.iloc[tr_idx], yh_tr.iloc[va_idx]\n",
    "        pipe.fit(Xtr, ytr)\n",
    "        yhat_tr = pipe.predict(Xtr)\n",
    "        yhat_va = pipe.predict(Xva)\n",
    "        tr_scores.append(accuracy_score(ytr, yhat_tr))\n",
    "        va_scores.append(accuracy_score(yva, yhat_va))\n",
    "    rows.append({\n",
    "        \"width\": int(w),\n",
    "        \"train_accuracy\": float(np.mean(tr_scores)),\n",
    "        \"val_accuracy\": float(np.mean(va_scores)),\n",
    "        \"params\": int(param_map[w])\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"width\")\n",
    "csv_path = os.path.join(WIDTH_DIR, \"mc_mlp_width.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"[NN-MC] Wrote: {csv_path}\")\n",
    "\n",
    "# 4) Plot (training + validation). Annotate each point with param count (in M).\n",
    "plt.figure()\n",
    "plt.plot(df[\"width\"], df[\"val_accuracy\"], marker=\"o\", label=\"Validation\")\n",
    "plt.plot(df[\"width\"], df[\"train_accuracy\"], marker=\"s\", label=\"Training\")\n",
    "for w, va, p in zip(df[\"width\"], df[\"val_accuracy\"], df[\"params\"]):\n",
    "    plt.text(w, va, f\"{p/1e6:.2f}M\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "plt.title(\"Hotel: NN (SGD) Model‑Complexity — width (two‑layer [w,w])\")\n",
    "plt.xlabel(\"Hidden width w\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "out_png = os.path.join(WIDTH_DIR, \"mc_mlp_width.png\")\n",
    "plt.tight_layout(); plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n",
    "print(f\"[NN-MC] Saved plot → {out_png}\")\n",
    "\n",
    "# Optional: if you prefer F1 for the y‑axis, replace accuracy_score with F1 on (predict ≥ 0.5),\n",
    "# or compute ROC‑AUC/PR‑AUC using predict_proba and change the metric accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d096bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Accidents] Streaming columns with Polars…\n",
      "[Accidents] Selecting columns=17; streaming up to 1,000,000 rows…\n",
      "[Accidents] Loaded rows=1,000,000, cols=21\n",
      "[Accidents] Counts: {'tag': 'Accidents', 'raw_rows': 1000000, 'cleaned_rows': 1000000, 'train_rows': 700000, 'val_rows': 150000, 'test_rows': 150000}\n",
      "[Accidents/DTR] Fitting DecisionTreeRegressor…\n",
      "[Profiler] START: Acc_DTR_fit\n",
      "[Profiler] END: Acc_DTR_fit | sec=11.41 | peakGB=3.488\n",
      "[Plot] Parity: Accidents: DTR Parity\n",
      "[Plot] Residuals: Accidents: DTR Residuals\n",
      "[Plot] Learning curve: Accidents: DTR Learning Curve\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# CHUNK 9 — ACCIDENTS load + time holdout + preprocessor\n",
    "# =======================================================\n",
    "\n",
    "print(\"[Accidents] Streaming columns with Polars…\")\n",
    "PREFER_ACC = [\n",
    "    \"Start_Time\",\"End_Time\",\n",
    "    \"Distance(mi)\",\"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\n",
    "    \"Visibility(mi)\",\"Wind_Speed(mph)\",\n",
    "    \"Crossing\",\"Junction\",\"Traffic_Signal\",\n",
    "    \"Sunrise_Sunset\",\"Civil_Twilight\",\"Amenity\",\"Bump\",\"No_Exit\",\"Side\",\n",
    "    \"State\"\n",
    "]\n",
    "\n",
    "head_cols = pl.read_csv(ACC_CSV, n_rows=5).columns\n",
    "use_cols = [c for c in PREFER_ACC if c in head_cols]\n",
    "\n",
    "sample_rows = 1000000\n",
    "print(f\"[Accidents] Selecting columns={len(use_cols)}; streaming up to {sample_rows:,} rows…\")\n",
    "\n",
    "# Build a lazy plan:\n",
    "acc_pl = (\n",
    "    pl.scan_csv(ACC_CSV, infer_schema_length=5000, null_values=CSV_NULLS)\n",
    "      .select([pl.col(c) for c in use_cols])\n",
    "      # Parse datetimes for time-based splitting & duration target\n",
    "      .with_columns(\n",
    "          pl.col(\"Start_Time\").str.strptime(pl.Datetime, strict=False).alias(\"_start_dt\"),\n",
    "          pl.col(\"End_Time\").str.strptime(pl.Datetime, strict=False).alias(\"_end_dt\"),\n",
    "      )\n",
    "      # Cast known numeric columns to floats (robust to parse errors)\n",
    "      .with_columns([\n",
    "          pl.col(c).cast(pl.Float64, strict=False)\n",
    "          for c in [\"Distance(mi)\",\"Temperature(F)\",\"Humidity(%)\",\"Pressure(in)\",\n",
    "                    \"Visibility(mi)\",\"Wind_Speed(mph)\"]\n",
    "          if c in use_cols\n",
    "      ])\n",
    "      # Create duration target (minutes), clipped to [1, 1440], and sortable timestamp\n",
    "      .with_columns(\n",
    "          ((pl.col(\"_end_dt\") - pl.col(\"_start_dt\")).dt.total_seconds() / 60.0)\n",
    "            .pipe(lambda s: pl.when(s < 1.0).then(1.0)\n",
    "                   .when(s > 24.0*60.0).then(24.0*60.0)\n",
    "                   .otherwise(s))\n",
    "            .alias(\"duration_minutes\"),\n",
    "          pl.col(\"_start_dt\").dt.epoch(\"s\").alias(\"_start_ts\")\n",
    "      )\n",
    "      .head(sample_rows)  # first N rows efficiently\n",
    ")\n",
    "\n",
    "# Materialize\n",
    "acc_df = acc_pl.collect()\n",
    "print(f\"[Accidents] Loaded rows={acc_df.height:,}, cols={acc_df.width}\")\n",
    "\n",
    "# Convert to pandas + clean NA\n",
    "acc = acc_df.to_pandas(use_pyarrow_extension_array=False).replace({pd.NA: np.nan})\n",
    "\n",
    "# Keep rows with valid duration\n",
    "acc = acc.loc[acc[\"duration_minutes\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# Sort by time and split 70/15/15 (time-aware holdout)\n",
    "acc[\"_start_ts\"] = pd.to_datetime(acc[\"_start_dt\"]).astype(\"int64\")  # ns since epoch\n",
    "acc_sorted = acc.sort_values(\"_start_ts\").reset_index(drop=True)\n",
    "n = len(acc_sorted)\n",
    "n_test = int(0.15 * n); n_val = int(0.15 * n)\n",
    "\n",
    "acc_tr = acc_sorted.iloc[: n - (n_val + n_test)]\n",
    "acc_va = acc_sorted.iloc[n - (n_val + n_test) : n - n_test]\n",
    "acc_te = acc_sorted.iloc[n - n_test : ]\n",
    "\n",
    "# Features/targets; drop time columns after splitting (avoid leakage)\n",
    "drop_time_cols = [c for c in [\"Start_Time\",\"End_Time\",\"_start_dt\",\"_end_dt\",\"_start_ts\"] if c in acc_tr.columns]\n",
    "\n",
    "X_acc_tr = acc_tr.drop(columns=drop_time_cols + [\"duration_minutes\"])\n",
    "y_acc_tr = acc_tr[\"duration_minutes\"].astype(\"float32\")\n",
    "X_acc_va = acc_va.drop(columns=drop_time_cols + [\"duration_minutes\"])\n",
    "y_acc_va = acc_va[\"duration_minutes\"].astype(\"float32\")\n",
    "X_acc_te = acc_te.drop(columns=drop_time_cols + [\"duration_minutes\"])\n",
    "y_acc_te = acc_te[\"duration_minutes\"].astype(\"float32\")\n",
    "\n",
    "# Small evaluation subset for heavy predictors (≤25k) within the test time block\n",
    "eval_cap = min(25000, len(X_acc_te))\n",
    "X_acc_te_eval = X_acc_te.iloc[:eval_cap]\n",
    "y_acc_te_eval = y_acc_te.iloc[:eval_cap]\n",
    "save_json_safe(os.path.join(OUT_ACC, \"eval_subset.json\"),\n",
    "               {\"eval_cap\": int(eval_cap), \"test_total\": int(len(X_acc_te))})\n",
    "\n",
    "# Keep full X/y for schema\n",
    "X_acc = acc_sorted.drop(columns=drop_time_cols + [\"duration_minutes\"])\n",
    "y_acc = acc_sorted[\"duration_minutes\"].astype(\"float32\")\n",
    "\n",
    "# Preprocessor for regression\n",
    "pre_acc = make_preprocessor(\n",
    "    pd.concat([X_acc, y_acc.rename(\"duration_minutes\")], axis=1),\n",
    "    target=\"duration_minutes\",\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "acc_counts = dict(\n",
    "    tag=\"Accidents\",\n",
    "    raw_rows=int(len(acc_df)),\n",
    "    cleaned_rows=int(len(acc_sorted)),\n",
    "    train_rows=int(len(X_acc_tr)),\n",
    "    val_rows=int(len(X_acc_va)),\n",
    "    test_rows=int(len(X_acc_te)),\n",
    ")\n",
    "save_json_safe(os.path.join(OUT_ACC, \"counts.json\"), acc_counts)\n",
    "print(f\"[Accidents] Counts: {acc_counts}\")\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# CHUNK 10 — ACCIDENTS: Decision Tree Regressor [R4][R2]\n",
    "# ========================================================\n",
    "\n",
    "cv_reg = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"[Accidents/DTR] Fitting DecisionTreeRegressor…\")\n",
    "dtr = Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                (\"reg\", DecisionTreeRegressor(\n",
    "                    max_depth=18,            # cap for large set [R2]\n",
    "                    min_samples_leaf=100,\n",
    "                    min_samples_split=200,\n",
    "                    random_state=RANDOM_STATE\n",
    "                ))])\n",
    "\n",
    "with Profiler(\"Acc_DTR_fit\") as prof_dtr:\n",
    "    dtr.fit(X_acc_tr, y_acc_tr)\n",
    "t0 = time.perf_counter(); yhat_dtr = dtr.predict(X_acc_te); pred_s = time.perf_counter()-t0\n",
    "m_dtr = summarize_regression(y_acc_te, yhat_dtr)\n",
    "pd.DataFrame([m_dtr]).to_csv(os.path.join(OUT_ACC, \"dtr_metrics.csv\"), index=False)\n",
    "plot_parity(y_acc_te.values, yhat_dtr, \"Accidents: DTR Parity\", os.path.join(OUT_ACC, \"dtr_parity.png\"))\n",
    "plot_residuals(y_acc_te.values, yhat_dtr, \"Accidents: DTR Residuals\", os.path.join(OUT_ACC, \"dtr_residuals.png\"))\n",
    "plot_learning_curve_single(dtr, X_acc_tr, y_acc_tr, \"Accidents: DTR Learning Curve\",\n",
    "                           os.path.join(OUT_ACC, \"dtr_learning_curve.png\"), cv=cv_reg)\n",
    "\n",
    "# Realized stats\n",
    "tree_r = dtr.named_steps[\"reg\"]\n",
    "realized_r = {\n",
    "    \"depth\": int(tree_r.get_depth()),\n",
    "    \"leaves\": int(tree_r.get_n_leaves()),\n",
    "    \"nodes\": int(tree_r.tree_.node_count),\n",
    "}\n",
    "save_json_safe(os.path.join(OUT_ACC, \"dtr_realized_stats.json\"), realized_r)\n",
    "save_profile(os.path.join(OUT_ACC, \"dtr_profile.json\"),\n",
    "             tag=\"Acc_DTR_fit+predict\", seconds_fit=prof_dtr.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_dtr.peak_gb, hardware=hw_info())\n",
    "\n",
    "# Permutation importances (top-10 by neg-MAE)\n",
    "imp_dtr = permutation_importance_topk(dtr, X_acc_te, y_acc_te, scoring=\"neg_mean_absolute_error\",\n",
    "                                      k=10, n_repeats=10)\n",
    "imp_dtr.to_csv(os.path.join(OUT_ACC, \"dtr_perm_importance_top10.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e00836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Accidents/kNN] Subsample train to 250,000 (exact brute)…\n",
      "[Profiler] START: Acc_kNNReg_fit\n",
      "[Profiler] END: Acc_kNNReg_fit | sec=0.65 | peakGB=3.078\n",
      "[Plot] Parity: Accidents: kNN Parity (≤25k eval)\n",
      "[Plot] Residuals: Accidents: kNN Residuals (≤25k eval)\n",
      "[Accidents/kNN] Learning curve (on subsample)…\n",
      "[Plot] Learning curve: Accidents: kNNReg Learning Curve (subsample)\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# CHUNK 11 — ACCIDENTS: kNN Regressor (caps) [R5][R2]\n",
    "# =======================================================\n",
    "\n",
    "cap_train = min(250000, len(X_acc_tr))\n",
    "print(f\"[Accidents/kNN] Subsample train to {cap_train:,} (exact brute)…\")\n",
    "idx_sub = np.random.RandomState(RANDOM_STATE).choice(len(X_acc_tr), size=cap_train, replace=False)\n",
    "Xa_knn, ya_knn = X_acc_tr.iloc[idx_sub], y_acc_tr.iloc[idx_sub]\n",
    "\n",
    "knr = Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                (\"reg\", KNeighborsRegressor(n_neighbors=21, algorithm=\"brute\", n_jobs=-1))])\n",
    "\n",
    "with Profiler(\"Acc_kNNReg_fit\") as prof_knr:\n",
    "    knr.fit(Xa_knn, ya_knn)\n",
    "\n",
    "def predict_in_chunks(model, X, chunk=5000):\n",
    "    out = []\n",
    "    for s in range(0, len(X), chunk):\n",
    "        out.append(model.predict(X.iloc[s:s+chunk]))\n",
    "    return np.concatenate(out)\n",
    "\n",
    "t0 = time.perf_counter(); yhat_knr = predict_in_chunks(knr, X_acc_te_eval, chunk=5000); pred_s = time.perf_counter()-t0\n",
    "m_knr = summarize_regression(y_acc_te_eval, yhat_knr)\n",
    "pd.DataFrame([m_knr]).to_csv(os.path.join(OUT_ACC, \"knn_metrics.csv\"), index=False)\n",
    "plot_parity(y_acc_te_eval.values, yhat_knr, \"Accidents: kNN Parity (≤25k eval)\", os.path.join(OUT_ACC, \"knn_parity.png\"))\n",
    "plot_residuals(y_acc_te_eval.values, yhat_knr, \"Accidents: kNN Residuals (≤25k eval)\", os.path.join(OUT_ACC, \"knn_residuals.png\"))\n",
    "print(\"[Accidents/kNN] Learning curve (on subsample)…\")\n",
    "\n",
    "# Smaller LC-only slice of the TRAIN set (diagnostic, not the final model)\n",
    "lc_cap = min(40000, len(X_acc_tr))  # if still slow, drop to 20000\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "lc_idx = rng.choice(len(X_acc_tr), size=lc_cap, replace=False)\n",
    "X_lc, y_lc = X_acc_tr.iloc[lc_idx], y_acc_tr.iloc[lc_idx]\n",
    "\n",
    "# Coarser grid & fewer folds\n",
    "cv_lc = KFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "plot_learning_curve_single(\n",
    "    knr, X_lc, y_lc,\n",
    "    title=\"Accidents: kNNReg Learning Curve (subsample)\",\n",
    "    outpath=os.path.join(OUT_ACC, \"knn_learning_curve.png\"),\n",
    "    cv=cv_lc,\n",
    "    train_sizes=np.linspace(0.2, 1.0, 4),    # 4 points\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "save_profile(os.path.join(OUT_ACC, \"knn_profile.json\"),\n",
    "             tag=\"Acc_kNNReg_fit+predict\", seconds_fit=prof_knr.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_knr.peak_gb, hardware=hw_info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bec10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Accidents/SVR] Subsample train to 50,000 for RBF SVR…\n",
      "[Profiler] START: Acc_SVR_fit\n",
      "[Profiler] END: Acc_SVR_fit | sec=85.72 | peakGB=0.753\n",
      "[Plot] Parity: Accidents: SVR Parity (≤25k eval)\n",
      "[Plot] Residuals: Accidents: SVR Residuals (≤25k eval)\n",
      "[Accidents/SVR] Learning curve (on subsample)…\n",
      "[Plot] Learning curve: Accidents: SVR(RBF) Learning Curve (subsample)\n",
      "[Accidents/SGDR] Linear ε-SVR analogue (SGDRegressor)…\n",
      "[Profiler] START: Acc_SGDR_fit\n",
      "[Profiler] END: Acc_SGDR_fit | sec=2.97 | peakGB=1.309\n",
      "[Plot] Parity: Accidents: SGDR(ε) Parity\n",
      "[Plot] Residuals: Accidents: SGDR(ε) Residuals\n",
      "[Plot] Learning curve: Accidents: SGDR(ε) Learning Curve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Accidents/SGDR] Model complexity sweep (alpha)…\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CHUNK 12 — ACCIDENTS: SVR (RBF) + linear baseline\n",
    "# ====================================================\n",
    "\n",
    "cap_svr = min(50000, len(X_acc_tr))  # tightened to meet ≤100k kernel SVM rule\n",
    "print(f\"[Accidents/SVR] Subsample train to {cap_svr:,} for RBF SVR…\")\n",
    "idx_svr = np.random.RandomState(RANDOM_STATE).choice(len(X_acc_tr), size=cap_svr, replace=False)\n",
    "Xa_svr, ya_svr = X_acc_tr.iloc[idx_svr], y_acc_tr.iloc[idx_svr]\n",
    "\n",
    "svr = Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                (\"reg\", SVR(C=1.0, kernel=\"rbf\"))])\n",
    "\n",
    "with Profiler(\"Acc_SVR_fit\") as prof_svr:\n",
    "    svr.fit(Xa_svr, ya_svr)\n",
    "t0 = time.perf_counter(); yhat_svr = svr.predict(X_acc_te_eval); pred_s = time.perf_counter()-t0\n",
    "m_svr = summarize_regression(y_acc_te_eval, yhat_svr)\n",
    "pd.DataFrame([m_svr]).to_csv(os.path.join(OUT_ACC, \"svr_metrics.csv\"), index=False)\n",
    "plot_parity(y_acc_te_eval.values, yhat_svr, \"Accidents: SVR Parity (≤25k eval)\", os.path.join(OUT_ACC, \"svr_parity.png\"))\n",
    "plot_residuals(y_acc_te_eval.values, yhat_svr, \"Accidents: SVR Residuals (≤25k eval)\", os.path.join(OUT_ACC, \"svr_residuals.png\"))\n",
    "\n",
    "print(\"[Accidents/SVR] Learning curve (on subsample)…\")\n",
    "svr_lc_cap = min(25000, len(X_acc_tr))\n",
    "svr_lc_idx = np.random.RandomState(RANDOM_STATE).choice(len(X_acc_tr), size=svr_lc_cap, replace=False)\n",
    "X_svr_lc, y_svr_lc = X_acc_tr.iloc[svr_lc_idx], y_acc_tr.iloc[svr_lc_idx]\n",
    "\n",
    "cv_svr_lc = KFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "plot_learning_curve_single(\n",
    "    svr, X_svr_lc, y_svr_lc,\n",
    "    title=\"Accidents: SVR(RBF) Learning Curve (subsample)\",\n",
    "    outpath=os.path.join(OUT_ACC, \"svr_learning_curve.png\"),\n",
    "    cv=cv_svr_lc,\n",
    "    train_sizes=np.linspace(0.25, 1.0, 3),   # 3 points\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "save_profile(os.path.join(OUT_ACC, \"svr_profile.json\"),\n",
    "             tag=\"Acc_SVR_fit+predict\", seconds_fit=prof_svr.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_svr.peak_gb, hardware=hw_info())\n",
    "# Optional linear baseline (fast)\n",
    "# print(\"[Accidents/SVR-Linear] Linear SVR baseline…\")\n",
    "# svr_lin = Pipeline([(\"pre\", clone(pre_acc)),\n",
    "#                     (\"reg\", SVR(C=1.0, kernel=\"linear\"))])\n",
    "# with Profiler(\"Acc_SVRLinear_fit\") as prof_svrL:\n",
    "#     svr_lin.fit(X_acc_tr, y_acc_tr)\n",
    "# t0 = time.perf_counter(); yhat_svrL = svr_lin.predict(X_acc_te); pred_s = time.perf_counter()-t0\n",
    "# m_svrL = summarize_regression(y_acc_te, yhat_svrL)\n",
    "# pd.DataFrame([m_svrL]).to_csv(os.path.join(OUT_ACC, \"svr_linear_metrics.csv\"), index=False)\n",
    "# plot_parity(y_acc_te.values, yhat_svrL, \"Accidents: Linear SVR Parity\", os.path.join(OUT_ACC, \"svr_linear_parity.png\"))\n",
    "# plot_residuals(y_acc_te.values, yhat_svrL, \"Accidents: Linear SVR Residuals\", os.path.join(OUT_ACC, \"svr_linear_residuals.png\"))\n",
    "# save_profile(os.path.join(OUT_ACC, \"svr_linear_profile.json\"),\n",
    "#              tag=\"Acc_SVRLinear_fit+predict\", seconds_fit=prof_svrL.seconds_fit,\n",
    "#              seconds_predict=pred_s, peak_GB=prof_svrL.peak_gb, hardware=hw_info())\n",
    "\n",
    "# ==========================================================\n",
    "# CHUNK 12b — ACCIDENTS: Linear ε-SVR via SGDRegressor [spec]\n",
    "# ==========================================================\n",
    "\n",
    "print(\"[Accidents/SGDR] Linear ε-SVR analogue (SGDRegressor)…\")\n",
    "sgdr = Pipeline([\n",
    "    (\"pre\", clone(pre_acc)),\n",
    "    (\"reg\", SGDRegressor(\n",
    "        loss=\"epsilon_insensitive\",      # linear ε-SVR analogue\n",
    "        alpha=1e-4,                      # L2 regularization\n",
    "        penalty=\"l2\",\n",
    "        learning_rate=\"constant\",\n",
    "        eta0=0.01,\n",
    "        max_iter=20000,\n",
    "        tol=1e-3,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "with Profiler(\"Acc_SGDR_fit\") as prof_sgdr:\n",
    "    sgdr.fit(X_acc_tr, y_acc_tr)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "yhat_sgdr = sgdr.predict(X_acc_te)\n",
    "pred_s = time.perf_counter() - t0\n",
    "\n",
    "m_sgdr = summarize_regression(y_acc_te, yhat_sgdr)\n",
    "pd.DataFrame([m_sgdr]).to_csv(os.path.join(OUT_ACC, \"sgdr_metrics.csv\"), index=False)\n",
    "plot_parity(y_acc_te.values, yhat_sgdr, \"Accidents: SGDR(ε) Parity\", os.path.join(OUT_ACC, \"sgdr_parity.png\"))\n",
    "plot_residuals(y_acc_te.values, yhat_sgdr, \"Accidents: SGDR(ε) Residuals\", os.path.join(OUT_ACC, \"sgdr_residuals.png\"))\n",
    "\n",
    "save_profile(os.path.join(OUT_ACC, \"sgdr_profile.json\"),\n",
    "             tag=\"Acc_SGDR_fit+predict\", seconds_fit=prof_sgdr.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_sgdr.peak_gb, hardware=hw_info())\n",
    "\n",
    "# Learning curve for SGDR\n",
    "plot_learning_curve_single(\n",
    "    sgdr, X_acc_tr, y_acc_tr,\n",
    "    title=\"Accidents: SGDR(ε) Learning Curve\",\n",
    "    outpath=os.path.join(OUT_ACC, \"sgdr_learning_curve.png\"),\n",
    "    cv=cv_reg,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Model-complexity curve (vary alpha)\n",
    "print(\"[Accidents/SGDR] Model complexity sweep (alpha)…\")\n",
    "def sgdr_maker(alpha):\n",
    "    return Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                     (\"reg\", SGDRegressor(loss=\"epsilon_insensitive\",\n",
    "                                          alpha=float(alpha), penalty=\"l2\",\n",
    "                                          learning_rate=\"constant\", eta0=0.01,\n",
    "                                          max_iter=20000, tol=1e-3,\n",
    "                                          random_state=RANDOM_STATE))])\n",
    "\n",
    "model_complexity_curve(\n",
    "    pipe_maker=sgdr_maker,\n",
    "    param_name=\"alpha\", param_grid=[1e-5, 1e-4, 1e-3],\n",
    "    X=X_acc_tr, y=y_acc_tr, cv=cv_reg, scorer=r2_score,\n",
    "    out_csv=os.path.join(OUT_ACC, \"mc_sgdr.csv\"),\n",
    "    title=\"Accidents: SGDR(ε) Model-Complexity (alpha)\",\n",
    "    out_png=os.path.join(OUT_ACC, \"mc_sgdr.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9e88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Profiler] START: Acc_MLPReg_fit\n",
      "[Accidents/MLPReg] Epoch-by-epoch (SGD-only, ≤15 epochs, early stopping)…\n",
      "[Accidents/MLPReg] Early stopping at epoch 4 (best=1)\n",
      "[Profiler] END: Acc_MLPReg_fit | sec=27.65 | peakGB=2.336\n",
      "Train feats: (700000, 71)  Test feats: (150000, 71)\n",
      "[Plot] Parity: Accidents: MLPReg Parity\n",
      "[Plot] Residuals: Accidents: MLPReg Residuals\n",
      "[Accidents/MLPReg] Learning curve (on subsample)…\n",
      "[Plot] Learning curve: Accidents: MLPReg Learning Curve (subsample)\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# CHUNK 13 — ACCIDENTS: MLPRegressor (SGD-only) [R2]\n",
    "# =======================================================\n",
    "\n",
    "def train_mlpr_sgd_epoch_curve(X_tr, y_tr, X_te, y_te, out_csv, out_png, patience=3):\n",
    "    \"\"\"MLPRegressor with SGD only; ≤15 epochs; early stopping marker; param budget log.\"\"\"\n",
    "    print(\"[Accidents/MLPReg] Epoch-by-epoch (SGD-only, ≤15 epochs, early stopping)…\")\n",
    "    reg = Pipeline([\n",
    "    (\"pre\", clone(pre_acc)),\n",
    "    (\"dense32\", FunctionTransformer(to_dense32, accept_sparse=True)),\n",
    "    (\"reg\", MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 256, 128, 128),\n",
    "        activation=\"tanh\",\n",
    "        solver=\"sgd\",\n",
    "        learning_rate_init=0.001,\n",
    "        learning_rate=\"constant\",\n",
    "        alpha=1e-3,\n",
    "        momentum=0.0,\n",
    "        batch_size=2048,\n",
    "        max_iter=1,       # one epoch per .fit()\n",
    "        warm_start=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "    rows, best = [], {\"epoch\": 0, \"val_MAE\": np.inf}\n",
    "    no_improve = 0\n",
    "    for epoch in range(1, 16):\n",
    "        reg.fit(X_tr, y_tr)  # one epoch\n",
    "        ytr = reg.predict(X_tr); yte = reg.predict(X_te)\n",
    "        mae_tr = mean_absolute_error(y_tr, ytr); mae_te = mean_absolute_error(y_te, yte)\n",
    "        rows.append({\"epoch\": epoch, \"train_MAE\": float(mae_tr), \"val_MAE\": float(mae_te)})\n",
    "        if mae_te + 1e-6 < best[\"val_MAE\"]:\n",
    "            best.update({\"epoch\": epoch, \"val_MAE\": float(mae_te)}); no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"[Accidents/MLPReg] Early stopping at epoch {epoch} (best={best['epoch']})\")\n",
    "                break\n",
    "    df = pd.DataFrame(rows); df.to_csv(out_csv, index=False)\n",
    "    plt.figure(); plt.plot(df[\"epoch\"], df[\"train_MAE\"], marker=\"o\", label=\"Train MAE\")\n",
    "    plt.plot(df[\"epoch\"], df[\"val_MAE\"], marker=\"s\", label=\"Val MAE\")\n",
    "    plt.axvline(best[\"epoch\"], linestyle=\"--\", label=f\"best@{best['epoch']}\")\n",
    "    plt.title(\"Accidents: MLPReg (SGD) Epoch Curve\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"MAE\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_png, bbox_inches=\"tight\"); plt.close()\n",
    "    # param budget audit\n",
    "    n_in = _estimate_input_dim(pre_acc, X_tr); n_params = _estimate_mlp_params(n_in, (256,256,128,128), 1)\n",
    "    save_json_safe(os.path.join(OUT_ACC, \"mlpreg_param_budget.json\"),\n",
    "                   {\"estimated_input_dim\": n_in, \"param_count\": n_params})\n",
    "    return reg\n",
    "\n",
    "with Profiler(\"Acc_MLPReg_fit\") as prof_mlpr:\n",
    "    mlpr = train_mlpr_sgd_epoch_curve(X_acc_tr, y_acc_tr, X_acc_te, y_acc_te,\n",
    "                                      os.path.join(OUT_ACC,\"mlp_epochs.csv\"),\n",
    "                                      os.path.join(OUT_ACC,\"mlp_epochs.png\"))\n",
    "t0 = time.perf_counter(); yhat_mlpr = mlpr.predict(X_acc_te); pred_s = time.perf_counter()-t0\n",
    "Xt_tr = mlpr.named_steps[\"dense32\"].transform( mlpr.named_steps[\"pre\"].transform(X_acc_tr) )\n",
    "Xt_te = mlpr.named_steps[\"dense32\"].transform( mlpr.named_steps[\"pre\"].transform(X_acc_te) )\n",
    "print(\"Train feats:\", Xt_tr.shape, \" Test feats:\", Xt_te.shape)\n",
    "\n",
    "m_mlpr = summarize_regression(y_acc_te, yhat_mlpr)\n",
    "pd.DataFrame([m_mlpr]).to_csv(os.path.join(OUT_ACC, \"mlp_metrics.csv\"), index=False)\n",
    "plot_parity(y_acc_te.values, yhat_mlpr, \"Accidents: MLPReg Parity\", os.path.join(OUT_ACC, \"mlp_parity.png\"))\n",
    "plot_residuals(y_acc_te.values, yhat_mlpr, \"Accidents: MLPReg Residuals\", os.path.join(OUT_ACC, \"mlp_residuals.png\"))\n",
    "print(\"[Accidents/MLPReg] Learning curve (on subsample)…\")\n",
    "\n",
    "mlpr_lc = Pipeline([\n",
    "    (\"pre\", clone(pre_acc)),\n",
    "    (\"dense32\", FunctionTransformer(to_dense32, accept_sparse=True)),\n",
    "    (\"reg\", MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 256, 128, 128),\n",
    "        activation=\"tanh\",\n",
    "        solver=\"sgd\",\n",
    "        learning_rate_init=0.001,\n",
    "        learning_rate=\"constant\",\n",
    "        alpha=1e-3,\n",
    "        momentum=0.0,\n",
    "        max_iter=50,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "mlp_lc_cap = min(30000, len(X_acc_tr))\n",
    "mlp_lc_idx = np.random.RandomState(RANDOM_STATE).choice(len(X_acc_tr), size=mlp_lc_cap, replace=False)\n",
    "X_mlp_lc, y_mlp_lc = X_acc_tr.iloc[mlp_lc_idx], y_acc_tr.iloc[mlp_lc_idx]\n",
    "\n",
    "cv_mlp_lc = KFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "plot_learning_curve_single(\n",
    "    mlpr_lc, X_mlp_lc, y_mlp_lc,\n",
    "    title=\"Accidents: MLPReg Learning Curve (subsample)\",\n",
    "    outpath=os.path.join(OUT_ACC, \"mlp_learning_curve.png\"),\n",
    "    cv=cv_mlp_lc,\n",
    "    train_sizes=np.linspace(0.2, 1.0, 4),\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "save_profile(os.path.join(OUT_ACC, \"mlp_profile.json\"),\n",
    "             tag=\"Acc_MLPReg_fit+predict\", seconds_fit=prof_mlpr.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_mlpr.peak_gb, hardware=hw_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d79485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hotel] Building metrics summary…\n",
      "    model  accuracy  precision    recall        f1   roc_auc    pr_auc\n",
      "3  RBFSVM  0.844659   0.860945  0.692493  0.767586  0.909404  0.880933\n",
      "0      DT  0.812999   0.826476  0.626771  0.712902  0.889125  0.850334\n",
      "2  LINSVM  0.808532   0.800713  0.643202  0.713366  0.886355  0.843167\n",
      "1     KNN  0.819030   0.787884  0.699879  0.741279  0.883948  0.855353\n",
      "4     MLP  0.806913   0.825277  0.607326  0.699722  0.863167  0.831103\n",
      "[Accidents] Building metrics summary…\n",
      "  model        MAE      MedAE       RMSE        R2\n",
      "4  SGDR  17.890766  14.396326  32.938952 -0.074464\n",
      "3   MLP  20.384327  17.570444  33.042102 -0.081204\n",
      "2   SVR  16.523454  12.159758  33.573486 -0.045412\n",
      "0   DTR  21.588574  17.669019  34.244566 -0.161330\n",
      "1   KNN  20.180107  15.571829  35.162493 -0.146711\n",
      "[All] Building runtime tables…\n",
      "✅ Pipeline finished. See outputs/ for figures, metrics, and logs.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CHUNK 14 — Summaries & runtime tables for the report\n",
    "# =====================================================\n",
    "\n",
    "# HOTEL summary table (sorted by ROC-AUC)\n",
    "print(\"[Hotel] Building metrics summary…\")\n",
    "hotel_rows = []\n",
    "for f in [\"dt_metrics.csv\",\"knn_metrics.csv\",\"linsvm_metrics.csv\",\"rbfsvm_metrics.csv\",\"mlp_metrics.csv\"]:\n",
    "    p = os.path.join(OUT_HOTEL, f)\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_csv(p); df.insert(0, \"model\", f.split(\"_\")[0].upper())\n",
    "        hotel_rows.append(df)\n",
    "if hotel_rows:\n",
    "    hotel_summary = pd.concat(hotel_rows, ignore_index=True).sort_values(\"roc_auc\", ascending=False)\n",
    "    hotel_summary.to_csv(os.path.join(OUT_HOTEL, \"metrics_summary.csv\"), index=False)\n",
    "    print(hotel_summary)\n",
    "\n",
    "# ACC summary table (sorted by RMSE ascending)\n",
    "print(\"[Accidents] Building metrics summary…\")\n",
    "acc_rows = []\n",
    "for f in [\"dtr_metrics.csv\",\"knn_metrics.csv\",\"svr_metrics.csv\",\n",
    "          \"mlp_metrics.csv\",\"svr_linear_metrics.csv\",\"sgdr_metrics.csv\"]:\n",
    "    p = os.path.join(OUT_ACC, f)\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_csv(p); df.insert(0, \"model\", f.split(\"_\")[0].upper())\n",
    "        acc_rows.append(df)\n",
    "if acc_rows:\n",
    "    acc_summary = pd.concat(acc_rows, ignore_index=True).sort_values(\"RMSE\", ascending=True)\n",
    "    acc_summary.to_csv(os.path.join(OUT_ACC, \"metrics_summary.csv\"), index=False)\n",
    "    print(acc_summary)\n",
    "\n",
    "# Aggregate runtime tables (fit+predict+hardware)\n",
    "def build_runtime_table(root, out_csv):\n",
    "    rows = []\n",
    "    for fname in os.listdir(root):\n",
    "        if fname.endswith(\"_profile.json\"):\n",
    "            j = json.load(open(os.path.join(root, fname)))\n",
    "            j[\"file\"] = fname\n",
    "            rows.append(j)\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"[All] Building runtime tables…\")\n",
    "build_runtime_table(OUT_HOTEL, os.path.join(OUT_HOTEL, \"runtime_table.csv\"))\n",
    "build_runtime_table(OUT_ACC,   os.path.join(OUT_ACC,   \"runtime_table.csv\"))\n",
    "\n",
    "print(\"✅ Pipeline finished. See outputs/ for figures, metrics, and logs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep cell: run this once before the patch cells\n",
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Simple helper for model-complexity plots (MAE for regression)\n",
    "def run_mc(pipe_maker, grid, X, y, cv, xlabel, title, out_png, scoring=\"neg_mean_absolute_error\"):\n",
    "    rows=[]\n",
    "    for val in grid:\n",
    "        est = pipe_maker(val)\n",
    "        scores = cross_val_score(est, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        rows.append((val, scores.mean(), scores.std()))\n",
    "    xs, means, stds = zip(*rows)\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.errorbar(xs, [-m for m in means] if scoring.startswith(\"neg_\") else means, yerr=stds, marker='o')\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"MAE\" if scoring.startswith(\"neg_\") else scoring.upper())\n",
    "    plt.title(title); plt.grid(alpha=.3)\n",
    "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "    plt.savefig(out_png, dpi=160); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1133342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-Complexity (max_depth) for Accidents DTR\n",
    "from sklearn.metrics import mean_absolute_error as _mae\n",
    "neg_mae = lambda yt, yp: -_mae(yt, yp)\n",
    "\n",
    "def dtr_maker(max_depth):\n",
    "    return Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                     (\"reg\", DecisionTreeRegressor(\n",
    "                         max_depth=int(max_depth),\n",
    "                         min_samples_leaf=100,\n",
    "                         min_samples_split=200,\n",
    "                         random_state=RANDOM_STATE))])\n",
    "\n",
    "model_complexity_curve(\n",
    "    pipe_maker=dtr_maker,\n",
    "    param_name=\"max_depth\",\n",
    "    param_grid=[6, 10, 14, 18],\n",
    "    X=X_acc_tr, y=y_acc_tr, cv=cv_reg, scorer=neg_mae,\n",
    "    out_csv=os.path.join(OUT_ACC, \"mc_dtr.csv\"),\n",
    "    title=\"Accidents: DTR Model-Complexity (max_depth)\",\n",
    "    out_png=os.path.join(OUT_ACC, \"mc_dtr.png\")\n",
    ")\n",
    "\n",
    "# Model-Complexity (k) for Accidents kNN on the 250k subset\n",
    "def knr_maker(k):\n",
    "    return Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                     (\"reg\", KNeighborsRegressor(n_neighbors=int(k),\n",
    "                                                algorithm=\"brute\", metric=\"euclidean\",\n",
    "                                                n_jobs=-1))])\n",
    "\n",
    "model_complexity_curve(\n",
    "    pipe_maker=knr_maker,\n",
    "    param_name=\"n_neighbors\",\n",
    "    param_grid=[3, 5, 11, 21],\n",
    "    X=Xa_knn, y=ya_knn, cv=cv_reg, scorer=neg_mae,\n",
    "    out_csv=os.path.join(OUT_ACC, \"mc_knn.csv\"),\n",
    "    title=\"Accidents: kNN Model-Complexity (k)\",\n",
    "    out_png=os.path.join(OUT_ACC, \"mc_knn.png\")\n",
    ")\n",
    "\n",
    "# Model-Complexity (C) for Accidents RBF-SVR on the ≤100k subset\n",
    "def rbf_svr_maker(C):\n",
    "    return Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                     (\"reg\", SVR(kernel=\"rbf\", C=float(C)))])\n",
    "\n",
    "model_complexity_curve(\n",
    "    pipe_maker=rbf_svr_maker,\n",
    "    param_name=\"C\",\n",
    "    param_grid=[0.5, 2.0, 8.0],\n",
    "    X=Xa_svr, y=ya_svr, cv=cv_reg, scorer=neg_mae,\n",
    "    out_csv=os.path.join(OUT_ACC, \"mc_svr_rbf.csv\"),\n",
    "    title=\"Accidents: RBF SVR Model-Complexity (C)\",\n",
    "    out_png=os.path.join(OUT_ACC, \"mc_svr_rbf.png\")\n",
    ")\n",
    "# NN Model-Complexity (width sweep within 0.2M–1.0M params)\n",
    "PARAM_MIN, PARAM_MAX = 2e5, 1e6\n",
    "n_in = _estimate_input_dim(pre_acc, X_acc_tr)\n",
    "candidates = [128, 256, 512]\n",
    "widths = [w for w in candidates if PARAM_MIN <= _estimate_mlp_params(n_in, (w, w), 1) <= PARAM_MAX]\n",
    "\n",
    "def mlpr_maker(w):\n",
    "    return Pipeline([\n",
    "        (\"pre\", clone(pre_acc)),\n",
    "        (\"dense32\", FunctionTransformer(to_dense32, accept_sparse=True)),\n",
    "        (\"reg\", MLPRegressor(hidden_layer_sizes=(int(w), int(w)),\n",
    "                             solver=\"sgd\", learning_rate=\"constant\", learning_rate_init=0.001,\n",
    "                             alpha=1e-3, momentum=0.0, batch_size=2048,\n",
    "                             max_iter=15, early_stopping=False, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "# Use a manageable slice for MC (keeps runtime reasonable)\n",
    "idx_w = np.random.RandomState(RANDOM_STATE).choice(len(X_acc_tr), size=min(300_000, len(X_acc_tr)), replace=False)\n",
    "model_complexity_curve(\n",
    "    pipe_maker=mlpr_maker,\n",
    "    param_name=\"width\",\n",
    "    param_grid=widths,\n",
    "    X=X_acc_tr.iloc[idx_w], y=y_acc_tr.iloc[idx_w],\n",
    "    cv=KFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scorer=neg_mae,\n",
    "    out_csv=os.path.join(OUT_ACC, \"mc_mlpr_width.csv\"),\n",
    "    title=\"Accidents: NN Model-Complexity (width)\",\n",
    "    out_png=os.path.join(OUT_ACC, \"mc_mlpr_width.png\")\n",
    ")\n",
    "\n",
    "# Linear SVR (literal linear SVM regressor) — LC + MC\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "print(\"[Accidents/LinearSVR] Fitting LinearSVR on full train…\")\n",
    "lsvr = Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                 (\"reg\", LinearSVR(C=1.0, epsilon=0.1, max_iter=20000, random_state=RANDOM_STATE))])\n",
    "\n",
    "with Profiler(\"Acc_LinearSVR_fit\") as prof_lsvr:\n",
    "    lsvr.fit(X_acc_tr, y_acc_tr)\n",
    "\n",
    "t0 = time.perf_counter(); yhat_lsvr = lsvr.predict(X_acc_te); pred_s = time.perf_counter()-t0\n",
    "m_lsvr = summarize_regression(y_acc_te, yhat_lsvr)\n",
    "pd.DataFrame([m_lsvr]).to_csv(os.path.join(OUT_ACC, \"linearsvr_metrics.csv\"), index=False)\n",
    "save_profile(os.path.join(OUT_ACC, \"linearsvr_profile.json\"),\n",
    "             tag=\"Acc_LinearSVR_fit+predict\", seconds_fit=prof_lsvr.seconds_fit,\n",
    "             seconds_predict=pred_s, peak_GB=prof_lsvr.peak_gb, hardware=hw_info())\n",
    "\n",
    "# Learning curve\n",
    "plot_learning_curve_single(\n",
    "    lsvr, X_acc_tr, y_acc_tr,\n",
    "    title=\"Accidents: Linear SVR Learning Curve\",\n",
    "    outpath=os.path.join(OUT_ACC, \"linearsvr_learning_curve.png\"),\n",
    "    cv=cv_reg, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
    ")\n",
    "\n",
    "# Model-Complexity (C)\n",
    "def lsvr_maker(C):\n",
    "    return Pipeline([(\"pre\", clone(pre_acc)),\n",
    "                     (\"reg\", LinearSVR(C=float(C), epsilon=0.1, max_iter=20000, random_state=RANDOM_STATE))])\n",
    "\n",
    "model_complexity_curve(\n",
    "    pipe_maker=lsvr_maker,\n",
    "    param_name=\"C\", param_grid=[0.1, 1.0, 10.0],\n",
    "    X=X_acc_tr, y=y_acc_tr, cv=cv_reg, scorer=neg_mae,\n",
    "    out_csv=os.path.join(OUT_ACC, \"mc_linearsvr.csv\"),\n",
    "    title=\"Accidents: Linear SVR Model-Complexity (C)\",\n",
    "    out_png=os.path.join(OUT_ACC, \"mc_linearsvr.png\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
