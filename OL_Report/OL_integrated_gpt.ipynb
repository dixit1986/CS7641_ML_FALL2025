{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbf3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Data cleaning complete.\n",
      "Final data preparation is complete. Preprocessor now includes Target Encoding.\n",
      "--- Starting Grid Search for DecisionTreeClassifier ---\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Grid Search completed in 7.14 seconds.\n",
      "Best parameters found: {'classifier__class_weight': 'balanced', 'classifier__max_depth': 16, 'classifier__min_samples_leaf': 100}\n",
      "--- Evaluation on Test Set ---\n",
      "ROC-AUC: 0.9336\n",
      "PR-AUC: 0.9019\n",
      "F1-Score (at threshold 0.5): 0.8027\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87     15033\n",
      "           1       0.75      0.86      0.80      8845\n",
      "\n",
      "    accuracy                           0.84     23878\n",
      "   macro avg       0.83      0.85      0.84     23878\n",
      "weighted avg       0.85      0.84      0.85     23878\n",
      "\n",
      "--- Starting Grid Search for KNeighborsClassifier ---\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Grid Search completed in 8.81 seconds.\n",
      "Best parameters found: {'classifier__n_neighbors': 11}\n",
      "--- Evaluation on Test Set ---\n",
      "ROC-AUC: 0.8923\n",
      "PR-AUC: 0.8651\n",
      "F1-Score (at threshold 0.5): 0.7518\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     15033\n",
      "           1       0.79      0.72      0.75      8845\n",
      "\n",
      "    accuracy                           0.82     23878\n",
      "   macro avg       0.82      0.80      0.81     23878\n",
      "weighted avg       0.82      0.82      0.82     23878\n",
      "\n",
      "--- Training Shallow-Wide Neural Network ---\n",
      "ROC-AUC: 0.9467\n",
      "PR-AUC: 0.9225\n",
      "F1-Score (at threshold 0.5): 0.8212\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90     15033\n",
      "           1       0.85      0.79      0.82      8845\n",
      "\n",
      "    accuracy                           0.87     23878\n",
      "   macro avg       0.87      0.86      0.86     23878\n",
      "weighted avg       0.87      0.87      0.87     23878\n",
      "\n",
      "--- Training Deeper-Narrower Neural Network ---\n",
      "--- Evaluation of Deeper-Narrower NN ---\n",
      "ROC-AUC: 0.9451\n",
      "PR-AUC: 0.9207\n",
      "F1-Score (at threshold 0.5): 0.8200\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90     15033\n",
      "           1       0.85      0.79      0.82      8845\n",
      "\n",
      "    accuracy                           0.87     23878\n",
      "   macro avg       0.87      0.86      0.86     23878\n",
      "weighted avg       0.87      0.87      0.87     23878\n",
      "\n",
      "--- Starting Grid Search for LinearSVC ---\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Grid Search completed in 14.28 seconds.\n",
      "Best parameters found: {'classifier__C': 1, 'classifier__class_weight': 'balanced'}\n",
      "--- Evaluation on Test Set ---\n",
      "ROC-AUC: 0.9004\n",
      "PR-AUC: 0.8592\n",
      "F1-Score (at threshold 0.0): 0.7647\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85     15033\n",
      "           1       0.73      0.80      0.76      8845\n",
      "\n",
      "    accuracy                           0.82     23878\n",
      "   macro avg       0.80      0.81      0.81     23878\n",
      "weighted avg       0.82      0.82      0.82     23878\n",
      "\n",
      "--- Starting Randomized Search for RBF SVM on 20000 samples ---\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters found: {'classifier__gamma': 'scale', 'classifier__class_weight': 'balanced', 'classifier__C': 8}\n",
      "--- Evaluation on Test Set ---\n",
      "ROC-AUC: 0.9232\n",
      "PR-AUC: 0.8915\n",
      "F1-Score (at threshold 0.5): 0.7966\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89     15033\n",
      "           1       0.83      0.77      0.80      8845\n",
      "\n",
      "    accuracy                           0.85     23878\n",
      "   macro avg       0.85      0.84      0.84     23878\n",
      "weighted avg       0.85      0.85      0.85     23878\n",
      "\n",
      "--- Generating Learning Curves ---\n",
      "--- Generating Model Complexity Curve for Decision Tree ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating NN Loss Curves ---\n",
      "--- Generating Final Summary Table ---\n",
      "--- Final Model Comparison ---\n",
      "           Model   ROC-AUC    PR-AUC  F1-Score  Predict Time (s)\n",
      "4     Shallow NN  0.946659  0.922481  0.821202          0.098443\n",
      "5        Deep NN  0.945137  0.920669  0.819971          0.074454\n",
      "0  Decision Tree  0.933580  0.901939  0.802707          0.039325\n",
      "3        RBF SVM  0.923201  0.891507  0.796622         15.747160\n",
      "2     Linear SVM  0.900357  0.859154  0.764709          0.037497\n",
      "1           k-NN  0.892281  0.865128  0.751825          1.236867\n",
      "--- Training Shallow-Wide Neural Network (Regressor with Scaled Target) ---\n",
      "Iteration 1, loss = 0.32512972\n",
      "Validation score: 0.453984\n",
      "Iteration 2, loss = 0.26401897\n",
      "Validation score: 0.487599\n",
      "Iteration 3, loss = 0.25245047\n",
      "Validation score: 0.503340\n",
      "Iteration 4, loss = 0.24538361\n",
      "Validation score: 0.513701\n",
      "Iteration 5, loss = 0.24011362\n",
      "Validation score: 0.523042\n",
      "Iteration 6, loss = 0.23553677\n",
      "Validation score: 0.529840\n",
      "Iteration 7, loss = 0.23184939\n",
      "Validation score: 0.533031\n",
      "Iteration 8, loss = 0.22859440\n",
      "Validation score: 0.541525\n",
      "Iteration 9, loss = 0.22555729\n",
      "Validation score: 0.546521\n",
      "Iteration 10, loss = 0.22276621\n",
      "Validation score: 0.548581\n",
      "Iteration 11, loss = 0.22060247\n",
      "Validation score: 0.552531\n",
      "Iteration 12, loss = 0.21838919\n",
      "Validation score: 0.554680\n",
      "Iteration 13, loss = 0.21641298\n",
      "Validation score: 0.560034\n",
      "Iteration 14, loss = 0.21457041\n",
      "Validation score: 0.562815\n",
      "Iteration 15, loss = 0.21266807\n",
      "Validation score: 0.563404\n",
      "Iteration 16, loss = 0.21092584\n",
      "Validation score: 0.566461\n",
      "Iteration 17, loss = 0.20936182\n",
      "Validation score: 0.568090\n",
      "Iteration 18, loss = 0.20779486\n",
      "Validation score: 0.571594\n",
      "Iteration 19, loss = 0.20622411\n",
      "Validation score: 0.573015\n",
      "Iteration 20, loss = 0.20471498\n",
      "Validation score: 0.574456\n",
      "Iteration 21, loss = 0.20340825\n",
      "Validation score: 0.575203\n",
      "Iteration 22, loss = 0.20196130\n",
      "Validation score: 0.578907\n",
      "Iteration 23, loss = 0.20102412\n",
      "Validation score: 0.577838\n",
      "Iteration 24, loss = 0.19969884\n",
      "Validation score: 0.580950\n",
      "Iteration 25, loss = 0.19850762\n",
      "Validation score: 0.579907\n",
      "Iteration 26, loss = 0.19746178\n",
      "Validation score: 0.581118\n",
      "Iteration 27, loss = 0.19616655\n",
      "Validation score: 0.584106\n",
      "Iteration 28, loss = 0.19523069\n",
      "Validation score: 0.583298\n",
      "Iteration 29, loss = 0.19423238\n",
      "Validation score: 0.580376\n",
      "Iteration 30, loss = 0.19301553\n",
      "Validation score: 0.586467\n",
      "Iteration 31, loss = 0.19294312\n",
      "Validation score: 0.585697\n",
      "Iteration 32, loss = 0.19112217\n",
      "Validation score: 0.584409\n",
      "Iteration 33, loss = 0.19032178\n",
      "Validation score: 0.588667\n",
      "Iteration 34, loss = 0.18931931\n",
      "Validation score: 0.591495\n",
      "Iteration 35, loss = 0.18812915\n",
      "Validation score: 0.587332\n",
      "Iteration 36, loss = 0.18728848\n",
      "Validation score: 0.591478\n",
      "Iteration 37, loss = 0.18720006\n",
      "Validation score: 0.593239\n",
      "Iteration 38, loss = 0.18582007\n",
      "Validation score: 0.587855\n",
      "Iteration 39, loss = 0.18461021\n",
      "Validation score: 0.591616\n",
      "Iteration 40, loss = 0.18377933\n",
      "Validation score: 0.589383\n",
      "Iteration 41, loss = 0.18267327\n",
      "Validation score: 0.595687\n",
      "Iteration 42, loss = 0.18201349\n",
      "Validation score: 0.591082\n",
      "Iteration 43, loss = 0.18129605\n",
      "Validation score: 0.589989\n",
      "Iteration 44, loss = 0.18104191\n",
      "Validation score: 0.595253\n",
      "Iteration 45, loss = 0.17978076\n",
      "Validation score: 0.594157\n",
      "Iteration 46, loss = 0.17902365\n",
      "Validation score: 0.594964\n",
      "Iteration 47, loss = 0.17884545\n",
      "Validation score: 0.596455\n",
      "Iteration 48, loss = 0.17825319\n",
      "Validation score: 0.590551\n",
      "Iteration 49, loss = 0.17678403\n",
      "Validation score: 0.594840\n",
      "Iteration 50, loss = 0.17590180\n",
      "Validation score: 0.598198\n",
      "Iteration 51, loss = 0.17606945\n",
      "Validation score: 0.575404\n",
      "Iteration 52, loss = 0.17971981\n",
      "Validation score: 0.604187\n",
      "Iteration 53, loss = 0.17433439\n",
      "Validation score: 0.595597\n",
      "Iteration 54, loss = 0.17463027\n",
      "Validation score: 0.596394\n",
      "Iteration 55, loss = 0.17377521\n",
      "Validation score: 0.599737\n",
      "Iteration 56, loss = 0.17394228\n",
      "Validation score: 0.596333\n",
      "Iteration 57, loss = 0.17272927\n",
      "Validation score: 0.600721\n",
      "Iteration 58, loss = 0.17123876\n",
      "Validation score: 0.604556\n",
      "Iteration 59, loss = 0.17099207\n",
      "Validation score: 0.600720\n",
      "Iteration 60, loss = 0.16998619\n",
      "Validation score: 0.573104\n",
      "Iteration 61, loss = 0.16982035\n",
      "Validation score: 0.602907\n",
      "Iteration 62, loss = 0.16977568\n",
      "Validation score: 0.602498\n",
      "Iteration 63, loss = 0.16938189\n",
      "Validation score: 0.595102\n",
      "Iteration 64, loss = 0.16786230\n",
      "Validation score: 0.603923\n",
      "Iteration 65, loss = 0.16805335\n",
      "Validation score: 0.603815\n",
      "Iteration 66, loss = 0.16643721\n",
      "Validation score: 0.603385\n",
      "Iteration 67, loss = 0.16553487\n",
      "Validation score: 0.601413\n",
      "Iteration 68, loss = 0.16601068\n",
      "Validation score: 0.606645\n",
      "Iteration 69, loss = 0.16709261\n",
      "Validation score: 0.590132\n",
      "Iteration 70, loss = 0.16598510\n",
      "Validation score: 0.596739\n",
      "Iteration 71, loss = 0.16569816\n",
      "Validation score: 0.562103\n",
      "Iteration 72, loss = 0.16668353\n",
      "Validation score: 0.569474\n",
      "Iteration 73, loss = 0.16404906\n",
      "Validation score: 0.600253\n",
      "Iteration 74, loss = 0.16660266\n",
      "Validation score: 0.581624\n",
      "Iteration 75, loss = 0.17276984\n",
      "Validation score: 0.604047\n",
      "Iteration 76, loss = 0.16387663\n",
      "Validation score: 0.604992\n",
      "Iteration 77, loss = 0.16222257\n",
      "Validation score: 0.587491\n",
      "Iteration 78, loss = 0.16233732\n",
      "Validation score: 0.599689\n",
      "Iteration 79, loss = 0.16625372\n",
      "Validation score: 0.583690\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 80, loss = 0.15742471\n",
      "Validation score: 0.607475\n",
      "Iteration 81, loss = 0.15498158\n",
      "Validation score: 0.607975\n",
      "Iteration 82, loss = 0.15456306\n",
      "Validation score: 0.605933\n",
      "Iteration 83, loss = 0.15426947\n",
      "Validation score: 0.607234\n",
      "Iteration 84, loss = 0.15401957\n",
      "Validation score: 0.607946\n",
      "Iteration 85, loss = 0.15396701\n",
      "Validation score: 0.607663\n",
      "Iteration 86, loss = 0.15378445\n",
      "Validation score: 0.609344\n",
      "Iteration 87, loss = 0.15363165\n",
      "Validation score: 0.606458\n",
      "Iteration 88, loss = 0.15347848\n",
      "Validation score: 0.606216\n",
      "Iteration 89, loss = 0.15322658\n",
      "Validation score: 0.607185\n",
      "Iteration 90, loss = 0.15319152\n",
      "Validation score: 0.607700\n",
      "Iteration 91, loss = 0.15295912\n",
      "Validation score: 0.607468\n",
      "Iteration 92, loss = 0.15281162\n",
      "Validation score: 0.607255\n",
      "Iteration 93, loss = 0.15266360\n",
      "Validation score: 0.606576\n",
      "Iteration 94, loss = 0.15256593\n",
      "Validation score: 0.608032\n",
      "Iteration 95, loss = 0.15239660\n",
      "Validation score: 0.607831\n",
      "Iteration 96, loss = 0.15230300\n",
      "Validation score: 0.605205\n",
      "Iteration 97, loss = 0.15214136\n",
      "Validation score: 0.607168\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 98, loss = 0.15132504\n",
      "Validation score: 0.608960\n",
      "Iteration 99, loss = 0.15105737\n",
      "Validation score: 0.608014\n",
      "Iteration 100, loss = 0.15097972\n",
      "Validation score: 0.608394\n",
      "--- Evaluation on Test Set ---\n",
      "ROC-AUC: 0.9464\n",
      "PR-AUC: 0.9200\n",
      "F1-Score (at threshold 0.0): 0.6104\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.26      0.41     15033\n",
      "           1       0.44      1.00      0.61      8845\n",
      "\n",
      "    accuracy                           0.53     23878\n",
      "   macro avg       0.71      0.63      0.51     23878\n",
      "weighted avg       0.79      0.53      0.48     23878\n",
      "\n",
      "--- Training Deeper-Narrower Neural Network (Regressor with Scaled Target) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.34689197\n",
      "Validation score: 0.442145\n",
      "Iteration 2, loss = 0.25926646\n",
      "Validation score: 0.483742\n",
      "Iteration 3, loss = 0.24514217\n",
      "Validation score: 0.499624\n",
      "Iteration 4, loss = 0.23716006\n",
      "Validation score: 0.509488\n",
      "Iteration 5, loss = 0.23100432\n",
      "Validation score: 0.521970\n",
      "Iteration 6, loss = 0.22552221\n",
      "Validation score: 0.529863\n",
      "Iteration 7, loss = 0.22101741\n",
      "Validation score: 0.531552\n",
      "Iteration 8, loss = 0.21694689\n",
      "Validation score: 0.543645\n",
      "Iteration 9, loss = 0.21316118\n",
      "Validation score: 0.546506\n",
      "Iteration 10, loss = 0.21004569\n",
      "Validation score: 0.552264\n",
      "Iteration 11, loss = 0.20702354\n",
      "Validation score: 0.558116\n",
      "Iteration 12, loss = 0.20436636\n",
      "Validation score: 0.562654\n",
      "Iteration 13, loss = 0.20186923\n",
      "Validation score: 0.564672\n",
      "Iteration 14, loss = 0.19934871\n",
      "Validation score: 0.567310\n",
      "Iteration 15, loss = 0.19683718\n",
      "Validation score: 0.571277\n",
      "Iteration 16, loss = 0.19508042\n",
      "Validation score: 0.573662\n",
      "Iteration 17, loss = 0.19305742\n",
      "Validation score: 0.576335\n",
      "Iteration 18, loss = 0.19087356\n",
      "Validation score: 0.573308\n",
      "Iteration 19, loss = 0.18969362\n",
      "Validation score: 0.579210\n",
      "Iteration 20, loss = 0.18763566\n",
      "Validation score: 0.581870\n",
      "Iteration 21, loss = 0.18606133\n",
      "Validation score: 0.580911\n",
      "Iteration 22, loss = 0.18462444\n",
      "Validation score: 0.583130\n",
      "Iteration 23, loss = 0.18306206\n",
      "Validation score: 0.576111\n",
      "Iteration 24, loss = 0.18179333\n",
      "Validation score: 0.585425\n",
      "Iteration 25, loss = 0.18016953\n",
      "Validation score: 0.587784\n",
      "Iteration 26, loss = 0.17918042\n",
      "Validation score: 0.575113\n",
      "Iteration 27, loss = 0.17804705\n",
      "Validation score: 0.588782\n",
      "Iteration 28, loss = 0.17630492\n",
      "Validation score: 0.588616\n",
      "Iteration 29, loss = 0.17522131\n",
      "Validation score: 0.585383\n",
      "Iteration 30, loss = 0.17428514\n",
      "Validation score: 0.589161\n",
      "Iteration 31, loss = 0.17332885\n",
      "Validation score: 0.582985\n",
      "Iteration 32, loss = 0.17171506\n",
      "Validation score: 0.586541\n",
      "Iteration 33, loss = 0.17173005\n",
      "Validation score: 0.594020\n",
      "Iteration 34, loss = 0.16981177\n",
      "Validation score: 0.595127\n",
      "Iteration 35, loss = 0.16845651\n",
      "Validation score: 0.593066\n",
      "Iteration 36, loss = 0.16781693\n",
      "Validation score: 0.592514\n",
      "Iteration 37, loss = 0.16653627\n",
      "Validation score: 0.581921\n",
      "Iteration 38, loss = 0.16563200\n",
      "Validation score: 0.598751\n",
      "Iteration 39, loss = 0.16463396\n",
      "Validation score: 0.588988\n",
      "Iteration 40, loss = 0.16458703\n",
      "Validation score: 0.588813\n",
      "Iteration 41, loss = 0.16395292\n",
      "Validation score: 0.599279\n",
      "Iteration 42, loss = 0.16175955\n",
      "Validation score: 0.574637\n",
      "Iteration 43, loss = 0.16151181\n",
      "Validation score: 0.597626\n",
      "Iteration 44, loss = 0.16021363\n",
      "Validation score: 0.573916\n",
      "Iteration 45, loss = 0.15959260\n",
      "Validation score: 0.596019\n",
      "Iteration 46, loss = 0.16016086\n",
      "Validation score: 0.595088\n",
      "Iteration 47, loss = 0.15960355\n",
      "Validation score: 0.576189\n",
      "Iteration 48, loss = 0.15693121\n",
      "Validation score: 0.585450\n",
      "Iteration 49, loss = 0.15716644\n",
      "Validation score: 0.600022\n",
      "Iteration 50, loss = 0.15498462\n",
      "Validation score: 0.590999\n",
      "Iteration 51, loss = 0.15546207\n",
      "Validation score: 0.586722\n",
      "Iteration 52, loss = 0.15658914\n",
      "Validation score: 0.597353\n",
      "Iteration 53, loss = 0.15314983\n",
      "Validation score: 0.576779\n",
      "Iteration 54, loss = 0.15509484\n",
      "Validation score: 0.589950\n",
      "Iteration 55, loss = 0.15119128\n",
      "Validation score: 0.598904\n",
      "Iteration 56, loss = 0.15112144\n",
      "Validation score: 0.601194\n",
      "Iteration 57, loss = 0.15091393\n",
      "Validation score: 0.579033\n",
      "Iteration 58, loss = 0.15098174\n",
      "Validation score: 0.599794\n",
      "Iteration 59, loss = 0.14979864\n",
      "Validation score: 0.581658\n",
      "Iteration 60, loss = 0.14949516\n",
      "Validation score: 0.597235\n",
      "Iteration 61, loss = 0.14877117\n",
      "Validation score: 0.581342\n",
      "Iteration 62, loss = 0.14639242\n",
      "Validation score: 0.598319\n",
      "Iteration 63, loss = 0.14813668\n",
      "Validation score: 0.547241\n",
      "Iteration 64, loss = 0.14505773\n",
      "Validation score: 0.576481\n",
      "Iteration 65, loss = 0.14587438\n",
      "Validation score: 0.595464\n",
      "Iteration 66, loss = 0.14634196\n",
      "Validation score: 0.597597\n",
      "Iteration 67, loss = 0.14288708\n",
      "Validation score: 0.596654\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.002000\n",
      "Iteration 68, loss = 0.13435753\n",
      "Validation score: 0.600093\n",
      "Iteration 69, loss = 0.13216764\n",
      "Validation score: 0.598967\n",
      "Iteration 70, loss = 0.13155515\n",
      "Validation score: 0.599331\n",
      "Iteration 71, loss = 0.13105120\n",
      "Validation score: 0.597934\n",
      "Iteration 72, loss = 0.13078160\n",
      "Validation score: 0.599649\n",
      "Iteration 73, loss = 0.13052337\n",
      "Validation score: 0.598206\n",
      "Iteration 74, loss = 0.13020791\n",
      "Validation score: 0.596051\n",
      "Iteration 75, loss = 0.12984592\n",
      "Validation score: 0.597966\n",
      "Iteration 76, loss = 0.12969395\n",
      "Validation score: 0.595437\n",
      "Iteration 77, loss = 0.12936403\n",
      "Validation score: 0.596703\n",
      "Iteration 78, loss = 0.12900462\n",
      "Validation score: 0.596418\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000400\n",
      "Iteration 79, loss = 0.12736933\n",
      "Validation score: 0.597245\n",
      "Iteration 80, loss = 0.12716564\n",
      "Validation score: 0.596687\n",
      "Iteration 81, loss = 0.12698901\n",
      "Validation score: 0.596873\n",
      "Iteration 82, loss = 0.12695826\n",
      "Validation score: 0.597250\n",
      "Iteration 83, loss = 0.12684762\n",
      "Validation score: 0.596943\n",
      "Iteration 84, loss = 0.12678598\n",
      "Validation score: 0.596962\n",
      "Iteration 85, loss = 0.12677068\n",
      "Validation score: 0.595101\n",
      "Iteration 86, loss = 0.12680087\n",
      "Validation score: 0.597163\n",
      "Iteration 87, loss = 0.12658894\n",
      "Validation score: 0.596888\n",
      "Iteration 88, loss = 0.12654617\n",
      "Validation score: 0.596330\n",
      "Iteration 89, loss = 0.12645072\n",
      "Validation score: 0.597171\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000080\n",
      "Iteration 90, loss = 0.12610370\n",
      "Validation score: 0.596494\n",
      "Iteration 91, loss = 0.12603044\n",
      "Validation score: 0.596660\n",
      "Iteration 92, loss = 0.12601459\n",
      "Validation score: 0.596944\n",
      "Iteration 93, loss = 0.12599112\n",
      "Validation score: 0.596845\n",
      "Iteration 94, loss = 0.12598782\n",
      "Validation score: 0.596765\n",
      "Iteration 95, loss = 0.12596151\n",
      "Validation score: 0.597072\n",
      "Iteration 96, loss = 0.12598207\n",
      "Validation score: 0.596729\n",
      "Iteration 97, loss = 0.12592721\n",
      "Validation score: 0.596562\n",
      "Iteration 98, loss = 0.12593278\n",
      "Validation score: 0.596791\n",
      "Iteration 99, loss = 0.12592174\n",
      "Validation score: 0.596764\n",
      "Iteration 100, loss = 0.12589952\n",
      "Validation score: 0.596765\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000016\n",
      "--- Evaluation on Test Set ---\n",
      "ROC-AUC: 0.9443\n",
      "PR-AUC: 0.9134\n",
      "F1-Score (at threshold 0.0): 0.6222\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.29      0.45     15033\n",
      "           1       0.45      1.00      0.62      8845\n",
      "\n",
      "    accuracy                           0.55     23878\n",
      "   macro avg       0.72      0.64      0.54     23878\n",
      "weighted avg       0.79      0.55      0.51     23878\n",
      "\n",
      "Output directory 'US_accdnt_out' is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Accidents dataset loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/sld03hs57ws4phfw9pp0djl40000gn/T/ipykernel_95248/3752891255.py:554: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned[col].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and feature engineering complete.\n",
      "Data sampling complete. Main training set: 1200000, k-NN sample: 250000, SVM sample: 25000\n",
      "Preprocessor (ColumnTransformer) defined to handle numeric and categorical features.\n",
      "--- Starting Grid Search for DecisionTreeRegressor ---\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Grid Search completed in 44.29 seconds.\n",
      "Best parameters found: {'regressor__max_depth': 16, 'regressor__min_samples_leaf': 200}\n",
      "Best MAE on validation data: 1108.1941\n",
      "--- Evaluation on Test Set ---\n",
      "Mean Absolute Error (MAE): 1126.6994\n",
      "Root Mean Squared Error (RMSE): 17030.2212\n",
      "--- Starting Grid Search for KNeighborsRegressor ---\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Grid Search completed in 45.50 seconds.\n",
      "Best parameters found: {'regressor__n_neighbors': 5}\n",
      "Best MAE on validation data: 1289.4968\n",
      "--- Evaluation on Test Set ---\n",
      "Mean Absolute Error (MAE): 1355.2783\n",
      "Root Mean Squared Error (RMSE): 20329.2404\n",
      "--- Starting Grid Search for LinearSVR ---\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Grid Search completed in 154.52 seconds.\n",
      "Best parameters found: {'regressor__C': 10}\n",
      "Best MAE on validation data: 718.4181\n",
      "--- Evaluation on Test Set ---\n",
      "Mean Absolute Error (MAE): 772.7074\n",
      "Root Mean Squared Error (RMSE): 19154.5992\n",
      "--- Starting Grid Search for SVR (RBF) on 25000 samples ---\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Best parameters found: {'regressor__C': 10, 'regressor__gamma': 'scale'}\n",
      "Best MAE on validation data: 637.2433\n",
      "--- Evaluation on Test Set ---\n",
      "Mean Absolute Error (MAE): 771.6780\n",
      "Root Mean Squared Error (RMSE): 19154.7766\n",
      "--- Generating and Saving Learning Curves ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/sld03hs57ws4phfw9pp0djl40000gn/T/ipykernel_95248/3752891255.py:647: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\")); plt.show()\n",
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating and Saving NN Loss Curves ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/sld03hs57ws4phfw9pp0djl40000gn/T/ipykernel_95248/3752891255.py:674: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/mw/sld03hs57ws4phfw9pp0djl40000gn/T/ipykernel_95248/3752891255.py:662: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating and Saving Model Complexity (Validation) Curves ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/var/folders/mw/sld03hs57ws4phfw9pp0djl40000gn/T/ipykernel_95248/3752891255.py:698: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Final Summary Table ---\n",
      "--- Final Model Comparison (Regression) ---\n",
      "           Model          MAE          RMSE  Predict Time (s)\n",
      "3        RBF SVR   771.678009  19154.776627        509.091508\n",
      "2     Linear SVR   772.707418  19154.599236          0.164550\n",
      "0  Decision Tree  1126.699403  17030.221157          0.218677\n",
      "1           k-NN  1355.278320  20329.240419         31.539708\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Supervised/Online Learning — Hotel (classification) + US Accidents (regression)\n",
    "\n",
    "References:\n",
    "[1] N. C. António, A. Almeida, and L. Nunes, \"Hotel booking demand datasets,\" Data in Brief, vol. 22, pp. 41-49, 2019.\n",
    "[2] S. Moosavi, M. H. Samavatian, et al., \"A Countrywide Traffic Accident Dataset,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. Workshops, 2019.\n",
    "[3] T. Saito and M. Rehmsmeier, \"The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets,\" PLOS ONE, vol. 10, no. 3, p. e0118432, 2015.\n",
    "[4] T. Chen and C. Guestrin, \"XGBoost: A Scalable Tree Boosting System,\" in Proc. 22nd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, 2016, pp. 785-794.\n",
    "[5] J. H. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine,” Annals of Statistics, vol. 29, no. 5, pp. 1189–1232, 2001.\n",
    "[6] T. M. Mitchell, \"Machine Learning,\" McGraw-Hill, 1997.\n",
    "[7] J. R. Quinlan, \"Induction of Decision Trees,\" Machine Learning, 1986.\n",
    "[8] P. E. Cover and P. E. Hart, \"Nearest Neighbor Pattern Classification,\" IEEE Transactions on Information Theory, vol. 13, no. 1, pp. 21-27, 1967.\n",
    "[9] C. J. Cortes and V. Vapnik, \"Support-Vector Networks,\" Machine Learning, vol. 20, no. 3, pp. 273-297, 1995.\n",
    "[10] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning representations by back-propagating errors,\" Nature, vol. 323, no. 6088, pp. 533-536, 1986.\n",
    "[11] N. C. António, A. Almeida, and L. Nunes, \"Hotel booking demand datasets,\" Data in Brief, vol. 22, pp. 41-49, 2019.\n",
    "[12] S. Moosavi et al., “US Accidents (since 2016),” [Online]. Available: Kaggle US Accidents.\n",
    "\"\"\"\n",
    "\n",
    "# Use a non-interactive backend for script runs (before importing pyplot)\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use(\"Agg\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython  # noqa\n",
    "    _IN_IPY = get_ipython() is not None\n",
    "except Exception:\n",
    "    _IN_IPY = False\n",
    "\n",
    "# memory_profiler: safe @mp_profile decorator & programmatic runner\n",
    "try:\n",
    "    from memory_profiler import profile as mp_profile, memory_usage as _memory_usage  # noqa\n",
    "    _HAVE_MEMPROF = True\n",
    "except Exception:\n",
    "    _HAVE_MEMPROF = False\n",
    "    def mp_profile(func):  # noqa\n",
    "        return func\n",
    "    def _memory_usage(*a, **k):  # noqa\n",
    "        return None\n",
    "\n",
    "# tqdm shim (fallback to identity if tqdm missing)\n",
    "try:\n",
    "    from tqdm import tqdm  # noqa\n",
    "except Exception:\n",
    "    def tqdm(x, **k):  # noqa\n",
    "        return x\n",
    "\n",
    "# display shim: if IPython not available, print instead\n",
    "try:\n",
    "    from IPython.display import display as ipy_display  # noqa\n",
    "except Exception:\n",
    "    def ipy_display(x):  # noqa\n",
    "        try:\n",
    "            import pandas as _pd\n",
    "            if isinstance(x, _pd.DataFrame):\n",
    "                print(x.head(20).to_string())\n",
    "            elif hasattr(x, \"to_string\"):\n",
    "                print(x.to_string())\n",
    "            else:\n",
    "                print(x)\n",
    "        except Exception:\n",
    "            print(x)\n",
    "\n",
    "# Programmatic memory-profiling runner for scripts\n",
    "import os, json, time, sys\n",
    "def run_with_memprofile(func, *args, enable=None, label=None, log_dir=\"./ol_outputs/logs\", **kwargs):\n",
    "    \"\"\"    Run a function and (optionally) record peak memory using memory_profiler.memory_usage.\n",
    "    - enable=True or set env ENABLE_MEMPROF=1 to activate\n",
    "    - returns (retval, peak_mb or None)\n",
    "    \"\"\"    \n",
    "    enabled = enable if enable is not None else (os.getenv(\"ENABLE_MEMPROF\", \"0\") == \"1\")\n",
    "    if not enabled or not _HAVE_MEMPROF:\n",
    "        return func(*args, **kwargs), None\n",
    "    mem_list, retval = _memory_usage((func, args, kwargs), retval=True, interval=0.1)\n",
    "    try:\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        with open(os.path.join(log_dir, \"memory_profile.jsonl\"), \"a\", encoding=\"utf-8\") as f:\n",
    "            rec = {\"ts\": time.time(), \"label\": label or getattr(func, \"__name__\", \"func\"),\n",
    "                   \"peak_mb\": float(max(mem_list) if mem_list else -1.0)}\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "    except Exception as _e:\n",
    "        print(\"[memprof] could not write log:\", _e, file=sys.stderr)\n",
    "    return retval, float(max(mem_list) if mem_list else -1.0)\n",
    "# ================================================================\n",
    "\n",
    "# %%\n",
    "# Install if needed else comment\n",
    "# [shell-magic removed] !pip install category_encoders\n",
    "# [shell-magic removed] !pip install memory_profiler\n",
    "# [notebook-magic removed] %load_ext memory_profiler\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Scikit-learn model selection and metrics\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV, learning_curve, validation_curve\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, f1_score, roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data loading and cleaning\n",
    "try:\n",
    "    df_hotel = pd.read_csv('hotel_bookings.csv')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'hotel_bookings.csv' not found.\")\n",
    "    df_hotel = pd.DataFrame()\n",
    "\n",
    "if not df_hotel.empty:\n",
    "    leakage_cols = ['reservation_status', 'reservation_status_date']\n",
    "    df_hotel_cleaned = df_hotel.drop(columns=leakage_cols).copy()  # Added .copy()\n",
    "\n",
    "    # Fix pandas FutureWarning by using .loc\n",
    "    df_hotel_cleaned.loc[:, 'agent'] = df_hotel_cleaned['agent'].fillna(0)\n",
    "    df_hotel_cleaned.loc[:, 'company'] = df_hotel_cleaned['company'].fillna(0) \n",
    "    df_hotel_cleaned.loc[:, 'country'] = df_hotel_cleaned['country'].fillna(df_hotel_cleaned['country'].mode()[0])\n",
    "    df_hotel_cleaned.loc[:, 'children'] = df_hotel_cleaned['children'].fillna(0)\n",
    "    \n",
    "    df_hotel_cleaned[['children', 'agent', 'company']] = df_hotel_cleaned[['children', 'agent', 'company']].astype(int)\n",
    "    print(\"Data cleaning complete.\")\n",
    "\n",
    "# Helper functions to evaluate the model\n",
    "def evaluate_model(y_true, y_scores, threshold=0.5):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    if np.min(y_scores) < 0:\n",
    "        threshold = 0.0\n",
    "    y_pred_class = (y_scores >= threshold).astype(int)\n",
    "    f1 = f1_score(y_true, y_pred_class)\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "    print(f\"F1-Score (at threshold {threshold}): {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred_class))\n",
    "\n",
    "def train_and_evaluate_model(pipeline, param_grid, X_train, y_train, X_test, y_test):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "    print(f\"--- Starting Grid Search for {pipeline.steps[-1][1].__class__.__name__} ---\")\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Grid Search completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_scores = best_model.decision_function(X_test)\n",
    "    print(\"--- Evaluation on Test Set ---\")\n",
    "    evaluate_model(y_test, y_scores)\n",
    "    return best_model\n",
    "\n",
    "# Target encoding for high cardinality and On-hot for low cardinality\n",
    "X = df_hotel_cleaned.drop(columns=['is_canceled'])\n",
    "y = df_hotel_cleaned['is_canceled']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "# We define high-cardinality features to be target encoded\n",
    "target_encode_features = ['country', 'agent', 'company']\n",
    "# All other non-numeric features will be one-hot encoded\n",
    "ohe_features = [col for col in X_train.select_dtypes(exclude=np.number).columns if col not in target_encode_features]\n",
    "\n",
    "# UPDATED: Create the new preprocessor with three steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ohe_features),\n",
    "        ('te', TargetEncoder() if TargetEncoder is not None else OneHotEncoder(handle_unknown='ignore', sparse_output=False), target_encode_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "print(\"Final data preparation is complete. Preprocessor now includes Target Encoding.\")\n",
    "\n",
    "# Run the experiments\n",
    "\n",
    "# --- Decision Tree --- #\n",
    "dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42))])\n",
    "dt_param_grid = {'classifier__max_depth': [8, 16], 'classifier__min_samples_leaf': [100, 200], 'classifier__class_weight': ['balanced']}\n",
    "best_dt_model = train_and_evaluate_model(dt_pipeline, dt_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# --- k-Nearest Neighbors ---\n",
    "knn_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "knn_param_grid = {'classifier__n_neighbors': [5, 11]}\n",
    "best_knn_model = train_and_evaluate_model(knn_pipeline, knn_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Train and evaluate model\n",
    "\n",
    "# %%\n",
    "# Neural Network\n",
    "\n",
    "# adding just in case it dosesnt show up in the initial import\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print(\"--- Training Shallow-Wide Neural Network ---\")\n",
    "nn_shallow_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', MLPClassifier(hidden_layer_sizes=(512, 512), solver='sgd', learning_rate_init=0.01, batch_size=512, max_iter=100, early_stopping=True, random_state=42, verbose=False))])\n",
    "# [notebook-magic removed] %memit nn_shallow_pipeline.fit(X_train, y_train)\n",
    "# First fit the pipeline\n",
    "nn_shallow_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Then make predictions \n",
    "y_scores_shallow = nn_shallow_pipeline.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_scores_shallow)\n",
    "\n",
    "print(\"--- Training Deeper-Narrower Neural Network ---\")\n",
    "nn_deep_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', MLPClassifier(hidden_layer_sizes=(256, 256, 128, 128), solver='sgd', learning_rate_init=0.01, batch_size=512, max_iter=100, early_stopping=True, random_state=42, verbose=False))])\n",
    "# [notebook-magic removed] %memit nn_deep_pipeline.fit(X_train, y_train)\n",
    "nn_deep_pipeline.fit(X_train, y_train)\n",
    "print(\"--- Evaluation of Deeper-Narrower NN ---\")\n",
    "y_scores_deep = nn_deep_pipeline.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_scores_deep)\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "\n",
    "# --- Linear SVM --- #\n",
    "linear_svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearSVC(random_state=42, max_iter=2000))])\n",
    "linear_svm_param_grid = {'classifier__C': [0.1, 1, 10], 'classifier__class_weight': ['balanced']}\n",
    "best_linear_svm_model = train_and_evaluate_model(linear_svm_pipeline, linear_svm_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# --- RBF SVM --- #\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=20000, random_state=42, stratify=y_train)\n",
    "rbf_svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', SVC(kernel='rbf', probability=True, random_state=42))])\n",
    "rbf_svm_param_grid = {'classifier__C': [0.5, 2, 8], 'classifier__gamma': ['scale', 'auto'], 'classifier__class_weight': ['balanced']}\n",
    "random_search_rbf = RandomizedSearchCV(rbf_svm_pipeline, param_distributions=rbf_svm_param_grid, n_iter=6, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
    "print(f\"--- Starting Randomized Search for RBF SVM on {len(X_train_sample)} samples ---\")\n",
    "\n",
    "# FIT the model first before accessing best_estimator_\n",
    "random_search_rbf.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Now you can access best_estimator_\n",
    "best_rbf_svm_model = random_search_rbf.best_estimator_\n",
    "print(\"Best parameters found:\", random_search_rbf.best_params_)\n",
    "y_scores_rbf = best_rbf_svm_model.predict_proba(X_test)[:, 1]\n",
    "print(\"--- Evaluation on Test Set ---\")\n",
    "evaluate_model(y_test, y_scores_rbf)\n",
    "\n",
    "# Add this helper function to safely access search results:\n",
    "def safe_get_best_model(search_cv, X_train, y_train):\n",
    "    \"\"\"Safely fit and get best model from search CV object.\"\"\"\n",
    "    if not hasattr(search_cv, 'best_estimator_') or search_cv.best_estimator_ is None:\n",
    "        print(\"Fitting search CV object...\")\n",
    "        search_cv.fit(X_train, y_train)\n",
    "    return search_cv.best_estimator_\n",
    "\n",
    "# %%\n",
    "# Plotting\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, cv=5, n_jobs=-1):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=np.linspace(.1, 1.0, 5), scoring='roc_auc'\n",
    "    )\n",
    "    train_scores_mean, test_scores_mean = np.mean(train_scores, axis=1), np.mean(test_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"ROC-AUC Score\")\n",
    "    plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f\"Hotel_{title.replace(' ', '_').lower()}.png\")  # Save as PNG\n",
    "    plt.close()  # Close the figure to avoid displaying it\n",
    "\n",
    "def plot_validation_curve(estimator, title, X, y, param_name, param_range, cv=5, n_jobs=-1):\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\"roc_auc\", n_jobs=n_jobs\n",
    "    )\n",
    "    train_scores_mean, test_scores_mean = np.mean(train_scores, axis=1), np.mean(test_scores, axis=1)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.grid()\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\")\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(f\"Hotel_{title.replace(' ', '_').lower()}.png\")  # Save as PNG\n",
    "    plt.close()  # Close the figure to avoid displaying it\n",
    "\n",
    "\n",
    "def plot_roc_pr_curves(models, X_test, y_test, save_dir=\".\"):\n",
    "    \"\"\"    Plots ROC and PR curves for a dictionary of trained models.\n",
    "    \"\"\"    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_scores = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_scores = model.decision_function(X_test)\n",
    "\n",
    "        RocCurveDisplay.from_predictions(y_test, y_scores, name=name, ax=ax1)\n",
    "        PrecisionRecallDisplay.from_predictions(y_test, y_scores, name=name, ax=ax2)\n",
    "\n",
    "    ax1.set_title(\"ROC Curves\")\n",
    "    ax2.set_title(\"Precision-Recall Curves\")\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Hotel_ROC_and_PR_Curves.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_nn_loss_curve(nn_model, title, save_dir=\".\"):\n",
    "    \"\"\"    Plots the training loss curve from a trained scikit-learn MLP model.\n",
    "    \"\"\"    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(nn_model.loss_curve_)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"--- Generating Learning Curves ---\")\n",
    "plot_learning_curve(best_dt_model, \"Learning Curve (Decision Tree)\", X_train, y_train)\n",
    "plot_learning_curve(best_knn_model, \"Learning Curve (k-NN)\", X_train, y_train)\n",
    "plot_learning_curve(best_linear_svm_model, \"Learning Curve (Linear SVM)\", X_train, y_train)\n",
    "plot_learning_curve(best_rbf_svm_model, \"Learning Curve (RBF SVM)\", X_train_sample, y_train_sample)\n",
    "\n",
    "print(\"--- Generating Model Complexity Curve for Decision Tree ---\")\n",
    "dt_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced'))])\n",
    "param_range_depth = np.arange(2, 25)\n",
    "plot_validation_curve(dt_pipeline_untuned, \"Validation Curve (Decision Tree vs. max_depth)\", X_train, y_train, param_name=\"classifier__max_depth\", param_range=param_range_depth)\n",
    "models_to_plot = {\n",
    "    \"Decision Tree\": best_dt_model,\n",
    "    \"k-NN\": best_knn_model,\n",
    "    \"Linear SVM\": best_linear_svm_model,\n",
    "    \"RBF SVM\": best_rbf_svm_model,\n",
    "    \"Shallow NN\": nn_shallow_pipeline,\n",
    "    \"Deep NN\": nn_deep_pipeline\n",
    "}\n",
    "plot_roc_pr_curves(models_to_plot, X_test, y_test)\n",
    "\n",
    "print(\"--- Generating NN Loss Curves ---\")\n",
    "plot_nn_loss_curve(nn_shallow_pipeline.named_steps['classifier'], \"NN classifier Loss Curve (Shallow)\")\n",
    "plot_nn_loss_curve(nn_deep_pipeline.named_steps['classifier'], \"NN classifier Loss Curve (Deep)\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# --- FINAL SUMMARY TABLE ---\n",
    "results, models = [], {\"Decision Tree\": best_dt_model, \"k-NN\": best_knn_model, \"Linear SVM\": best_linear_svm_model, \"RBF SVM\": best_rbf_svm_model, \"Shallow NN\": nn_shallow_pipeline, \"Deep NN\": nn_deep_pipeline}\n",
    "\n",
    "print(\"--- Generating Final Summary Table ---\")\n",
    "for name, model in models.items():\n",
    "\n",
    "    # Time the prediction step\n",
    "    start_time = time.time()\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_scores = model.decision_function(X_test)\n",
    "    end_time = time.time()\n",
    "    predict_time = end_time - start_time\n",
    "\n",
    "    # Calculate metrics\n",
    "    roc_auc, (precision, recall, _) = roc_auc_score(y_test, y_scores), precision_recall_curve(y_test, y_scores)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    threshold = 0.0 if \"Linear SVM\" in name else 0.5 # RBF SVM has predict_proba, so it uses 0.5\n",
    "    f1 = f1_score(y_test, (y_scores >= threshold).astype(int))\n",
    "\n",
    "    # Append all results\n",
    "    results.append({\"Model\": name, \"ROC-AUC\": roc_auc, \"PR-AUC\": pr_auc, \"F1-Score\": f1, \"Predict Time (s)\": predict_time})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"--- Final Model Comparison ---\")\n",
    "print(df_results.sort_values(by='ROC-AUC', ascending=False))\n",
    "\n",
    "# %%\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Neural Network\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Create and fit a scaler for the target variable ---\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "print(\"--- Training Shallow-Wide Neural Network (Regressor with Scaled Target) ---\")\n",
    "\n",
    "nn_shallow_reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MLPRegressor(\n",
    "        hidden_layer_sizes=(512, 512),\n",
    "        solver='sgd',\n",
    "        learning_rate='adaptive', # Keep the adaptive rate\n",
    "        learning_rate_init=0.01,\n",
    "        batch_size=1024,\n",
    "        max_iter=100,  # Increased from 15\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the model using the scaled y_train\n",
    "# [notebook-magic removed] %memit nn_shallow_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
    "\n",
    "\n",
    "nn_shallow_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
    "y_pred_scaled = nn_shallow_reg_pipeline.predict(X_test)\n",
    "\n",
    "# Convert the scaled predictions back to the original, unscaled y_test\n",
    "y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "\n",
    "print(\"--- Evaluation on Test Set ---\")\n",
    "# Compare the unscaled predictions to the original, unscaled y_test\n",
    "evaluate_model(y_test, y_pred_unscaled.ravel())\n",
    "\n",
    "\n",
    "# NOTE: We are reusing the 'y_scaler' that was fit on y_train in the previous step.\n",
    "\n",
    "# --- Define and train the DEEP NN pipeline on the SCALED target data ---\n",
    "print(\"--- Training Deeper-Narrower Neural Network (Regressor with Scaled Target) ---\")\n",
    "\n",
    "nn_deep_reg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 256, 128, 128), # Deeper-narrower architecture\n",
    "        solver='sgd',\n",
    "        learning_rate='adaptive',\n",
    "        learning_rate_init=0.01,\n",
    "        batch_size=1024,\n",
    "        max_iter=100,  # Increased from 15\n",
    "        early_stopping=True,\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the model using the same scaled y_train\n",
    "# [notebook-magic removed] %memit nn_deep_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
    "\n",
    "nn_deep_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
    "# The model will predict in the scaled space\n",
    "y_pred_deep_scaled = nn_deep_reg_pipeline.predict(X_test)\n",
    "\n",
    "# Convert the scaled predictions back to the original units (minutes) using the same scaler\n",
    "y_pred_deep_unscaled = y_scaler.inverse_transform(y_pred_deep_scaled.reshape(-1, 1))\n",
    "\n",
    "\n",
    "print(\"--- Evaluation on Test Set ---\")\n",
    "# Compare the unscaled predictions to the original, unscaled y_test\n",
    "evaluate_model(y_test, y_pred_deep_unscaled.ravel())\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn preprocessing and pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn regression models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Scikit-learn model selection and metrics for regression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, learning_curve, validation_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "output_dir = \"US_accdnt_out\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory '{output_dir}' is ready.\")\n",
    "\n",
    "# Helper functions for model evaluation\n",
    "def evaluate_regression_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "def train_and_evaluate_regression_model(pipeline, param_grid, X_train, y_train, X_test, y_test):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "    print(f\"--- Starting Grid Search for {pipeline.steps[-1][1].__class__.__name__} ---\")\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Grid Search completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best MAE on validation data: {-grid_search.best_score_:.4f}\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(\"--- Evaluation on Test Set ---\")\n",
    "    evaluate_regression_model(y_test, y_pred)\n",
    "    return best_model\n",
    "\n",
    "# Data loading and cleaning\n",
    "try:\n",
    "    df_accidents = pd.read_csv('US_Accidents_March23.csv')\n",
    "    print(\"US Accidents dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Accidents dataset file not found.\")\n",
    "    df_accidents = pd.DataFrame()\n",
    "\n",
    "if not df_accidents.empty:\n",
    "    cols_to_drop = ['ID', 'Weather_Timestamp', 'Airport_Code', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'City', 'Street', 'Zipcode', 'County', 'Country', 'State', 'Timezone', 'Weather_Condition']\n",
    "    df_cleaned = df_accidents.drop(columns=cols_to_drop)\n",
    "    # Use format='mixed' to handle variations in datetime strings\n",
    "    df_cleaned['Start_Time'] = pd.to_datetime(df_cleaned['Start_Time'], format='mixed')\n",
    "    df_cleaned['End_Time'] = pd.to_datetime(df_cleaned['End_Time'], format='mixed')\n",
    "    df_cleaned['Duration'] = (df_cleaned['End_Time'] - df_cleaned['Start_Time']).dt.total_seconds() / 60\n",
    "    df_cleaned = df_cleaned.drop(columns=['Start_Time', 'End_Time'])\n",
    "    df_cleaned = df_cleaned[df_cleaned['Duration'] > 0]\n",
    "\n",
    "    # Drop the 'Description' column as it contains string values and is not used in modeling\n",
    "    if 'Description' in df_cleaned.columns:\n",
    "        df_cleaned = df_cleaned.drop(columns=['Description'])\n",
    "\n",
    "    # Identify categorical columns for imputation\n",
    "    categorical_cols = df_cleaned.select_dtypes(include='object').columns\n",
    "\n",
    "    # Impute missing values in categorical columns with 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        df_cleaned[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Drop rows with any remaining missing values (should only be numeric now)\n",
    "    df_cleaned.dropna(inplace=True)\n",
    "\n",
    "    bool_cols = df_cleaned.select_dtypes(include='bool').columns\n",
    "    df_cleaned[bool_cols] = df_cleaned[bool_cols].astype(int)\n",
    "    num_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
    "    df_cleaned[num_cols] = df_cleaned[num_cols].astype(np.float32)\n",
    "\n",
    "\n",
    "    print(\"Data cleaning and feature engineering complete.\")\n",
    "\n",
    "# Sampling and Data preparation\n",
    "df_sample, _ = train_test_split(df_cleaned, train_size=1500000, stratify=df_cleaned['Severity'], random_state=42)\n",
    "X = df_sample.drop(columns=['Duration', 'Severity'])\n",
    "y = df_sample['Duration']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_svm, _, y_train_svm, _ = train_test_split(X_train, y_train, train_size=25000, random_state=42)\n",
    "X_train_knn, _, y_train_knn, _ = train_test_split(X_train, y_train, train_size=250000, random_state=42)\n",
    "\n",
    "# Define numeric features based on the updated X_train\n",
    "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "# Identify all non-numeric columns as categorical features\n",
    "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "# Create a ColumnTransformer to handle different feature types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (if any)\n",
    ")\n",
    "\n",
    "print(f\"Data sampling complete. Main training set: {len(X_train)}, k-NN sample: {len(X_train_knn)}, SVM sample: {len(X_train_svm)}\")\n",
    "print(\"Preprocessor (ColumnTransformer) defined to handle numeric and categorical features.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Run the Experiments\n",
    "# --- Decision Tree Regressor ---\n",
    "dt_reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))])\n",
    "dt_reg_param_grid = {'regressor__max_depth': [8, 16], 'regressor__min_samples_leaf': [200, 400]}\n",
    "dt_reg_pipeline.fit(X_train, y_train)  # Initial fit to avoid refit issues\n",
    "best_dt_reg_model = train_and_evaluate_regression_model(dt_reg_pipeline, dt_reg_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# --- k-NN Regressor --- (Uses k-NN sample)\n",
    "knn_reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_jobs=-1))])\n",
    "knn_reg_param_grid = {'regressor__n_neighbors': [5, 11]}\n",
    "knn_reg_pipeline.fit(X_train_knn, y_train_knn)  # Initial fit to avoid refit issues\n",
    "best_knn_reg_model = train_and_evaluate_regression_model(knn_reg_pipeline, knn_reg_param_grid, X_train_knn, y_train_knn, X_test, y_test)\n",
    "\n",
    "# --- Linear SVR ---\n",
    "linear_svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearSVR(random_state=42, max_iter=2000))])\n",
    "linear_svr_param_grid = {'regressor__C': [0.1, 1, 10]}\n",
    "linear_svr_pipeline.fit(X_train, y_train)  # Initial fit to avoid refit issues\n",
    "best_linear_svr_model = train_and_evaluate_regression_model(linear_svr_pipeline, linear_svr_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# --- RBF SVR (Optimized) --- (Uses SVM sample)\n",
    "rbf_svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR(kernel='rbf'))])\n",
    "rbf_svr_param_grid = {'regressor__C': [1, 10], 'regressor__gamma': ['scale']}\n",
    "# Changed RandomizedSearchCV to GridSearchCV\n",
    "rbf_svr_pipeline.fit(X_train_svm, y_train_svm)  # Initial fit to avoid refit issues\n",
    "grid_search_svr = GridSearchCV(rbf_svr_pipeline, param_grid=rbf_svr_param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "print(f\"--- Starting Grid Search for SVR (RBF) on {len(X_train_svm)} samples ---\")\n",
    "grid_search_svr.fit(X_train_svm, y_train_svm)\n",
    "best_rbf_svr_model = grid_search_svr.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search_svr.best_params_)\n",
    "print(f\"Best MAE on validation data: {-grid_search_svr.best_score_:.4f}\")\n",
    "# Evaluate on test set \n",
    "y_pred_rbf = best_rbf_svr_model.predict(X_test)\n",
    "print(\"--- Evaluation on Test Set ---\"); evaluate_regression_model(y_test, y_pred_rbf)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curve_regression(estimator, title, X, y, cv=3, n_jobs=-1, save_dir=\".\"):\n",
    "    # Calculate training and cross-validation scores\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=np.linspace(.1, 1.0, 5), scoring='neg_mean_absolute_error')\n",
    "    train_scores_mean, test_scores_mean = -np.mean(train_scores, axis=1), -np.mean(test_scores, axis=1)\n",
    "    # Plot learning curve\n",
    "    plt.figure(); plt.title(title); plt.xlabel(\"Training examples\"); plt.ylabel(\"Mean Absolute Error\"); plt.grid()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    # Save and show plot\n",
    "    plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\")); plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title, save_dir=\".\"):\n",
    "    # Plots the residuals of a regression model.\n",
    "    residuals = y_true - y_pred\n",
    "    # Scatter plot of residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.2)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    # Save and show plot\n",
    "    plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_nn_loss_curve(nn_model, title, save_dir=\".\"):\n",
    "    # Plots the training loss curve from a trained scikit-learn MLP model.\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(nn_model.loss_curve_)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    # Save and show plot\n",
    "    plt.savefig(os.path.join(save_dir, f\"{title.replace(' ', '_')}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_validation_curve_regression(estimator, title, X, y, param_name, param_range, cv=3, n_jobs=-1, save_dir=\".\"):\n",
    "    # Calculate training and cross-validation scores for validation curve\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator, X, y, param_name=param_name, param_range=param_range,\n",
    "        cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "\n",
    "    # Plot validation curve\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.grid()\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\")\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    # Save and show plot\n",
    "    plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate and Save Learning Curves\n",
    "print(\"--- Generating and Saving Learning Curves ---\")\n",
    "plot_learning_curve_regression(best_dt_reg_model, \"Learning Curve (Decision Tree)\", X_train, y_train, save_dir=output_dir)\n",
    "plot_learning_curve_regression(best_knn_reg_model, \"Learning Curve (k-NN)\", X_train_knn, y_train_knn, save_dir=output_dir)\n",
    "plot_learning_curve_regression(best_linear_svr_model, \"Learning Curve (Linear SVR)\", X_train, y_train, save_dir=output_dir)\n",
    "plot_learning_curve_regression(best_rbf_svr_model, \"Learning Curve (RBF SVR)\", X_train_svm, y_train_svm, save_dir=output_dir)\n",
    "\n",
    "# Generate and Save NN Loss Curves\n",
    "print(\"--- Generating and Saving NN Loss Curves ---\")\n",
    "plot_nn_loss_curve(nn_shallow_reg_pipeline.named_steps['regressor'],\n",
    "                   \"NN Regressor Loss Curve (Shallow)\",\n",
    "                   save_dir=output_dir)\n",
    "\n",
    "plot_nn_loss_curve(nn_deep_reg_pipeline.named_steps['regressor'],\n",
    "                   \"NN Regressor Loss Curve (Deep)\",\n",
    "                   save_dir=output_dir)\n",
    "\n",
    "# Generating residual plots\n",
    "y_pred_dt = best_dt_reg_model.predict(X_test)\n",
    "plot_residuals(y_test, y_pred_dt, \"Residuals Plot (Decision Tree)\", save_dir=output_dir)\n",
    "\n",
    "# Generating Model Complexity (Validation) Curves\n",
    "print(\"--- Generating and Saving Model Complexity (Validation) Curves ---\")\n",
    "\n",
    "# Validation Curve for Decision Tree Regressor\n",
    "dt_reg_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))])\n",
    "param_range_depth = np.arange(4, 25, 4) # Test depths from 4 to 24\n",
    "\n",
    "plot_validation_curve_regression(\n",
    "    dt_reg_pipeline_untuned,\n",
    "    \"Validation Curve (Decision Tree vs max_depth)\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    param_name=\"regressor__max_depth\",\n",
    "    param_range=param_range_depth,\n",
    "    save_dir=output_dir\n",
    ")\n",
    "\n",
    "# Validation Curve for k-NN Regressor\n",
    "# Note: This runs on the smaller k-NN subsample for efficiency\n",
    "knn_reg_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_jobs=-1))])\n",
    "param_range_k = [3, 5, 8, 11, 15, 21] # Test various numbers of neighbors\n",
    "\n",
    "plot_validation_curve_regression(\n",
    "    knn_reg_pipeline_untuned,\n",
    "    \"Validation Curve (k-NN vs n_neighbors)\",\n",
    "    X_train_knn, # Using the k-NN subsample\n",
    "    y_train_knn,\n",
    "    param_name=\"regressor__n_neighbors\",\n",
    "    param_range=param_range_k,\n",
    "    save_dir=output_dir\n",
    ")\n",
    "\n",
    "# Validation Curve for Linear SVR\n",
    "linear_svr_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearSVR(random_state=42, max_iter=2000))])\n",
    "# Test the regularization parameter C on a logarithmic scale\n",
    "param_range_c_linear = np.logspace(-2, 2, 5) # e.g., [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "plot_validation_curve_regression(\n",
    "    linear_svr_pipeline_untuned,\n",
    "    \"Validation Curve (Linear SVR vs C)\",\n",
    "    X_train,\n",
    "    y_train,\n",
    "    param_name=\"regressor__C\",\n",
    "    param_range=param_range_c_linear,\n",
    "    save_dir=output_dir\n",
    ")\n",
    "\n",
    "# Validation Curve for RBF SVR\n",
    "# IMPORTANT: This MUST be run on the smallest SVM subsample to be computationally feasible.\n",
    "rbf_svr_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR(kernel='rbf'))])\n",
    "param_range_c_rbf = np.logspace(-1, 2, 4) # e.g., [0.1, 1, 10, 100]\n",
    "\n",
    "plot_validation_curve_regression(\n",
    "    rbf_svr_pipeline_untuned,\n",
    "    \"Validation Curve (RBF SVR vs C)\",\n",
    "    X_train_svm, # Using the smallest SVM subsample\n",
    "    y_train_svm,\n",
    "    param_name=\"regressor__C\",\n",
    "    param_range=param_range_c_rbf,\n",
    "    save_dir=output_dir\n",
    ")\n",
    "\n",
    "# Final Summary Table (Regression)\n",
    "results, models_reg = [], {\"Decision Tree\": best_dt_reg_model, \"k-NN\": best_knn_reg_model, \"Linear SVR\": best_linear_svr_model, \"RBF SVR\": best_rbf_svr_model}\n",
    "print(\"--- Generating Final Summary Table ---\")\n",
    "for name, model in models_reg.items():\n",
    "    start_time = time.time(); y_pred = model.predict(X_test); end_time = time.time()\n",
    "    predict_time = end_time - start_time\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"Predict Time (s)\": predict_time})\n",
    "df_results_reg = pd.DataFrame(results)\n",
    "print(\"--- Final Model Comparison (Regression) ---\")\n",
    "print(df_results_reg.sort_values(by='MAE', ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e3cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OL] Torch device: mps (cuda? False)\n",
      "[FAST] mode=False device=mps budgets(BUDGET_UPDATES=200, RO={'RHC': 2000, 'SA': 4000, 'GA': 8000, 'GA_pop': 40})\n",
      "[OL] Adapter bound? -> True\n",
      "[OL] Parity meta logged -> ./ol_outputs/logs/parity_meta.json\n",
      "[OL] Parity meta logged -> ./ol_outputs/logs/parity_meta.json\n",
      "[OL] Hotel MLP: MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=81, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "[OL] Accidents MLP: MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=93, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[OL] Params total=18,882 | trainable(last 1)=130\n",
      "[OL/Part1] Saved RO summary -> ./ol_outputs/logs/ro_summary_hotel.csv\n",
      "[OL/Part1] Saved RO curves -> ./ol_outputs/figs/ro_progress_hotel.png\n",
      "[OL] Params total=179,713 | trainable(last 1)=257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 719\u001b[39m\n\u001b[32m    716\u001b[39m     run_part1_RO(m_cls, H_loaders, task=\u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m, out_dim=(\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_BCE_FOR_BINARY \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m2\u001b[39m),\n\u001b[32m    717\u001b[39m                  base_tag=\u001b[33m\"\u001b[39m\u001b[33mhotel\u001b[39m\u001b[33m\"\u001b[39m, k_unfrozen=\u001b[32m1\u001b[39m, ro_cfg=RO_CFG)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RUN_ACCIDENTS \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mA_loaders\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mm_reg\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[43mrun_part1_RO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mbase_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccidents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_unfrozen\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mro_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRO_CFG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[38;5;66;03m# PART 2 — Optimizer ablations (equal budgets; same splits)\u001b[39;00m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mH_loaders\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mm_cls\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 588\u001b[39m, in \u001b[36mrun_part1_RO\u001b[39m\u001b[34m(model, loaders, task, out_dim, base_tag, k_unfrozen, ro_cfg)\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m validation_objective(m, loaders[\u001b[32m1\u001b[39m], task, out_dim)\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# Profile each RO algo\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m (ret_rhc, t_rhc, mb_rhc) = \u001b[43mprofile_call\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPart1::\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_tag\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m::RHC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mRHC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mFAST_MODE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mro_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRHC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    590\u001b[39m xr, fr, hr = ret_rhc\n\u001b[32m    591\u001b[39m (ret_sa,  t_sa,  mb_sa ) = profile_call(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPart1::\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m::SA\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    592\u001b[39m                                         SA, f, x0, \u001b[32m1.0\u001b[39m, \u001b[32m0.997\u001b[39m, \u001b[32m0.05\u001b[39m, ro_cfg[\u001b[33m\"\u001b[39m\u001b[33mSA\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mprofile_call\u001b[39m\u001b[34m(label, func, enable, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     peak = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mmax\u001b[39m(mem_list) \u001b[38;5;28;01mif\u001b[39;00m mem_list \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1.0\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     retval = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m dt = time.perf_counter() - t0\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 538\u001b[39m, in \u001b[36mRHC\u001b[39m\u001b[34m(f, x0, step, restarts, budget)\u001b[39m\n\u001b[32m    536\u001b[39m improved=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m     c = x + step*np.random.randn(*x.shape); fc = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m; evals += \u001b[32m1\u001b[39m\n\u001b[32m    539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fc < fx: x, fx = c, fc; improved=\u001b[38;5;28;01mTrue\u001b[39;00m; hist.append(fx)\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m evals>=budget: \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 585\u001b[39m, in \u001b[36mrun_part1_RO.<locals>.f\u001b[39m\u001b[34m(vec)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    584\u001b[39m     unpack_to_last(vec.astype(np.float32), last, Wshape)\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidation_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mvalidation_objective\u001b[39m\u001b[34m(model, val_loader, task, out_dim)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_objective\u001b[39m(model, val_loader, task, out_dim):\n\u001b[32m    525\u001b[39m     model.eval()\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     vL, _ = \u001b[43meval_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(vL)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 318\u001b[39m, in \u001b[36meval_metrics\u001b[39m\u001b[34m(model, loader, task, out_dim)\u001b[39m\n\u001b[32m    316\u001b[39m ys, ps, loss_sum, n = [], [], \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    317\u001b[39m crit = make_criterion(task, out_dim)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 232\u001b[39m, in \u001b[36mNPDataset.__getitem__\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (torch.from_numpy(\u001b[38;5;28mself\u001b[39m.X[i]),\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m             \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Adding the code for OL enhancements (FAST MODE, with timing + memory profiling)\n",
    "# ---------- (0) One‐time setup & parity guarantees ----------\n",
    "import os, time, random, warnings, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use a non-interactive backend for script runs (saves plots to files)\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use(\"Agg\")\n",
    "except Exception:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======= SPEED KNOBS (dev mode) =======\n",
    "FAST_MODE = False  # flip False for full report-quality runs\n",
    "\n",
    "# Prefer MPS on Macs, then CUDA, else CPU\n",
    "try:\n",
    "    import torch\n",
    "    _HAS_TORCH = True\n",
    "    DEVICE = (torch.device(\"mps\") if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "              else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "              else torch.device(\"cpu\"))\n",
    "except Exception:\n",
    "    _HAS_TORCH = False\n",
    "    class _CPU:\n",
    "        def __str__(self): return \"cpu\"\n",
    "    DEVICE = _CPU()\n",
    "\n",
    "# After DEVICE is computed\n",
    "IS_CUDA = (hasattr(torch, \"cuda\") and torch.cuda.is_available())\n",
    "IS_CPU  = (str(DEVICE) == \"cpu\")\n",
    "# Use 0 workers for MPS/CPU to avoid spawn/pickle issues and overhead\n",
    "NUM_WORKERS = 0 if (not IS_CUDA) else 2\n",
    "\n",
    "SEED = 4242\n",
    "def set_seed(s=SEED):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    if _HAS_TORCH:\n",
    "        torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "        torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "print(f\"[OL] Torch device: {DEVICE} (cuda? {torch.cuda.is_available() if _HAS_TORCH else False})\")\n",
    "\n",
    "# ---- profiling helpers (OL) ----\n",
    "_MEMPROF_ENABLED = os.getenv(\"ENABLE_MEMPROF\", \"0\") == \"1\"\n",
    "try:\n",
    "    from memory_profiler import memory_usage as _mem_usage\n",
    "    _HAVE_MEMPROF_OL = True\n",
    "except Exception:\n",
    "    _HAVE_MEMPROF_OL = False\n",
    "\n",
    "def profile_call(label, func, *args, enable=None, **kwargs):\n",
    "    \"\"\"Run func(*args, **kwargs), record wall time and (optionally) peak MB.\"\"\"\n",
    "    enable = _MEMPROF_ENABLED if enable is None else bool(enable)\n",
    "    t0 = time.perf_counter()\n",
    "    peak = None\n",
    "    if enable and _HAVE_MEMPROF_OL:\n",
    "        mem_list, retval = _mem_usage((func, args, kwargs), retval=True, interval=0.1)\n",
    "        peak = float(max(mem_list) if mem_list else -1.0)\n",
    "    else:\n",
    "        retval = func(*args, **kwargs)\n",
    "    dt = time.perf_counter() - t0\n",
    "    try:\n",
    "        os.makedirs(\"./ol_outputs/logs\", exist_ok=True)\n",
    "        with open(\"./ol_outputs/logs/ol_profile.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps({\"ts\": time.time(), \"label\": label, \"seconds\": dt, \"peak_mb\": peak}) + \"\\n\")\n",
    "    except Exception as _e:\n",
    "        print(\"[OL] profile log write fail:\", _e)\n",
    "    return retval, dt, peak\n",
    "\n",
    "# Global budgets + toggles (equal budgets per study = OL requirement)\n",
    "if FAST_MODE:\n",
    "    BUDGET_UPDATES = 60\n",
    "    RO_CFG = {\"RHC\": 300, \"SA\": 600, \"GA\": 1200, \"GA_pop\": 20}\n",
    "    OPT_VARIANTS = [(\"adam\", {}), (\"sgd\", {})]\n",
    "    DO_HEATMAPS = False\n",
    "    DO_STABILITY = False\n",
    "    RUN_ACCIDENTS = False\n",
    "    TRAIN_BS = 256\n",
    "    EVAL_BS = 1024\n",
    "    NUM_WORKERS = 0\n",
    "else:\n",
    "    BUDGET_UPDATES = 200\n",
    "    RO_CFG = {\"RHC\": 2000, \"SA\": 4000, \"GA\": 8000, \"GA_pop\": 40}\n",
    "    OPT_VARIANTS = [\n",
    "        (\"sgd\", {}), (\"momentum\", {}), (\"nesterov\", {}),\n",
    "        (\"adam\", {}), (\"adam_nobias\", {}), (\"adam_beta1zero\", {}),\n",
    "        (\"adamw\", {\"wd\": 1e-4}),\n",
    "    ]\n",
    "    DO_HEATMAPS = True\n",
    "    DO_STABILITY = True\n",
    "    RUN_ACCIDENTS = True\n",
    "    TRAIN_BS = 256\n",
    "    EVAL_BS = 1024\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "print(f\"[FAST] mode={FAST_MODE} device={DEVICE} budgets(BUDGET_UPDATES={BUDGET_UPDATES}, RO={RO_CFG})\")\n",
    "\n",
    "OUT_DIR = \"./ol_outputs\"; FIG_DIR = os.path.join(OUT_DIR, \"figs\"); LOG_DIR = os.path.join(OUT_DIR, \"logs\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "def log_parity(info: dict, fname=\"parity_meta.json\"):\n",
    "    path = os.path.join(LOG_DIR, fname)\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(info, f, indent=2, default=str)\n",
    "        print(f\"[OL] Parity meta logged -> {path}\")\n",
    "    except Exception as e:\n",
    "        print(\"[OL] Could not write parity meta:\", e)\n",
    "\n",
    "# ---------- Optional raw→preprocess (only if split arrays aren't exposed) ----------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def _make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def _make_safe_target_encoder(cols):\n",
    "    try:\n",
    "        from category_encoders import TargetEncoder as _CE_TargetEncoder\n",
    "    except Exception as e:\n",
    "        print(\"[ENC] category_encoders not installed; using OneHotEncoder. Reason:\", e)\n",
    "        return _make_ohe()\n",
    "    try:\n",
    "        import pandas as _pd\n",
    "        te = _CE_TargetEncoder(cols=cols, smoothing=1.0)\n",
    "        Xs = _pd.DataFrame({c: [\"a\", \"b\", \"a\", \"c\"] for c in cols})\n",
    "        ys = _pd.Series([0, 1, 1, 0])\n",
    "        te.fit(Xs, ys); _ = te.transform(Xs)\n",
    "        print(\"[ENC] Using TargetEncoder for:\", cols)\n",
    "        return te\n",
    "    except Exception as e:\n",
    "        print(\"[ENC] TargetEncoder not usable; falling back to OneHotEncoder. Reason:\", e)\n",
    "        return _make_ohe()\n",
    "\n",
    "def _split_cols_by_type(df): \n",
    "    return (df.select_dtypes(include=np.number).columns.tolist(),\n",
    "            df.select_dtypes(exclude=np.number).columns.tolist())\n",
    "\n",
    "def _simple_na_fill(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype.kind in \"biufc\":\n",
    "            df.loc[:, c] = df[c].fillna(df[c].median())\n",
    "        else:\n",
    "            df.loc[:, c] = df[c].fillna(\"Unknown\")\n",
    "    return df\n",
    "\n",
    "def build_preproc(X, high_card_cutoff=25):\n",
    "    nums, cats = _split_cols_by_type(X)\n",
    "    hi = [c for c in cats if X[c].nunique(dropna=False) > high_card_cutoff]\n",
    "    lo = [c for c in cats if c not in hi]\n",
    "    steps=[]\n",
    "    if nums: steps.append((\"num\", StandardScaler(), nums))\n",
    "    if lo:   steps.append((\"ohe\", _make_ohe(), lo))\n",
    "    if hi:   steps.append((\"hi_cat\", _make_safe_target_encoder(hi), hi))\n",
    "    return ColumnTransformer(steps, remainder=\"drop\", verbose_feature_names_out=False)\n",
    "\n",
    "HOTEL_CSV = os.environ.get(\"HOTEL_CSV\", \"hotel_bookings.csv\")\n",
    "ACCIDENTS_CSV = os.environ.get(\"ACCIDENTS_CSV\", \"US_Accidents_March23.csv\")\n",
    "\n",
    "def load_hotel_raw():\n",
    "    df = pd.read_csv(HOTEL_CSV, low_memory=False)\n",
    "    for c in [\"reservation_status\", \"reservation_status_date\"]:\n",
    "        if c in df: df = df.drop(columns=[c])\n",
    "    df.loc[:, \"agent\"] = df.get(\"agent\", pd.Series(index=df.index)).fillna(0)\n",
    "    df.loc[:, \"company\"] = df.get(\"company\", pd.Series(index=df.index)).fillna(0)\n",
    "    if \"country\" in df:\n",
    "        df.loc[:, \"country\"] = df[\"country\"].fillna(df[\"country\"].mode(dropna=True).iloc[0])\n",
    "    if \"children\" in df:\n",
    "        df.loc[:, \"children\"] = df[\"children\"].fillna(0)\n",
    "    y = df[\"is_canceled\"].astype(int)\n",
    "    X = _simple_na_fill(df.drop(columns=[\"is_canceled\"]))\n",
    "    return X, y\n",
    "\n",
    "def load_accidents_raw():\n",
    "    df = pd.read_csv(ACCIDENTS_CSV, low_memory=True)\n",
    "    df[\"Start_Time\"] = pd.to_datetime(df[\"Start_Time\"], errors=\"coerce\")\n",
    "    df[\"End_Time\"]   = pd.to_datetime(df[\"End_Time\"], errors=\"coerce\")\n",
    "    df[\"Duration\"]   = (df[\"End_Time\"] - df[\"Start_Time\"]).dt.total_seconds()/60.0\n",
    "    df = df.dropna(subset=[\"Duration\"]).loc[lambda d: d[\"Duration\"]>0]\n",
    "    if \"Description\" in df: df = df.drop(columns=[\"Description\"])\n",
    "    y = df[\"Duration\"].astype(np.float32)\n",
    "    X = _simple_na_fill(df.drop(columns=[\"Duration\",\"Start_Time\",\"End_Time\"]))\n",
    "    return X, y\n",
    "\n",
    "def _np1d(v):\n",
    "    import pandas as _pd\n",
    "    return v.values if isinstance(v, (_pd.Series, _pd.DataFrame)) else v\n",
    "\n",
    "# Build arrays if SL didn’t export them\n",
    "if not all(n in globals() for n in [\"Xh_tr_f\",\"Xh_va_f\",\"Xh_te_f\",\"yh_tr\",\"yh_va\",\"yh_te\"]):\n",
    "    Xh, yh = load_hotel_raw()\n",
    "    pre_h = build_preproc(Xh)\n",
    "    Xh_tr, Xh_tmp, yh_tr, yh_tmp = train_test_split(Xh, yh, test_size=0.3, random_state=SEED, stratify=yh)\n",
    "    Xh_va, Xh_te, yh_va, yh_te   = train_test_split(Xh_tmp, yh_tmp, test_size=0.5, random_state=SEED, stratify=yh_tmp)\n",
    "    Xh_tr_f = pre_h.fit_transform(Xh_tr, yh_tr); Xh_va_f = pre_h.transform(Xh_va); Xh_te_f = pre_h.transform(Xh_te)\n",
    "\n",
    "if FAST_MODE:\n",
    "    RUN_ACCIDENTS = False  # ensure\n",
    "if RUN_ACCIDENTS and not all(n in globals() for n in [\"Xa_tr_f\",\"Xa_va_f\",\"Xa_te_f\",\"ya_tr\",\"ya_va\",\"ya_te\"]):\n",
    "    Xa, ya = load_accidents_raw()\n",
    "    pre_a = build_preproc(Xa)\n",
    "    Xa_tr, Xa_tmp, ya_tr, ya_tmp = train_test_split(Xa, ya, test_size=0.3, random_state=SEED)\n",
    "    Xa_va, Xa_te, ya_va, ya_te   = train_test_split(Xa_tmp, ya_tmp, test_size=0.5, random_state=SEED)\n",
    "    Xa_tr_f = pre_a.fit_transform(Xa_tr, ya_tr); Xa_va_f = pre_a.transform(Xa_va); Xa_te_f = pre_a.transform(Xa_te)\n",
    "\n",
    "ADAPTER_OK = all(n in globals() for n in [\n",
    "    \"Xh_tr_f\",\"Xh_va_f\",\"Xh_te_f\",\"yh_tr\",\"yh_va\",\"yh_te\"\n",
    "]) and (True if not RUN_ACCIDENTS else all(n in globals() for n in [\n",
    "    \"Xa_tr_f\",\"Xa_va_f\",\"Xa_te_f\",\"ya_tr\",\"ya_va\",\"ya_te\"\n",
    "]))\n",
    "print(\"[OL] Adapter bound? ->\", ADAPTER_OK)\n",
    "\n",
    "# ---------- (1) Data: arrays → DataLoaders ----------\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NPDataset(Dataset):\n",
    "    def __init__(self, X, y, task):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = _np1d(y).astype(np.int64 if task==\"cls\" else np.float32)\n",
    "        self.task = task\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return (torch.from_numpy(self.X[i]),\n",
    "                torch.tensor(self.y[i]))\n",
    "\n",
    "def make_loaders(Xtr, ytr, Xva, yva, Xte, yte, task, bs_train=None, bs_eval=None, num_workers=None):\n",
    "    bs_train = (bs_train or 256)\n",
    "    bs_eval  = (bs_eval  or 1024)\n",
    "    if num_workers is None:\n",
    "        num_workers = NUM_WORKERS\n",
    "    pin = True if IS_CUDA else False\n",
    "\n",
    "    dtr = NPDataset(Xtr, ytr, task)\n",
    "    dva = NPDataset(Xva, yva, task)\n",
    "    dte = NPDataset(Xte, yte, task)\n",
    "\n",
    "    tr = DataLoader(dtr, batch_size=bs_train, shuffle=True,  drop_last=False,\n",
    "                    num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
    "    va = DataLoader(dva, batch_size=bs_eval,  shuffle=False, drop_last=False,\n",
    "                    num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
    "    te = DataLoader(dte, batch_size=bs_eval,  shuffle=False, drop_last=False,\n",
    "                    num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
    "    return tr, va, te\n",
    "\n",
    "\n",
    "if all(n in globals() for n in [\"Xh_tr_f\",\"Xh_va_f\",\"Xh_te_f\",\"yh_tr\",\"yh_va\",\"yh_te\"]):\n",
    "    H_in = np.asarray(Xh_tr_f).shape[1]\n",
    "    H_loaders = make_loaders(Xh_tr_f, yh_tr, Xh_va_f, yh_va, Xh_te_f, yh_te, task=\"cls\",\n",
    "                             bs_train=TRAIN_BS, bs_eval=EVAL_BS, num_workers=NUM_WORKERS)\n",
    "    log_parity({\"dataset\":\"Hotel\",\"Xtr\":list(np.asarray(Xh_tr_f).shape),\"Xva\":list(np.asarray(Xh_va_f).shape),\n",
    "                \"Xte\":list(np.asarray(Xh_te_f).shape),\"bs_train\":TRAIN_BS,\"bs_eval\":EVAL_BS})\n",
    "\n",
    "if RUN_ACCIDENTS and all(n in globals() for n in [\"Xa_tr_f\",\"Xa_va_f\",\"Xa_te_f\",\"ya_tr\",\"ya_va\",\"ya_te\"]):\n",
    "    A_in = np.asarray(Xa_tr_f).shape[1]\n",
    "    A_loaders = make_loaders(Xa_tr_f, ya_tr, Xa_va_f, ya_va, Xa_te_f, ya_te, task=\"reg\",\n",
    "                             bs_train=TRAIN_BS, bs_eval=EVAL_BS, num_workers=NUM_WORKERS)\n",
    "    log_parity({\"dataset\":\"Accidents\",\"Xtr\":list(np.asarray(Xa_tr_f).shape),\"Xva\":list(np.asarray(Xa_va_f).shape),\n",
    "                \"Xte\":list(np.asarray(Xa_te_f).shape),\"bs_train\":TRAIN_BS,\"bs_eval\":EVAL_BS})\n",
    "\n",
    "# ---------- (2) Model: mirror SL MLP with nn.Module ----------\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=(128,64), out_dim=2, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        layers=[]; dims=[in_dim]+list(hidden)\n",
    "        for a,b in zip(dims[:-1], dims[1:]):\n",
    "            layers += [nn.Linear(a,b), nn.ReLU()]\n",
    "            if dropout_p>0: layers += [nn.Dropout(dropout_p)]\n",
    "        layers += [nn.Linear(dims[-1] if hidden else in_dim, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "if 'H_in' in globals():\n",
    "    m_cls = MLP(H_in, hidden=(128,64), out_dim=2, dropout_p=0.0).to(DEVICE)\n",
    "    print(\"[OL] Hotel MLP:\", m_cls)\n",
    "if RUN_ACCIDENTS and 'A_in' in globals():\n",
    "    m_reg = MLP(A_in, hidden=(512,256), out_dim=1, dropout_p=0.0).to(DEVICE)\n",
    "    print(\"[OL] Accidents MLP:\", m_reg)\n",
    "\n",
    "# ---------- (3) Freezing all but last k layers (RO) ----------\n",
    "def linear_layers(model): return [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "def freeze_all_but_last_k(model, k=1):\n",
    "    for p in model.parameters(): p.requires_grad = False\n",
    "    Ls = linear_layers(model)\n",
    "    for m in Ls[-k:]:\n",
    "        for p in m.parameters(): p.requires_grad = True\n",
    "    tot = sum(p.numel() for p in model.parameters())\n",
    "    trn = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"[OL] Params total={tot:,} | trainable(last {k})={trn:,}\")\n",
    "    return trn\n",
    "\n",
    "# ---------- (4) Losses & metrics ----------\n",
    "USE_BCE_FOR_BINARY = False\n",
    "\n",
    "def make_criterion(task=\"cls\", out_dim=2):\n",
    "    if task==\"cls\":\n",
    "        if USE_BCE_FOR_BINARY and out_dim==1: return nn.BCEWithLogitsLoss()\n",
    "        return nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        return nn.MSELoss()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score, mean_absolute_error\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_metrics(model, loader, task, out_dim):\n",
    "    model.eval()\n",
    "    ys, ps, loss_sum, n = [], [], 0.0, 0\n",
    "    crit = make_criterion(task, out_dim)\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        out = model(xb)\n",
    "        if task==\"cls\":\n",
    "            if USE_BCE_FOR_BINARY and out_dim==1:\n",
    "                loss = crit(out.squeeze(1), yb.float())\n",
    "                prob = torch.sigmoid(out.squeeze(1)).cpu().numpy(); y = yb.cpu().numpy()\n",
    "            else:\n",
    "                loss = crit(out, yb)\n",
    "                prob = torch.softmax(out, dim=1)[:,1].cpu().numpy(); y = yb.cpu().numpy()\n",
    "            ys.append(y); ps.append(prob)\n",
    "        else:\n",
    "            loss = crit(out.squeeze(-1), yb)\n",
    "            ys.append(yb.cpu().numpy()); ps.append(out.squeeze(-1).cpu().numpy())\n",
    "        loss_sum += float(loss.item()) * xb.size(0); n += xb.size(0)\n",
    "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
    "    avg_loss = loss_sum / max(n,1)\n",
    "    if task==\"cls\":\n",
    "        auroc = roc_auc_score(y, p) if (len(np.unique(y))>1) else np.nan\n",
    "        ap    = average_precision_score(y, p) if (len(np.unique(y))>1) else np.nan\n",
    "        pred  = (p>=0.5).astype(int)\n",
    "        acc   = accuracy_score(y, pred); f1 = f1_score(y, pred)\n",
    "        return avg_loss, {\"auroc\":auroc, \"prauc\":ap, \"acc\":acc, \"f1\":f1}\n",
    "    else:\n",
    "        mae = mean_absolute_error(y, p)\n",
    "        return avg_loss, {\"mae\":mae}\n",
    "\n",
    "# ---------- (5) Training loop (counts grad evals) ----------\n",
    "def run_epoch(model, loader, optimizer=None, task=\"cls\", out_dim=2, l2_lambda=0.0, label_smooth=0.0, input_noise_std=0.0):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    crit = make_criterion(task, out_dim)\n",
    "    total_loss, n, grad_evals = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        if input_noise_std>0 and task==\"reg\" and is_train:\n",
    "            xb = xb + input_noise_std*torch.randn_like(xb)\n",
    "        if is_train: optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(xb)\n",
    "        if task==\"cls\" and (label_smooth>0) and (not USE_BCE_FOR_BINARY or out_dim!=1):\n",
    "            ncls = out.size(1)\n",
    "            y_one = torch.zeros((yb.size(0), ncls), device=out.device).scatter_(1, yb.view(-1,1), 1.0)\n",
    "            y_s = (1.0 - label_smooth)*y_one + label_smooth/(ncls-1)*(1.0 - y_one)\n",
    "            loss = -(y_s * torch.log_softmax(out, dim=1)).sum(dim=1).mean()\n",
    "        else:\n",
    "            loss = (nn.BCEWithLogitsLoss()(out.squeeze(1), yb.float()) if (task==\"cls\" and USE_BCE_FOR_BINARY and out_dim==1)\n",
    "                    else crit(out if task==\"cls\" else out.squeeze(-1), yb))\n",
    "        if l2_lambda>0:\n",
    "            l2 = sum((p**2).sum() for p in model.parameters() if p.requires_grad)\n",
    "            loss = loss + l2_lambda*l2\n",
    "        if is_train:\n",
    "            loss.backward(); optimizer.step(); grad_evals += 1\n",
    "        total_loss += float(loss.item())*xb.size(0); n += xb.size(0)\n",
    "    return total_loss/max(n,1), grad_evals\n",
    "\n",
    "# ---------- (6) Optimizer ablations (identical budgets & threshold ℓ) ----------\n",
    "class AdamNoBias(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9,0.999), eps=1e-8, weight_decay=0.0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None: loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]; beta1, beta2 = group[\"betas\"]; eps = group[\"eps\"]; wd = group[\"weight_decay\"]\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                g = p.grad\n",
    "                st = self.state[p]\n",
    "                if len(st)==0:\n",
    "                    st[\"m\"] = torch.zeros_like(p); st[\"v\"]=torch.zeros_like(p)\n",
    "                m, v = st[\"m\"], st[\"v\"]\n",
    "                m.mul_(beta1).add_(g, alpha=1.0-beta1)\n",
    "                v.mul_(beta2).addcmul_(g, g, value=1.0-beta2)\n",
    "                step = m/(v.sqrt()+eps)  # no bias correction\n",
    "                if wd!=0.0: step = step + wd*p\n",
    "                p.add_(step, alpha=-lr)\n",
    "        return loss\n",
    "\n",
    "def make_optimizer(params, kind, lr, beta1=0.9, beta2=0.999, wd=0.0):\n",
    "    filt = [p for p in params if p.requires_grad]\n",
    "    if kind==\"sgd\":         return torch.optim.SGD(filt, lr=lr, momentum=0.0, nesterov=False)\n",
    "    if kind==\"momentum\":    return torch.optim.SGD(filt, lr=lr, momentum=0.9, nesterov=False)\n",
    "    if kind==\"nesterov\":    return torch.optim.SGD(filt, lr=lr, momentum=0.9, nesterov=True)\n",
    "    if kind==\"adam\":        return torch.optim.Adam(filt, lr=lr, betas=(beta1,beta2))\n",
    "    if kind==\"adam_nobias\": return AdamNoBias(filt, lr=lr, betas=(beta1,beta2))\n",
    "    if kind==\"adam_beta1zero\": return torch.optim.Adam(filt, lr=lr, betas=(0.0,beta2))\n",
    "    if kind==\"adamw\":       return torch.optim.AdamW(filt, lr=lr, betas=(beta1,beta2), weight_decay=wd)\n",
    "    raise ValueError(kind)\n",
    "\n",
    "def train_to_budget(model, train_loader, val_loader, task, out_dim,\n",
    "                    max_updates=200, L_threshold=None, tag=\"run\", opt_kind=\"adam\",\n",
    "                    lr=1e-3, beta1=0.9, beta2=0.999, wd=0.0, label_smooth=0.0, input_noise_std=0.0):\n",
    "    import copy, time as _time\n",
    "    m = copy.deepcopy(model).to(DEVICE)\n",
    "    for p in m.parameters(): p.requires_grad = True  # Part 2 trains full net\n",
    "    opt = make_optimizer(m.parameters(), opt_kind, lr, beta1, beta2, wd)\n",
    "    grad_total, best_val, t0, reached = 0, float(\"inf\"), _time.time(), None\n",
    "    hist=[]\n",
    "    while grad_total < max_updates:\n",
    "        trL, ge = run_epoch(m, train_loader, opt, task, out_dim, l2_lambda=0.0,\n",
    "                            label_smooth=label_smooth, input_noise_std=input_noise_std)\n",
    "        grad_total += ge\n",
    "        vaL, extra = eval_metrics(m, val_loader, task, out_dim)\n",
    "        best_val = min(best_val, vaL)\n",
    "        if (L_threshold is not None) and (reached is None) and (vaL <= L_threshold):\n",
    "            reached = {\"grad_evals\":grad_total, \"time_sec\": _time.time()-t0}\n",
    "        hist.append({\"grad_evals\":grad_total,\"val_loss\":vaL, **extra})\n",
    "    df = pd.DataFrame(hist); logf = os.path.join(LOG_DIR, f\"{tag}.csv\"); df.to_csv(logf, index=False)\n",
    "    return {\"best_val\":best_val, \"reached_L\":reached, \"log\":logf}\n",
    "\n",
    "L_HOTEL = 0.20\n",
    "L_ACC   = 500.0\n",
    "\n",
    "def part2_ablation(model, loaders, task, out_dim, base_lr, base_beta1=0.9, base_beta2=0.999, variants=None):\n",
    "    tr, va, _ = loaders\n",
    "    variants = variants or OPT_VARIANTS\n",
    "    results=[]\n",
    "    for name, kw in variants:\n",
    "        tag = f\"opt_{name}_{task}\"\n",
    "        Lth = L_HOTEL if task==\"cls\" else L_ACC\n",
    "        # ---- profile this optimizer run ----\n",
    "        (r, dt, peak) = profile_call(f\"Part2::{task}::{name}\",\n",
    "                                     train_to_budget, model, tr, va, task, out_dim,\n",
    "                                     BUDGET_UPDATES, L_threshold=Lth, tag=tag,\n",
    "                                     opt_kind=name, lr=base_lr, beta1=base_beta1, beta2=base_beta2, **kw)\n",
    "        r[\"variant\"]=name; r[\"time_sec\"]=dt; r[\"peak_mb\"]=peak\n",
    "        results.append(r)\n",
    "        print(f\"[OL/Part2] {name}: best_val={r['best_val']:.4f} reached_L={r['reached_L']} time={dt:.2f}s peakMB={peak}\")\n",
    "    # save summary with time & memory\n",
    "    pd.DataFrame([{k:v for k,v in r.items() if k!='log'} for r in results]).to_csv(\n",
    "        os.path.join(LOG_DIR, f\"ablation_summary_{task}.csv\"), index=False)\n",
    "    plot_loss_vs_compute(os.path.join(LOG_DIR, \"opt_adam_cls.csv\"),\n",
    "                     \"Hotel: Adam loss vs compute\",\n",
    "                     \"loss_vs_compute_hotel_adam.png\")\n",
    "    return results\n",
    "\n",
    "def sensitivity_heatmap(model, loaders, task, out_dim, alphas=(1e-4,3e-4,1e-3), betas=(0.5,0.9,0.99), which=\"b1\"):\n",
    "    if FAST_MODE:\n",
    "        print(\"[OL/Part2] Heatmaps disabled in FAST_MODE\"); \n",
    "        return pd.DataFrame()\n",
    "    tr, va, _ = loaders\n",
    "    rec=[]\n",
    "    for a in alphas:\n",
    "        for b in betas:\n",
    "            b1, b2 = (b, 0.999) if which==\"b1\" else (0.9, b)\n",
    "            (r, _, _) = profile_call(f\"Part2::heatmap::{task}::{which}::a{a}_b{b}\",\n",
    "                                     train_to_budget, model, tr, va, task, out_dim,\n",
    "                                     max_updates=BUDGET_UPDATES//2, L_threshold=None,\n",
    "                                     tag=f\"heat_{task}_{which}_a{a}_b{b}\",\n",
    "                                     opt_kind=\"adam\", lr=a, beta1=b1, beta2=b2)\n",
    "            rec.append({\"alpha\":a,\"beta\":b,\"best\":r[\"best_val\"]})\n",
    "    df = pd.DataFrame(rec); piv = df.pivot(index=\"beta\", columns=\"alpha\", values=\"best\")\n",
    "    plt.figure(); plt.imshow(piv.values, aspect=\"auto\")\n",
    "    plt.xticks(range(len(piv.columns)), piv.columns); plt.yticks(range(len(piv.index)), piv.index)\n",
    "    plt.xlabel(\"alpha (lr)\"); plt.ylabel(\"beta\" + (\"1\" if which==\"b1\" else \"2\"))\n",
    "    plt.colorbar(label=\"validation loss\"); plt.title(f\"{task.upper()} Adam sensitivity ({which})\")\n",
    "    fn = os.path.join(FIG_DIR, f\"heatmap_{task}_{which}.png\"); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
    "    print(\"[OL/Part2] Saved heatmap ->\", fn)\n",
    "    return df\n",
    "\n",
    "def stability_band(model, loaders, task, out_dim, seeds=(0,1,2), opt_kind=\"adam\", lr=1e-3):\n",
    "    if FAST_MODE:\n",
    "        print(\"[OL/Part2] Stability bands disabled in FAST_MODE\"); \n",
    "        return pd.DataFrame()\n",
    "    import copy\n",
    "    tr, va, _ = loaders\n",
    "    traj=[]\n",
    "    for sd in seeds:\n",
    "        set_seed(sd)\n",
    "        m = copy.deepcopy(model).to(DEVICE)\n",
    "        (r, dt, peak) = profile_call(f\"Part2::stability::{task}::{opt_kind}::seed{sd}\",\n",
    "                                     train_to_budget, m, tr, va, task, out_dim,\n",
    "                                     BUDGET_UPDATES, L_threshold=None,\n",
    "                                     tag=f\"stab_{task}_{opt_kind}_seed{sd}\", opt_kind=opt_kind, lr=lr)\n",
    "        df = pd.read_csv(r[\"log\"]); df[\"seed\"]=sd; traj.append(df)\n",
    "        print(f\"[OL/Part2] seed {sd} time={dt:.2f}s peakMB={peak}\")\n",
    "    D = pd.concat(traj, axis=0)\n",
    "    g = D.groupby(\"grad_evals\")[\"val_loss\"]\n",
    "    xs = g.median().index.values; med=g.median().values; q25=g.quantile(0.25).values; q75=g.quantile(0.75).values\n",
    "    plt.figure(); plt.plot(xs, med, label=\"median\"); plt.fill_between(xs, q25, q75, alpha=0.3, label=\"IQR\")\n",
    "    plt.xlabel(\"# grad evals\"); plt.ylabel(\"validation loss\"); plt.title(f\"{task.upper()} {opt_kind} stability\")\n",
    "    plt.legend(); fn = os.path.join(FIG_DIR, f\"stability_{task}_{opt_kind}.png\"); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
    "    print(\"[OL/Part2] Saved stability band ->\", fn)\n",
    "    return D\n",
    "\n",
    "# ---------- (7) RO hygiene (Part 1) ----------\n",
    "def last_linear(model):\n",
    "    Ls = linear_layers(model)\n",
    "    if not Ls: raise RuntimeError(\"No nn.Linear layers found.\")\n",
    "    return Ls[-1]\n",
    "\n",
    "def pack_last(last_layer):\n",
    "    W = last_layer.weight.detach().cpu().numpy().copy()\n",
    "    b = last_layer.bias.detach().cpu().numpy().copy()\n",
    "    return np.concatenate([W.ravel(), b.ravel()]), W.shape\n",
    "\n",
    "def unpack_to_last(vec, last_layer, Wshape):\n",
    "    out, in_ = Wshape\n",
    "    W = vec[:out*in_].reshape(out, in_)\n",
    "    b = vec[out*in_:]\n",
    "    last_layer.weight.data = torch.from_numpy(W).to(last_layer.weight.device).type_as(last_layer.weight)\n",
    "    last_layer.bias.data   = torch.from_numpy(b).to(last_layer.bias.device).type_as(last_layer.bias)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_objective(model, val_loader, task, out_dim):\n",
    "    model.eval()\n",
    "    vL, _ = eval_metrics(model, val_loader, task, out_dim)\n",
    "    return float(vL)  # one full val pass = one function evaluation\n",
    "\n",
    "def RHC(f, x0, step=0.05, restarts=5, budget=2000):\n",
    "    best_x, best_f = x0.copy(), f(x0); evals=1; hist=[best_f]\n",
    "    for r in range(restarts+1):\n",
    "        x = x0.copy() if r==0 else x0 + 0.1*np.random.randn(*x0.shape)\n",
    "        fx = f(x); evals += 1\n",
    "        improved=True\n",
    "        while evals<budget and improved:\n",
    "            improved=False\n",
    "            for _ in range(20):\n",
    "                c = x + step*np.random.randn(*x.shape); fc = f(c); evals += 1\n",
    "                if fc < fx: x, fx = c, fc; improved=True; hist.append(fx)\n",
    "                if evals>=budget: break\n",
    "        if fx < best_f: best_x, best_f = x, fx\n",
    "    return best_x, best_f, np.array(hist)\n",
    "\n",
    "def SA(f, x0, T0=1.0, cooling=0.997, step=0.05, budget=4000):\n",
    "    x, fx = x0.copy(), f(x0); evals=1; hist=[fx]; T=T0\n",
    "    while evals<budget:\n",
    "        c = x + step*np.random.randn(*x.shape); fc = f(c); evals += 1\n",
    "        if fc < fx or np.random.rand() < np.exp((fx-fc)/max(T,1e-8)): x, fx = c, fc\n",
    "        hist.append(fx); T *= cooling\n",
    "    return x, fx, np.array(hist)\n",
    "\n",
    "def GA(f, x0, pop=40, elites=2, cx=0.7, mut=0.2, step=0.05, budget=8000):\n",
    "    dim = x0.size; evals=0\n",
    "    P = x0 + 0.1*np.random.randn(pop, dim)\n",
    "    fit = np.array([f(ind) for ind in P]); evals += pop; best_hist=[fit.min()]\n",
    "    while evals<budget:\n",
    "        idx = np.argsort(fit); P = P[idx]; fit = fit[idx]\n",
    "        new = [P[i] for i in range(elites)]\n",
    "        while len(new) < pop:\n",
    "            if np.random.rand()<cx:\n",
    "                a,b = P[np.random.randint(0,pop//2,2)]; cut = np.random.randint(0, dim)\n",
    "                child = np.concatenate([a[:cut], b[cut:]])\n",
    "            else: child = P[np.random.randint(0,pop)]\n",
    "            if np.random.rand()<mut: child = child + step*np.random.randn(dim)\n",
    "            new.append(child)\n",
    "        P = np.array(new)\n",
    "        fit = np.array([f(ind) for ind in P]); evals += pop; best_hist.append(fit.min())\n",
    "    best = P[fit.argmin()]\n",
    "    return best, fit.min(), np.array(best_hist)\n",
    "\n",
    "def run_part1_RO(model, loaders, task, out_dim, base_tag=\"dataset\", k_unfrozen=1, ro_cfg=None):\n",
    "    import copy\n",
    "    ro_cfg = ro_cfg or RO_CFG\n",
    "    m = copy.deepcopy(model).to(DEVICE)\n",
    "    tparams = freeze_all_but_last_k(m, k=k_unfrozen)\n",
    "    assert tparams <= 50_000, f\"RO trainable params {tparams} exceed ~50k cap\"\n",
    "    m.eval()\n",
    "\n",
    "    last = last_linear(m)\n",
    "    x0, Wshape = pack_last(last)\n",
    "\n",
    "    def f(vec):\n",
    "        with torch.no_grad():\n",
    "            unpack_to_last(vec.astype(np.float32), last, Wshape)\n",
    "            return validation_objective(m, loaders[1], task, out_dim)\n",
    "\n",
    "    # Profile each RO algo\n",
    "    (ret_rhc, t_rhc, mb_rhc) = profile_call(f\"Part1::{base_tag}::RHC\",\n",
    "                                            RHC, f, x0, 0.05, (3 if FAST_MODE else 5), ro_cfg[\"RHC\"])\n",
    "    xr, fr, hr = ret_rhc\n",
    "    (ret_sa,  t_sa,  mb_sa ) = profile_call(f\"Part1::{base_tag}::SA\",\n",
    "                                            SA, f, x0, 1.0, 0.997, 0.05, ro_cfg[\"SA\"])\n",
    "    xs, fs, hs = ret_sa\n",
    "    (ret_ga,  t_ga,  mb_ga ) = profile_call(f\"Part1::{base_tag}::GA\",\n",
    "                                            GA, f, x0, ro_cfg[\"GA_pop\"], 2, 0.7, 0.2, 0.05, ro_cfg[\"GA\"])\n",
    "    xg, fg, hg = ret_ga\n",
    "\n",
    "    def summarize(hist, algo, Lth):\n",
    "        best = float(np.min(hist))\n",
    "        # first eval reaching threshold (if any)\n",
    "        cummin = np.minimum.accumulate(hist)\n",
    "        idx = int(np.where(cummin <= Lth)[0][0]) if np.any(cummin <= Lth) else None\n",
    "        return {\"algo\": algo, \"best_val\": best, \"reached_L\": idx is not None, \"evals_to_L\": (idx if idx is not None else None)}\n",
    "\n",
    "    Lth = L_HOTEL if task==\"cls\" else L_ACC\n",
    "    summary = pd.DataFrame([\n",
    "        summarize(hr, \"RHC\", Lth),\n",
    "        summarize(hs, \"SA\",  Lth),\n",
    "        summarize(hg, \"GA\",  Lth),\n",
    "    ])\n",
    "    summary_path = os.path.join(LOG_DIR, f\"ro_summary_{base_tag}.csv\")\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    print(\"[OL/Part1] Saved RO summary ->\", summary_path)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(hr)), np.minimum.accumulate(hr), label=f\"RHC ({t_rhc:.1f}s)\")\n",
    "    plt.plot(np.arange(len(hs)), np.minimum.accumulate(hs), label=f\"SA ({t_sa:.1f}s)\")\n",
    "    plt.plot(np.arange(len(hg)), np.minimum.accumulate(hg), label=f\"GA ({t_ga:.1f}s)\")\n",
    "    plt.xlabel(\"function evaluations\")\n",
    "    plt.ylabel(\"best-so-far validation loss\")\n",
    "    plt.title(f\"RO progress ({base_tag})\"); plt.legend(); plt.tight_layout()\n",
    "    fn = os.path.join(FIG_DIR, f\"ro_progress_{base_tag}.png\"); plt.savefig(fn, dpi=160); plt.close()\n",
    "    print(\"[OL/Part1] Saved RO curves ->\", fn)\n",
    "\n",
    "# ---------- (8) Regularization study (Adam only) ----------\n",
    "def part3_regularization(model, loaders, task, out_dim, adam_lr, adam_beta1=0.9, adam_beta2=0.999):\n",
    "    import copy\n",
    "    tr, va, _ = loaders\n",
    "    def train_cfg(tag, l2=0.0, es_patience=None, drop_p=None, label_smooth=0.0, input_noise=0.0):\n",
    "        m = copy.deepcopy(model).to(DEVICE)\n",
    "        if drop_p is not None:\n",
    "            Ls = [m for m in m.modules() if isinstance(m, nn.Linear)]\n",
    "            in_dim = Ls[0].in_features; outs=[l.out_features for l in Ls[:-1]]\n",
    "            out_dim_loc = Ls[-1].out_features\n",
    "            new_m = MLP(in_dim, hidden=tuple(outs), out_dim=out_dim_loc, dropout_p=drop_p).to(DEVICE)\n",
    "            new_m.load_state_dict(m.state_dict(), strict=False)\n",
    "            m = new_m\n",
    "        opt = make_optimizer(m.parameters(), \"adam\", adam_lr, adam_beta1, adam_beta2, wd=0.0)\n",
    "\n",
    "        best=float(\"inf\"); ge_total=0; hist=[]; patience_left=es_patience\n",
    "        while ge_total < BUDGET_UPDATES:\n",
    "            trL, ge = run_epoch(m, tr, opt, task, out_dim, l2_lambda=l2, label_smooth=label_smooth, input_noise_std=input_noise)\n",
    "            ge_total += ge\n",
    "            vaL, extra = eval_metrics(m, va, task, out_dim)\n",
    "            best = min(best, vaL); hist.append({\"grad_evals\":ge_total,\"val_loss\":vaL, **extra})\n",
    "            if es_patience is not None:\n",
    "                if len(hist)>1 and hist[-1][\"val_loss\"] > min(h[\"val_loss\"] for h in hist[:-1]):\n",
    "                    patience_left -= 1\n",
    "                    if patience_left <= 0: break\n",
    "        df = pd.DataFrame(hist); logf=os.path.join(LOG_DIR,f\"{tag}.csv\"); df.to_csv(logf, index=False)\n",
    "        return best\n",
    "\n",
    "    # run & profile each config\n",
    "    (baseline, t_base, mb_base) = profile_call(f\"Part3::{task}::baseline\",\n",
    "                                               train_cfg, \"reg_baseline_\"+task)\n",
    "    single = []\n",
    "    for wd in [1e-5, 1e-4, 5e-4]:\n",
    "        (bv, t, mb) = profile_call(f\"Part3::{task}::l2::{wd}\", train_cfg, f\"reg_l2_{wd}_{task}\", l2=wd)\n",
    "        single.append((\"l2\", wd, bv, t, mb))\n",
    "    (bv, t, mb) = profile_call(f\"Part3::{task}::earlystop::3\", train_cfg, f\"reg_es3_{task}\", es_patience=3)\n",
    "    single.append((\"earlystop\", 3, bv, t, mb))\n",
    "    for p in ([0.1, 0.3] if task==\"cls\" else [0.05, 0.2]):\n",
    "        (bv, t, mb) = profile_call(f\"Part3::{task}::dropout::{p}\", train_cfg, f\"reg_dropout_{p}_{task}\", drop_p=p)\n",
    "        single.append((\"dropout\", p, bv, t, mb))\n",
    "    if task==\"cls\":\n",
    "        for eps in [0.05, 0.10]:\n",
    "            (bv, t, mb) = profile_call(f\"Part3::{task}::label_smooth::{eps}\", train_cfg, f\"reg_ls_{eps}_{task}\", label_smooth=eps)\n",
    "            single.append((\"label_smooth\", eps, bv, t, mb))\n",
    "    else:\n",
    "        for s in [0.01, 0.05]:\n",
    "            (bv, t, mb) = profile_call(f\"Part3::{task}::input_noise::{s}\", train_cfg, f\"reg_inoise_{s}_{task}\", input_noise=s)\n",
    "            single.append((\"input_noise\", s, bv, t, mb))\n",
    "\n",
    "    sdf = pd.DataFrame(single, columns=[\"reg\",\"value\",\"best_val\",\"time_sec\",\"peak_mb\"]).sort_values(\"best_val\")\n",
    "    best_reg = sdf.iloc[0].to_dict()\n",
    "    combo_kwargs = {}\n",
    "    if best_reg[\"reg\"]==\"l2\": combo_kwargs[\"l2\"]=float(best_reg[\"value\"])\n",
    "    if best_reg[\"reg\"]==\"earlystop\": combo_kwargs[\"es_patience\"]=int(best_reg[\"value\"])\n",
    "    if best_reg[\"reg\"]==\"dropout\": combo_kwargs[\"drop_p\"]=float(best_reg[\"value\"])\n",
    "    if best_reg[\"reg\"]==\"label_smooth\": combo_kwargs[\"label_smooth\"]=float(best_reg[\"value\"])\n",
    "    if best_reg[\"reg\"]==\"input_noise\": combo_kwargs[\"input_noise\"]=float(best_reg[\"value\"])\n",
    "    if task==\"cls\" and \"label_smooth\" not in combo_kwargs: combo_kwargs[\"label_smooth\"]=0.05\n",
    "    if task==\"reg\" and \"input_noise\" not in combo_kwargs: combo_kwargs[\"input_noise\"]=0.01\n",
    "    (combo, t_combo, mb_combo) = profile_call(f\"Part3::{task}::combo\", train_cfg, \"reg_combo_\"+task, **combo_kwargs)\n",
    "\n",
    "    plt.figure()\n",
    "    names = [\"Baseline\",\"Best Single\",\"Best Combo\"]\n",
    "    vals  = [baseline, best_reg[\"best_val\"], combo]\n",
    "    plt.bar(names, vals); plt.ylabel(\"validation loss\"); plt.title(f\"{task.upper()} — regularization\")\n",
    "    fn = os.path.join(FIG_DIR, f\"regularization_{task}.png\"); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
    "    print(\"[OL/Part3] Saved regularization bar plot ->\", fn)\n",
    "    sdf.to_csv(os.path.join(LOG_DIR, f\"reg_single_{task}.csv\"), index=False)\n",
    "    # also log combo timing\n",
    "    with open(os.path.join(LOG_DIR, f\"reg_combo_{task}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"best_single\":best_reg, \"combo_val\":combo, \"combo_time_sec\":t_combo, \"combo_peak_mb\":mb_combo}, f, indent=2)\n",
    "    return sdf, {\"baseline\":baseline, \"best_single\":best_reg, \"combo\":combo}\n",
    "\n",
    "# ---------- (9) Reporting & accounting helpers ----------\n",
    "def plot_loss_vs_compute(csv_log, title, outfile):\n",
    "    if not os.path.exists(csv_log):\n",
    "        print(\"[OL] Missing log:\", csv_log); return\n",
    "    df = pd.read_csv(csv_log)\n",
    "    if \"grad_evals\" not in df or \"val_loss\" not in df:\n",
    "        print(\"[OL] Missing required columns in\", csv_log); return\n",
    "    plt.figure(); plt.plot(df[\"grad_evals\"], df[\"val_loss\"])\n",
    "    plt.xlabel(\"# gradient evaluations\"); plt.ylabel(\"validation loss\"); plt.title(title)\n",
    "    fn=os.path.join(FIG_DIR, outfile); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
    "    print(\"[OL] Saved ->\", fn)\n",
    "\n",
    "# ============================================================\n",
    "# ============ PART 1 / PART 2 / PART 3 EXECUTION ============\n",
    "# ============================================================\n",
    "\n",
    "# PART 1 — Randomized Optimization (RHC/SA/GA)\n",
    "if 'H_loaders' in globals() and 'm_cls' in globals():\n",
    "    run_part1_RO(m_cls, H_loaders, task=\"cls\", out_dim=(1 if USE_BCE_FOR_BINARY else 2),\n",
    "                 base_tag=\"hotel\", k_unfrozen=1, ro_cfg=RO_CFG)\n",
    "if RUN_ACCIDENTS and 'A_loaders' in globals() and 'm_reg' in globals():\n",
    "    run_part1_RO(m_reg, A_loaders, task=\"reg\", out_dim=1,\n",
    "                 base_tag=\"accidents\", k_unfrozen=1, ro_cfg=RO_CFG)\n",
    "\n",
    "# PART 2 — Optimizer ablations (equal budgets; same splits)\n",
    "if 'H_loaders' in globals() and 'm_cls' in globals():\n",
    "    _ab_h = part2_ablation(m_cls, H_loaders, task=\"cls\", out_dim=(1 if USE_BCE_FOR_BINARY else 2), base_lr=1e-3)\n",
    "    if DO_HEATMAPS:\n",
    "        _ = sensitivity_heatmap(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2))\n",
    "        _ = sensitivity_heatmap(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2), which=\"b2\")\n",
    "    if DO_STABILITY:\n",
    "        _ = stability_band(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2), seeds=(0,1,2), opt_kind=\"adam\", lr=1e-3)\n",
    "\n",
    "if RUN_ACCIDENTS and 'A_loaders' in globals() and 'm_reg' in globals():\n",
    "    _ab_a = part2_ablation(m_reg, A_loaders, task=\"reg\", out_dim=1, base_lr=3e-4)\n",
    "    if DO_HEATMAPS:\n",
    "        _ = sensitivity_heatmap(m_reg, A_loaders, \"reg\", 1)\n",
    "        _ = sensitivity_heatmap(m_reg, A_loaders, \"reg\", 1, which=\"b2\")\n",
    "    if DO_STABILITY:\n",
    "        _ = stability_band(m_reg, A_loaders, \"reg\", 1, seeds=(0,1,2), opt_kind=\"adam\", lr=3e-4)\n",
    "\n",
    "# PART 3 — Regularization study (Adam only; reuse Part-2 Adam LR/betas)\n",
    "if 'H_loaders' in globals() and 'm_cls' in globals():\n",
    "    _s_h, _bars_h = part3_regularization(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2), adam_lr=1e-3)\n",
    "if RUN_ACCIDENTS and 'A_loaders' in globals() and 'm_reg' in globals():\n",
    "    _s_a, _bars_a = part3_regularization(m_reg, A_loaders, \"reg\", 1, adam_lr=3e-4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
