{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sobhanmoosavi/us-accidents\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "8JiW37V8CEY0"
      },
      "id": "8JiW37V8CEY0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbbf3a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bbbf3a5",
        "outputId": "b67d4420-5dce-472f-a38f-e65a42f5cf2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Data cleaning complete.\n",
            "Final data preparation is complete. Preprocessor now includes Target Encoding.\n",
            "--- Starting Grid Search for DecisionTreeClassifier ---\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "Grid Search completed in 7.72 seconds.\n",
            "Best parameters found: {'classifier__class_weight': 'balanced', 'classifier__max_depth': 16, 'classifier__min_samples_leaf': 100}\n",
            "--- Evaluation on Test Set ---\n",
            "ROC-AUC: 0.9332\n",
            "PR-AUC: 0.9020\n",
            "F1-Score (at threshold 0.5): 0.8043\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.84      0.87     15033\n",
            "           1       0.76      0.86      0.80      8845\n",
            "\n",
            "    accuracy                           0.85     23878\n",
            "   macro avg       0.83      0.85      0.84     23878\n",
            "weighted avg       0.85      0.85      0.85     23878\n",
            "\n",
            "--- Starting Grid Search for KNeighborsClassifier ---\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Supervised/Online Learning — Hotel (classification) + US Accidents (regression)\n",
        "\n",
        "References:\n",
        "[1] N. C. António, A. Almeida, and L. Nunes, \"Hotel booking demand datasets,\" Data in Brief, vol. 22, pp. 41-49, 2019.\n",
        "[2] S. Moosavi, M. H. Samavatian, et al., \"A Countrywide Traffic Accident Dataset,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. Workshops, 2019.\n",
        "[3] T. Saito and M. Rehmsmeier, \"The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets,\" PLOS ONE, vol. 10, no. 3, p. e0118432, 2015.\n",
        "[4] T. Chen and C. Guestrin, \"XGBoost: A Scalable Tree Boosting System,\" in Proc. 22nd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, 2016, pp. 785-794.\n",
        "[5] J. H. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine,” Annals of Statistics, vol. 29, no. 5, pp. 1189–1232, 2001.\n",
        "[6] T. M. Mitchell, \"Machine Learning,\" McGraw-Hill, 1997.\n",
        "[7] J. R. Quinlan, \"Induction of Decision Trees,\" Machine Learning, 1986.\n",
        "[8] P. E. Cover and P. E. Hart, \"Nearest Neighbor Pattern Classification,\" IEEE Transactions on Information Theory, vol. 13, no. 1, pp. 21-27, 1967.\n",
        "[9] C. J. Cortes and V. Vapnik, \"Support-Vector Networks,\" Machine Learning, vol. 20, no. 3, pp. 273-297, 1995.\n",
        "[10] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \"Learning representations by back-propagating errors,\" Nature, vol. 323, no. 6088, pp. 533-536, 1986.\n",
        "[11] N. C. António, A. Almeida, and L. Nunes, \"Hotel booking demand datasets,\" Data in Brief, vol. 22, pp. 41-49, 2019.\n",
        "[12] S. Moosavi et al., “US Accidents (since 2016),” [Online]. Available: Kaggle US Accidents.\n",
        "\"\"\"\n",
        "\n",
        "# Use a non-interactive backend for script runs (before importing pyplot)\n",
        "import matplotlib\n",
        "try:\n",
        "    matplotlib.use(\"Agg\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from IPython import get_ipython  # noqa\n",
        "    _IN_IPY = get_ipython() is not None\n",
        "except Exception:\n",
        "    _IN_IPY = False\n",
        "\n",
        "# memory_profiler: safe @mp_profile decorator & programmatic runner\n",
        "try:\n",
        "    from memory_profiler import profile as mp_profile, memory_usage as _memory_usage  # noqa\n",
        "    _HAVE_MEMPROF = True\n",
        "except Exception:\n",
        "    _HAVE_MEMPROF = False\n",
        "    def mp_profile(func):  # noqa\n",
        "        return func\n",
        "    def _memory_usage(*a, **k):  # noqa\n",
        "        return None\n",
        "\n",
        "# tqdm shim (fallback to identity if tqdm missing)\n",
        "try:\n",
        "    from tqdm import tqdm  # noqa\n",
        "except Exception:\n",
        "    def tqdm(x, **k):  # noqa\n",
        "        return x\n",
        "\n",
        "# display shim: if IPython not available, print instead\n",
        "try:\n",
        "    from IPython.display import display as ipy_display  # noqa\n",
        "except Exception:\n",
        "    def ipy_display(x):  # noqa\n",
        "        try:\n",
        "            import pandas as _pd\n",
        "            if isinstance(x, _pd.DataFrame):\n",
        "                print(x.head(20).to_string())\n",
        "            elif hasattr(x, \"to_string\"):\n",
        "                print(x.to_string())\n",
        "            else:\n",
        "                print(x)\n",
        "        except Exception:\n",
        "            print(x)\n",
        "\n",
        "# Programmatic memory-profiling runner for scripts\n",
        "import os, json, time, sys\n",
        "def run_with_memprofile(func, *args, enable=None, label=None, log_dir=\"./ol_outputs/logs\", **kwargs):\n",
        "    \"\"\"    Run a function and (optionally) record peak memory using memory_profiler.memory_usage.\n",
        "    - enable=True or set env ENABLE_MEMPROF=1 to activate\n",
        "    - returns (retval, peak_mb or None)\n",
        "    \"\"\"\n",
        "    enabled = enable if enable is not None else (os.getenv(\"ENABLE_MEMPROF\", \"0\") == \"1\")\n",
        "    if not enabled or not _HAVE_MEMPROF:\n",
        "        return func(*args, **kwargs), None\n",
        "    mem_list, retval = _memory_usage((func, args, kwargs), retval=True, interval=0.1)\n",
        "    try:\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        with open(os.path.join(log_dir, \"memory_profile.jsonl\"), \"a\", encoding=\"utf-8\") as f:\n",
        "            rec = {\"ts\": time.time(), \"label\": label or getattr(func, \"__name__\", \"func\"),\n",
        "                   \"peak_mb\": float(max(mem_list) if mem_list else -1.0)}\n",
        "            f.write(json.dumps(rec) + \"\\n\")\n",
        "    except Exception as _e:\n",
        "        print(\"[memprof] could not write log:\", _e, file=sys.stderr)\n",
        "    return retval, float(max(mem_list) if mem_list else -1.0)\n",
        "# ================================================================\n",
        "\n",
        "# %%\n",
        "# Install if needed else comment\n",
        "# [shell-magic removed] !pip install category_encoders\n",
        "# [shell-magic removed] !pip install memory_profiler\n",
        "# [notebook-magic removed] %load_ext memory_profiler\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Scikit-learn preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, TargetEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Scikit-learn models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# Scikit-learn model selection and metrics\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV, learning_curve, validation_curve\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, f1_score, roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix\n",
        "# Import matplotlib for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Data loading and cleaning\n",
        "try:\n",
        "    df_hotel = pd.read_csv('/hotel_bookings.csv')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'hotel_bookings.csv' not found.\")\n",
        "    df_hotel = pd.DataFrame()\n",
        "\n",
        "if not df_hotel.empty:\n",
        "    leakage_cols = ['reservation_status', 'reservation_status_date']\n",
        "    df_hotel_cleaned = df_hotel.drop(columns=leakage_cols).copy()  # Added .copy()\n",
        "\n",
        "    # Fix pandas FutureWarning by using .loc\n",
        "    df_hotel_cleaned.loc[:, 'agent'] = df_hotel_cleaned['agent'].fillna(0)\n",
        "    df_hotel_cleaned.loc[:, 'company'] = df_hotel_cleaned['company'].fillna(0)\n",
        "    df_hotel_cleaned.loc[:, 'country'] = df_hotel_cleaned['country'].fillna(df_hotel_cleaned['country'].mode()[0])\n",
        "    df_hotel_cleaned.loc[:, 'children'] = df_hotel_cleaned['children'].fillna(0)\n",
        "\n",
        "    df_hotel_cleaned[['children', 'agent', 'company']] = df_hotel_cleaned[['children', 'agent', 'company']].astype(int)\n",
        "    print(\"Data cleaning complete.\")\n",
        "\n",
        "# Helper functions to evaluate the model\n",
        "def evaluate_model(y_true, y_scores, threshold=0.5):\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    roc_auc = roc_auc_score(y_true, y_scores)\n",
        "    if np.min(y_scores) < 0:\n",
        "        threshold = 0.0\n",
        "    y_pred_class = (y_scores >= threshold).astype(int)\n",
        "    f1 = f1_score(y_true, y_pred_class)\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR-AUC: {pr_auc:.4f}\")\n",
        "    print(f\"F1-Score (at threshold {threshold}): {f1:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred_class))\n",
        "\n",
        "def train_and_evaluate_model(pipeline, param_grid, X_train, y_train, X_test, y_test):\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "    print(f\"--- Starting Grid Search for {pipeline.steps[-1][1].__class__.__name__} ---\")\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Grid Search completed in {end_time - start_time:.2f} seconds.\")\n",
        "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "    best_model = grid_search.best_estimator_\n",
        "    if hasattr(best_model, 'predict_proba'):\n",
        "        y_scores = best_model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_scores = best_model.decision_function(X_test)\n",
        "    print(\"--- Evaluation on Test Set ---\")\n",
        "    evaluate_model(y_test, y_scores)\n",
        "    return best_model\n",
        "\n",
        "# Target encoding for high cardinality and On-hot for low cardinality\n",
        "X = df_hotel_cleaned.drop(columns=['is_canceled'])\n",
        "y = df_hotel_cleaned['is_canceled']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "# We define high-cardinality features to be target encoded\n",
        "target_encode_features = ['country', 'agent', 'company']\n",
        "# All other non-numeric features will be one-hot encoded\n",
        "ohe_features = [col for col in X_train.select_dtypes(exclude=np.number).columns if col not in target_encode_features]\n",
        "\n",
        "# UPDATED: Create the new preprocessor with three steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ohe_features),\n",
        "        ('te', TargetEncoder() if TargetEncoder is not None else OneHotEncoder(handle_unknown='ignore', sparse_output=False), target_encode_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "print(\"Final data preparation is complete. Preprocessor now includes Target Encoding.\")\n",
        "\n",
        "# Run the experiments\n",
        "\n",
        "# --- Decision Tree --- #\n",
        "dt_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42))])\n",
        "dt_param_grid = {'classifier__max_depth': [8, 16], 'classifier__min_samples_leaf': [100, 200], 'classifier__class_weight': ['balanced']}\n",
        "best_dt_model = train_and_evaluate_model(dt_pipeline, dt_param_grid, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# --- k-Nearest Neighbors ---\n",
        "knn_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
        "knn_param_grid = {'classifier__n_neighbors': [5, 11]}\n",
        "best_knn_model = train_and_evaluate_model(knn_pipeline, knn_param_grid, X_train, y_train, X_test, y_test)\n",
        "\n",
        "\n",
        "# ## Train and evaluate model\n",
        "\n",
        "# Neural Network\n",
        "\n",
        "# adding just in case it dosesnt show up in the initial import\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "print(\"--- Training Shallow-Wide Neural Network ---\")\n",
        "nn_shallow_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', MLPClassifier(hidden_layer_sizes=(512, 512), solver='sgd', learning_rate_init=0.01, batch_size=512, max_iter=100, early_stopping=True, random_state=42, verbose=False))])\n",
        "# [notebook-magic removed] %memit nn_shallow_pipeline.fit(X_train, y_train)\n",
        "# First fit the pipeline\n",
        "nn_shallow_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Then make predictions\n",
        "y_scores_shallow = nn_shallow_pipeline.predict_proba(X_test)[:, 1]\n",
        "evaluate_model(y_test, y_scores_shallow)\n",
        "\n",
        "print(\"--- Training Deeper-Narrower Neural Network ---\")\n",
        "nn_deep_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', MLPClassifier(hidden_layer_sizes=(256, 256, 128, 128), solver='sgd', learning_rate_init=0.01, batch_size=512, max_iter=100, early_stopping=True, random_state=42, verbose=False))])\n",
        "# [notebook-magic removed] %memit nn_deep_pipeline.fit(X_train, y_train)\n",
        "nn_deep_pipeline.fit(X_train, y_train)\n",
        "print(\"--- Evaluation of Deeper-Narrower NN ---\")\n",
        "y_scores_deep = nn_deep_pipeline.predict_proba(X_test)[:, 1]\n",
        "evaluate_model(y_test, y_scores_deep)\n",
        "\n",
        "# %%\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
        "\n",
        "# --- Linear SVM --- #\n",
        "linear_svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearSVC(random_state=42, max_iter=2000))])\n",
        "linear_svm_param_grid = {'classifier__C': [0.1, 1, 10], 'classifier__class_weight': ['balanced']}\n",
        "best_linear_svm_model = train_and_evaluate_model(linear_svm_pipeline, linear_svm_param_grid, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# --- RBF SVM --- #\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=20000, random_state=42, stratify=y_train)\n",
        "rbf_svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', SVC(kernel='rbf', probability=True, random_state=42))])\n",
        "rbf_svm_param_grid = {'classifier__C': [0.5, 2, 8], 'classifier__gamma': ['scale', 'auto'], 'classifier__class_weight': ['balanced']}\n",
        "random_search_rbf = RandomizedSearchCV(rbf_svm_pipeline, param_distributions=rbf_svm_param_grid, n_iter=6, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1, random_state=42)\n",
        "print(f\"--- Starting Randomized Search for RBF SVM on {len(X_train_sample)} samples ---\")\n",
        "\n",
        "# FIT the model first before accessing best_estimator_\n",
        "random_search_rbf.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "# Now you can access best_estimator_\n",
        "best_rbf_svm_model = random_search_rbf.best_estimator_\n",
        "print(\"Best parameters found:\", random_search_rbf.best_params_)\n",
        "y_scores_rbf = best_rbf_svm_model.predict_proba(X_test)[:, 1]\n",
        "print(\"--- Evaluation on Test Set ---\")\n",
        "evaluate_model(y_test, y_scores_rbf)\n",
        "\n",
        "# Add this helper function to safely access search results:\n",
        "def safe_get_best_model(search_cv, X_train, y_train):\n",
        "    \"\"\"Safely fit and get best model from search CV object.\"\"\"\n",
        "    if not hasattr(search_cv, 'best_estimator_') or search_cv.best_estimator_ is None:\n",
        "        print(\"Fitting search CV object...\")\n",
        "        search_cv.fit(X_train, y_train)\n",
        "    return search_cv.best_estimator_\n",
        "\n",
        "# %%\n",
        "# Plotting\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, cv=5, n_jobs=-1):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=np.linspace(.1, 1.0, 5), scoring='roc_auc'\n",
        "    )\n",
        "    train_scores_mean, test_scores_mean = np.mean(train_scores, axis=1), np.mean(test_scores, axis=1)\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"ROC-AUC Score\")\n",
        "    plt.grid()\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.savefig(f\"Hotel_{title.replace(' ', '_').lower()}.png\")  # Save as PNG\n",
        "    plt.close()  # Close the figure to avoid displaying it\n",
        "\n",
        "def plot_validation_curve(estimator, title, X, y, param_name, param_range, cv=5, n_jobs=-1):\n",
        "    train_scores, test_scores = validation_curve(\n",
        "        estimator, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\"roc_auc\", n_jobs=n_jobs\n",
        "    )\n",
        "    train_scores_mean, test_scores_mean = np.mean(train_scores, axis=1), np.mean(test_scores, axis=1)\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(param_name)\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0.0, 1.1)\n",
        "    plt.grid()\n",
        "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\")\n",
        "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.savefig(f\"Hotel_{title.replace(' ', '_').lower()}.png\")  # Save as PNG\n",
        "    plt.close()  # Close the figure to avoid displaying it\n",
        "\n",
        "\n",
        "def plot_roc_pr_curves(models, X_test, y_test, save_dir=\".\"):\n",
        "    \"\"\"    Plots ROC and PR curves for a dictionary of trained models.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_scores = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_scores = model.decision_function(X_test)\n",
        "\n",
        "        RocCurveDisplay.from_predictions(y_test, y_scores, name=name, ax=ax1)\n",
        "        PrecisionRecallDisplay.from_predictions(y_test, y_scores, name=name, ax=ax2)\n",
        "\n",
        "    ax1.set_title(\"ROC Curves\")\n",
        "    ax2.set_title(\"Precision-Recall Curves\")\n",
        "    ax1.grid(True)\n",
        "    ax2.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Hotel_ROC_and_PR_Curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_nn_loss_curve(nn_model, title, save_dir=\".\"):\n",
        "    \"\"\"    Plots the training loss curve from a trained scikit-learn MLP model.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(nn_model.loss_curve_)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{title.replace(' ', '_')}.png\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"--- Generating Learning Curves ---\")\n",
        "plot_learning_curve(best_dt_model, \"Learning Curve (Decision Tree)\", X_train, y_train)\n",
        "plot_learning_curve(best_knn_model, \"Learning Curve (k-NN)\", X_train, y_train)\n",
        "plot_learning_curve(best_linear_svm_model, \"Learning Curve (Linear SVM)\", X_train, y_train)\n",
        "plot_learning_curve(best_rbf_svm_model, \"Learning Curve (RBF SVM)\", X_train_sample, y_train_sample)\n",
        "\n",
        "print(\"--- Generating Model Complexity Curve for Decision Tree ---\")\n",
        "dt_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier(random_state=42, class_weight='balanced'))])\n",
        "param_range_depth = np.arange(2, 25)\n",
        "plot_validation_curve(dt_pipeline_untuned, \"Validation Curve (Decision Tree vs. max_depth)\", X_train, y_train, param_name=\"classifier__max_depth\", param_range=param_range_depth)\n",
        "models_to_plot = {\n",
        "    \"Decision Tree\": best_dt_model,\n",
        "    \"k-NN\": best_knn_model,\n",
        "    \"Linear SVM\": best_linear_svm_model,\n",
        "    \"RBF SVM\": best_rbf_svm_model,\n",
        "    \"Shallow NN\": nn_shallow_pipeline,\n",
        "    \"Deep NN\": nn_deep_pipeline\n",
        "}\n",
        "plot_roc_pr_curves(models_to_plot, X_test, y_test)\n",
        "\n",
        "print(\"--- Generating NN Loss Curves ---\")\n",
        "plot_nn_loss_curve(nn_shallow_pipeline.named_steps['classifier'], \"NN classifier Loss Curve (Shallow)\")\n",
        "plot_nn_loss_curve(nn_deep_pipeline.named_steps['classifier'], \"NN classifier Loss Curve (Deep)\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# --- FINAL SUMMARY TABLE ---\n",
        "results, models = [], {\"Decision Tree\": best_dt_model, \"k-NN\": best_knn_model, \"Linear SVM\": best_linear_svm_model, \"RBF SVM\": best_rbf_svm_model, \"Shallow NN\": nn_shallow_pipeline, \"Deep NN\": nn_deep_pipeline}\n",
        "\n",
        "print(\"--- Generating Final Summary Table ---\")\n",
        "for name, model in models.items():\n",
        "\n",
        "    # Time the prediction step\n",
        "    start_time = time.time()\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_scores = model.decision_function(X_test)\n",
        "    end_time = time.time()\n",
        "    predict_time = end_time - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    roc_auc, (precision, recall, _) = roc_auc_score(y_test, y_scores), precision_recall_curve(y_test, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    threshold = 0.0 if \"Linear SVM\" in name else 0.5 # RBF SVM has predict_proba, so it uses 0.5\n",
        "    f1 = f1_score(y_test, (y_scores >= threshold).astype(int))\n",
        "\n",
        "    # Append all results\n",
        "    results.append({\"Model\": name, \"ROC-AUC\": roc_auc, \"PR-AUC\": pr_auc, \"F1-Score\": f1, \"Predict Time (s)\": predict_time})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"--- Final Model Comparison ---\")\n",
        "print(df_results.sort_values(by='ROC-AUC', ascending=False))\n",
        "\n",
        "# %%\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Neural Network\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- Create and fit a scaler for the target variable ---\n",
        "y_scaler = StandardScaler()\n",
        "y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "print(\"--- Training Shallow-Wide Neural Network (Regressor with Scaled Target) ---\")\n",
        "\n",
        "nn_shallow_reg_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', MLPRegressor(\n",
        "        hidden_layer_sizes=(512, 512),\n",
        "        solver='sgd',\n",
        "        learning_rate='adaptive', # Keep the adaptive rate\n",
        "        learning_rate_init=0.01,\n",
        "        batch_size=1024,\n",
        "        max_iter=100,  # Increased from 15\n",
        "        early_stopping=True,\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit the model using the scaled y_train\n",
        "# [notebook-magic removed] %memit nn_shallow_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "\n",
        "nn_shallow_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
        "y_pred_scaled = nn_shallow_reg_pipeline.predict(X_test)\n",
        "\n",
        "# Convert the scaled predictions back to the original, unscaled y_test\n",
        "y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
        "\n",
        "\n",
        "print(\"--- Evaluation on Test Set ---\")\n",
        "# Compare the unscaled predictions to the original, unscaled y_test\n",
        "evaluate_model(y_test, y_pred_unscaled.ravel())\n",
        "\n",
        "\n",
        "# NOTE: We are reusing the 'y_scaler' that was fit on y_train in the previous step.\n",
        "\n",
        "# --- Define and train the DEEP NN pipeline on the SCALED target data ---\n",
        "print(\"--- Training Deeper-Narrower Neural Network (Regressor with Scaled Target) ---\")\n",
        "\n",
        "nn_deep_reg_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', MLPRegressor(\n",
        "        hidden_layer_sizes=(256, 256, 128, 128), # Deeper-narrower architecture\n",
        "        solver='sgd',\n",
        "        learning_rate='adaptive',\n",
        "        learning_rate_init=0.01,\n",
        "        batch_size=1024,\n",
        "        max_iter=100,  # Increased from 15\n",
        "        early_stopping=True,\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit the model using the same scaled y_train\n",
        "# [notebook-magic removed] %memit nn_deep_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "nn_deep_reg_pipeline.fit(X_train, y_train_scaled.ravel())\n",
        "# The model will predict in the scaled space\n",
        "y_pred_deep_scaled = nn_deep_reg_pipeline.predict(X_test)\n",
        "\n",
        "# Convert the scaled predictions back to the original units (minutes) using the same scaler\n",
        "y_pred_deep_unscaled = y_scaler.inverse_transform(y_pred_deep_scaled.reshape(-1, 1))\n",
        "\n",
        "\n",
        "print(\"--- Evaluation on Test Set ---\")\n",
        "# Compare the unscaled predictions to the original, unscaled y_test\n",
        "evaluate_model(y_test, y_pred_deep_unscaled.ravel())\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn preprocessing and pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Scikit-learn regression models\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Scikit-learn model selection and metrics for regression\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, learning_curve, validation_curve\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "output_dir = \"US_accdnt_out\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory '{output_dir}' is ready.\")\n",
        "\n",
        "# Helper functions for model evaluation\n",
        "def evaluate_regression_model(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "\n",
        "def train_and_evaluate_regression_model(pipeline, param_grid, X_train, y_train, X_test, y_test):\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
        "    print(f\"--- Starting Grid Search for {pipeline.steps[-1][1].__class__.__name__} ---\")\n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Grid Search completed in {end_time - start_time:.2f} seconds.\")\n",
        "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "    print(f\"Best MAE on validation data: {-grid_search.best_score_:.4f}\")\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    print(\"--- Evaluation on Test Set ---\")\n",
        "    evaluate_regression_model(y_test, y_pred)\n",
        "    return best_model\n",
        "\n",
        "# Data loading and cleaning\n",
        "try:\n",
        "    # df_accidents = pd.read_csv('US_Accidents_March23.csv')\n",
        "    df_accidents = pd.read_csv('/kaggle/input/us-accidents/US_Accidents_March23.csv')\n",
        "    print(\"US Accidents dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Accidents dataset file not found.\")\n",
        "    df_accidents = pd.DataFrame()\n",
        "\n",
        "if not df_accidents.empty:\n",
        "    cols_to_drop = ['ID', 'Weather_Timestamp', 'Airport_Code', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'City', 'Street', 'Zipcode', 'County', 'Country', 'State', 'Timezone', 'Weather_Condition']\n",
        "    df_cleaned = df_accidents.drop(columns=cols_to_drop)\n",
        "    # Use format='mixed' to handle variations in datetime strings\n",
        "    df_cleaned['Start_Time'] = pd.to_datetime(df_cleaned['Start_Time'], format='mixed')\n",
        "    df_cleaned['End_Time'] = pd.to_datetime(df_cleaned['End_Time'], format='mixed')\n",
        "    df_cleaned['Duration'] = (df_cleaned['End_Time'] - df_cleaned['Start_Time']).dt.total_seconds() / 60\n",
        "    df_cleaned = df_cleaned[df_cleaned['Duration'] > 0]\n",
        "\n",
        "    # Drop the 'Description' column as it contains string values and is not used in modeling\n",
        "    if 'Description' in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.drop(columns=['Description'])\n",
        "\n",
        "    # Identify categorical columns for imputation\n",
        "    categorical_cols = df_cleaned.select_dtypes(include='object').columns\n",
        "\n",
        "    # Impute missing values in categorical columns with 'Unknown'\n",
        "    for col in categorical_cols:\n",
        "        df_cleaned[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "    # Drop rows with any remaining missing values (should only be numeric now)\n",
        "    df_cleaned.dropna(inplace=True)\n",
        "\n",
        "    bool_cols = df_cleaned.select_dtypes(include='bool').columns\n",
        "    df_cleaned[bool_cols] = df_cleaned[bool_cols].astype(int)\n",
        "    num_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
        "    df_cleaned[num_cols] = df_cleaned[num_cols].astype(np.float32)\n",
        "\n",
        "\n",
        "    print(\"Data cleaning and feature engineering complete.\")\n",
        "\n",
        "# Sampling and Data preparation\n",
        "df_sample, _ = train_test_split(df_cleaned, train_size=1500000, stratify=df_cleaned['Severity'], random_state=42)\n",
        "X = df_sample.drop(columns=['Duration', 'Severity'])\n",
        "y = df_sample['Duration']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_svm, _, y_train_svm, _ = train_test_split(X_train, y_train, train_size=25000, random_state=42)\n",
        "X_train_knn, _, y_train_knn, _ = train_test_split(X_train, y_train, train_size=250000, random_state=42)\n",
        "\n",
        "# Define numeric features based on the updated X_train\n",
        "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "# Identify all non-numeric columns as categorical features\n",
        "categorical_features = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "# Create a ColumnTransformer to handle different feature types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns (if any)\n",
        ")\n",
        "\n",
        "print(f\"Data sampling complete. Main training set: {len(X_train)}, k-NN sample: {len(X_train_knn)}, SVM sample: {len(X_train_svm)}\")\n",
        "print(\"Preprocessor (ColumnTransformer) defined to handle numeric and categorical features.\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# Run the Experiments\n",
        "# --- Decision Tree Regressor ---\n",
        "dt_reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))])\n",
        "dt_reg_param_grid = {'regressor__max_depth': [8, 16], 'regressor__min_samples_leaf': [200, 400]}\n",
        "dt_reg_pipeline.fit(X_train, y_train)  # Initial fit to avoid refit issues\n",
        "best_dt_reg_model = train_and_evaluate_regression_model(dt_reg_pipeline, dt_reg_param_grid, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# --- k-NN Regressor --- (Uses k-NN sample)\n",
        "knn_reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_jobs=-1))])\n",
        "knn_reg_param_grid = {'regressor__n_neighbors': [5, 11]}\n",
        "knn_reg_pipeline.fit(X_train_knn, y_train_knn)  # Initial fit to avoid refit issues\n",
        "best_knn_reg_model = train_and_evaluate_regression_model(knn_reg_pipeline, knn_reg_param_grid, X_train_knn, y_train_knn, X_test, y_test)\n",
        "\n",
        "# --- Linear SVR ---\n",
        "linear_svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearSVR(random_state=42, max_iter=2000))])\n",
        "linear_svr_param_grid = {'regressor__C': [0.1, 1, 10]}\n",
        "linear_svr_pipeline.fit(X_train, y_train)  # Initial fit to avoid refit issues\n",
        "best_linear_svr_model = train_and_evaluate_regression_model(linear_svr_pipeline, linear_svr_param_grid, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# --- RBF SVR (Optimized) --- (Uses SVM sample)\n",
        "rbf_svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR(kernel='rbf'))])\n",
        "rbf_svr_param_grid = {'regressor__C': [1, 10], 'regressor__gamma': ['scale']}\n",
        "# Changed RandomizedSearchCV to GridSearchCV\n",
        "rbf_svr_pipeline.fit(X_train_svm, y_train_svm)  # Initial fit to avoid refit issues\n",
        "grid_search_svr = GridSearchCV(rbf_svr_pipeline, param_grid=rbf_svr_param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
        "print(f\"--- Starting Grid Search for SVR (RBF) on {len(X_train_svm)} samples ---\")\n",
        "grid_search_svr.fit(X_train_svm, y_train_svm)\n",
        "best_rbf_svr_model = grid_search_svr.best_estimator_\n",
        "print(\"Best parameters found:\", grid_search_svr.best_params_)\n",
        "print(f\"Best MAE on validation data: {-grid_search_svr.best_score_:.4f}\")\n",
        "# Evaluate on test set\n",
        "y_pred_rbf = best_rbf_svr_model.predict(X_test)\n",
        "print(\"--- Evaluation on Test Set ---\"); evaluate_regression_model(y_test, y_pred_rbf)\n",
        "\n",
        "# Plotting functions\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import validation_curve, learning_curve\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def plot_learning_curve_regression(estimator, title, X, y, cv=3, n_jobs=-1, save_dir=\".\"):\n",
        "    # Calculate training and cross-validation scores\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=np.linspace(.1, 1.0, 5), scoring='neg_mean_absolute_error')\n",
        "    train_scores_mean, test_scores_mean = -np.mean(train_scores, axis=1), -np.mean(test_scores, axis=1)\n",
        "    # Plot learning curve\n",
        "    plt.figure(); plt.title(title); plt.xlabel(\"Training examples\"); plt.ylabel(\"Mean Absolute Error\"); plt.grid()\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    # Save and show plot\n",
        "    plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\")); plt.show()\n",
        "\n",
        "def plot_residuals(y_true, y_pred, title, save_dir=\".\"):\n",
        "    # Plots the residuals of a regression model.\n",
        "    residuals = y_true - y_pred\n",
        "    # Scatter plot of residuals\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_pred, residuals, alpha=0.2)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel(\"Predicted Values\")\n",
        "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    # Save and show plot\n",
        "    plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\"))\n",
        "    plt.show()\n",
        "\n",
        "def plot_nn_loss_curve(nn_model, title, save_dir=\".\"):\n",
        "    # Plots the training loss curve from a trained scikit-learn MLP model.\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(nn_model.loss_curve_)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    # Save and show plot\n",
        "    plt.savefig(os.path.join(save_dir, f\"{title.replace(' ', '_')}.png\"))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_validation_curve_regression(estimator, title, X, y, param_name, param_range, cv=3, n_jobs=-1, save_dir=\".\"):\n",
        "    # Calculate training and cross-validation scores for validation curve\n",
        "    train_scores, test_scores = validation_curve(\n",
        "        estimator, X, y, param_name=param_name, param_range=param_range,\n",
        "        cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=n_jobs\n",
        "    )\n",
        "\n",
        "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
        "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
        "\n",
        "    # Plot validation curve\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(param_name)\n",
        "    plt.ylabel(\"Mean Absolute Error\")\n",
        "    plt.grid()\n",
        "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\")\n",
        "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    # Save and show plot\n",
        "    plt.savefig(os.path.join(save_dir, f\"Accidnt_{title.replace(' ', '_')}.png\"))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate and Save Learning Curves\n",
        "print(\"--- Generating and Saving Learning Curves ---\")\n",
        "plot_learning_curve_regression(best_dt_reg_model, \"Learning Curve (Decision Tree)\", X_train, y_train, save_dir=output_dir)\n",
        "plot_learning_curve_regression(best_knn_reg_model, \"Learning Curve (k-NN)\", X_train_knn, y_train_knn, save_dir=output_dir)\n",
        "plot_learning_curve_regression(best_linear_svr_model, \"Learning Curve (Linear SVR)\", X_train, y_train, save_dir=output_dir)\n",
        "plot_learning_curve_regression(best_rbf_svr_model, \"Learning Curve (RBF SVR)\", X_train_svm, y_train_svm, save_dir=output_dir)\n",
        "\n",
        "# Generate and Save NN Loss Curves\n",
        "print(\"--- Generating and Saving NN Loss Curves ---\")\n",
        "plot_nn_loss_curve(nn_shallow_reg_pipeline.named_steps['regressor'],\n",
        "                   \"NN Regressor Loss Curve (Shallow)\",\n",
        "                   save_dir=output_dir)\n",
        "\n",
        "plot_nn_loss_curve(nn_deep_reg_pipeline.named_steps['regressor'],\n",
        "                   \"NN Regressor Loss Curve (Deep)\",\n",
        "                   save_dir=output_dir)\n",
        "\n",
        "# Generating residual plots\n",
        "y_pred_dt = best_dt_reg_model.predict(X_test)\n",
        "plot_residuals(y_test, y_pred_dt, \"Residuals Plot (Decision Tree)\", save_dir=output_dir)\n",
        "\n",
        "# Generating Model Complexity (Validation) Curves\n",
        "print(\"--- Generating and Saving Model Complexity (Validation) Curves ---\")\n",
        "\n",
        "# Validation Curve for Decision Tree Regressor\n",
        "dt_reg_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', DecisionTreeRegressor(random_state=42))])\n",
        "param_range_depth = np.arange(4, 25, 4) # Test depths from 4 to 24\n",
        "\n",
        "plot_validation_curve_regression(\n",
        "    dt_reg_pipeline_untuned,\n",
        "    \"Validation Curve (Decision Tree vs max_depth)\",\n",
        "    X_train,\n",
        "    y_train,\n",
        "    param_name=\"regressor__max_depth\",\n",
        "    param_range=param_range_depth,\n",
        "    save_dir=output_dir\n",
        ")\n",
        "\n",
        "# Validation Curve for k-NN Regressor\n",
        "# Note: This runs on the smaller k-NN subsample for efficiency\n",
        "knn_reg_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', KNeighborsRegressor(n_jobs=-1))])\n",
        "param_range_k = [3, 5, 8, 11, 15, 21] # Test various numbers of neighbors\n",
        "\n",
        "plot_validation_curve_regression(\n",
        "    knn_reg_pipeline_untuned,\n",
        "    \"Validation Curve (k-NN vs n_neighbors)\",\n",
        "    X_train_knn, # Using the k-NN subsample\n",
        "    y_train_knn,\n",
        "    param_name=\"regressor__n_neighbors\",\n",
        "    param_range=param_range_k,\n",
        "    save_dir=output_dir\n",
        ")\n",
        "\n",
        "# Validation Curve for Linear SVR\n",
        "linear_svr_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearSVR(random_state=42, max_iter=2000))])\n",
        "# Test the regularization parameter C on a logarithmic scale\n",
        "param_range_c_linear = np.logspace(-2, 2, 5) # e.g., [0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "plot_validation_curve_regression(\n",
        "    linear_svr_pipeline_untuned,\n",
        "    \"Validation Curve (Linear SVR vs C)\",\n",
        "    X_train,\n",
        "    y_train,\n",
        "    param_name=\"regressor__C\",\n",
        "    param_range=param_range_c_linear,\n",
        "    save_dir=output_dir\n",
        ")\n",
        "\n",
        "# Validation Curve for RBF SVR\n",
        "# IMPORTANT: This MUST be run on the smallest SVM subsample to be computationally feasible.\n",
        "rbf_svr_pipeline_untuned = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', SVR(kernel='rbf'))])\n",
        "param_range_c_rbf = np.logspace(-1, 2, 4) # e.g., [0.1, 1, 10, 100]\n",
        "\n",
        "plot_validation_curve_regression(\n",
        "    rbf_svr_pipeline_untuned,\n",
        "    \"Validation Curve (RBF SVR vs C)\",\n",
        "    X_train_svm, # Using the smallest SVM subsample\n",
        "    y_train_svm,\n",
        "    param_name=\"regressor__C\",\n",
        "    param_range=param_range_c_rbf,\n",
        "    save_dir=output_dir\n",
        ")\n",
        "\n",
        "# Final Summary Table (Regression)\n",
        "results, models_reg = [], {\"Decision Tree\": best_dt_reg_model, \"k-NN\": best_knn_reg_model, \"Linear SVR\": best_linear_svr_model, \"RBF SVR\": best_rbf_svr_model}\n",
        "print(\"--- Generating Final Summary Table ---\")\n",
        "for name, model in models_reg.items():\n",
        "    start_time = time.time(); y_pred = model.predict(X_test); end_time = time.time()\n",
        "    predict_time = end_time - start_time\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"Predict Time (s)\": predict_time})\n",
        "df_results_reg = pd.DataFrame(results)\n",
        "print(\"--- Final Model Comparison (Regression) ---\")\n",
        "print(df_results_reg.sort_values(by='MAE', ascending=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e3cb11",
      "metadata": {
        "id": "f9e3cb11"
      },
      "outputs": [],
      "source": [
        "# Adding the code for OL enhancements (FAST MODE, with timing + memory profiling)\n",
        "# ---------- (0) One‐time setup & parity guarantees ----------\n",
        "import os, time, random, warnings, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Use a non-interactive backend for script runs (saves plots to files)\n",
        "import matplotlib\n",
        "try:\n",
        "    matplotlib.use(\"Agg\")\n",
        "except Exception:\n",
        "    pass\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ======= SPEED KNOBS (dev mode) =======\n",
        "FAST_MODE = True  # flip False for full report-quality runs\n",
        "\n",
        "# Prefer MPS on Macs, then CUDA, else CPU\n",
        "try:\n",
        "    import torch\n",
        "    _HAS_TORCH = True\n",
        "    DEVICE = (torch.device(\"mps\") if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
        "              else torch.device(\"cuda\") if torch.cuda.is_available()\n",
        "              else torch.device(\"cpu\"))\n",
        "except Exception:\n",
        "    _HAS_TORCH = False\n",
        "    class _CPU:\n",
        "        def __str__(self): return \"cpu\"\n",
        "    DEVICE = _CPU()\n",
        "\n",
        "# After DEVICE is computedfas\n",
        "IS_CUDA = (hasattr(torch, \"cuda\") and torch.cuda.is_available())\n",
        "IS_CPU  = (str(DEVICE) == \"cpu\")\n",
        "# Use 0 workers for MPS/CPU to avoid spawn/pickle issues and overhead\n",
        "NUM_WORKERS = 0 if (not IS_CUDA) else 2\n",
        "\n",
        "SEED = 4242\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s)\n",
        "    if _HAS_TORCH:\n",
        "        torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "        torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)\n",
        "print(f\"[OL] Torch device: {DEVICE} (cuda? {torch.cuda.is_available() if _HAS_TORCH else False})\")\n",
        "\n",
        "# ---- profiling helpers (OL) ----\n",
        "_MEMPROF_ENABLED = os.getenv(\"ENABLE_MEMPROF\", \"0\") == \"1\"\n",
        "try:\n",
        "    from memory_profiler import memory_usage as _mem_usage\n",
        "    _HAVE_MEMPROF_OL = True\n",
        "except Exception:\n",
        "    _HAVE_MEMPROF_OL = False\n",
        "\n",
        "def profile_call(label, func, *args, enable=None, **kwargs):\n",
        "    \"\"\"Run func(*args, **kwargs), record wall time and (optionally) peak MB.\"\"\"\n",
        "    enable = _MEMPROF_ENABLED if enable is None else bool(enable)\n",
        "    t0 = time.perf_counter()\n",
        "    peak = None\n",
        "    if enable and _HAVE_MEMPROF_OL:\n",
        "        mem_list, retval = _mem_usage((func, args, kwargs), retval=True, interval=0.1)\n",
        "        peak = float(max(mem_list) if mem_list else -1.0)\n",
        "    else:\n",
        "        retval = func(*args, **kwargs)\n",
        "    dt = time.perf_counter() - t0\n",
        "    try:\n",
        "        os.makedirs(\"./ol_outputs/logs\", exist_ok=True)\n",
        "        with open(\"./ol_outputs/logs/ol_profile.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps({\"ts\": time.time(), \"label\": label, \"seconds\": dt, \"peak_mb\": peak}) + \"\\n\")\n",
        "    except Exception as _e:\n",
        "        print(\"[OL] profile log write fail:\", _e)\n",
        "    return retval, dt, peak\n",
        "\n",
        "# Global budgets + toggles (equal budgets per study = OL requirement)\n",
        "if FAST_MODE:\n",
        "    BUDGET_UPDATES = 60\n",
        "    RO_CFG = {\"RHC\": 300, \"SA\": 600, \"GA\": 1200, \"GA_pop\": 20}\n",
        "    OPT_VARIANTS = [(\"adam\", {}), (\"sgd\", {})]\n",
        "    DO_HEATMAPS = False\n",
        "    DO_STABILITY = False\n",
        "    RUN_ACCIDENTS = False\n",
        "    TRAIN_BS = 256\n",
        "    EVAL_BS = 1024\n",
        "    NUM_WORKERS = 0\n",
        "else:\n",
        "    BUDGET_UPDATES = 200\n",
        "    RO_CFG = {\"RHC\": 2000, \"SA\": 4000, \"GA\": 8000, \"GA_pop\": 40}\n",
        "    OPT_VARIANTS = [\n",
        "        (\"sgd\", {}), (\"momentum\", {}), (\"nesterov\", {}),\n",
        "        (\"adam\", {}), (\"adam_nobias\", {}), (\"adam_beta1zero\", {}),\n",
        "        (\"adamw\", {\"wd\": 1e-4}),\n",
        "    ]\n",
        "    DO_HEATMAPS = True\n",
        "    DO_STABILITY = True\n",
        "    RUN_ACCIDENTS = True\n",
        "    TRAIN_BS = 256\n",
        "    EVAL_BS = 1024\n",
        "    NUM_WORKERS = 0\n",
        "\n",
        "print(f\"[FAST] mode={FAST_MODE} device={DEVICE} budgets(BUDGET_UPDATES={BUDGET_UPDATES}, RO={RO_CFG})\")\n",
        "\n",
        "OUT_DIR = \"./ol_outputs\"; FIG_DIR = os.path.join(OUT_DIR, \"figs\"); LOG_DIR = os.path.join(OUT_DIR, \"logs\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "def log_parity(info: dict, fname=\"parity_meta.json\"):\n",
        "    path = os.path.join(LOG_DIR, fname)\n",
        "    try:\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(info, f, indent=2, default=str)\n",
        "        print(f\"[OL] Parity meta logged -> {path}\")\n",
        "    except Exception as e:\n",
        "        print(\"[OL] Could not write parity meta:\", e)\n",
        "\n",
        "# ---------- Optional raw→preprocess (only if split arrays aren't exposed) ----------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def _make_ohe():\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "def _make_safe_target_encoder(cols):\n",
        "    try:\n",
        "        from category_encoders import TargetEncoder as _CE_TargetEncoder\n",
        "    except Exception as e:\n",
        "        print(\"[ENC] category_encoders not installed; using OneHotEncoder. Reason:\", e)\n",
        "        return _make_ohe()\n",
        "    try:\n",
        "        import pandas as _pd\n",
        "        te = _CE_TargetEncoder(cols=cols, smoothing=1.0)\n",
        "        Xs = _pd.DataFrame({c: [\"a\", \"b\", \"a\", \"c\"] for c in cols})\n",
        "        ys = _pd.Series([0, 1, 1, 0])\n",
        "        te.fit(Xs, ys); _ = te.transform(Xs)\n",
        "        print(\"[ENC] Using TargetEncoder for:\", cols)\n",
        "        return te\n",
        "    except Exception as e:\n",
        "        print(\"[ENC] TargetEncoder not usable; falling back to OneHotEncoder. Reason:\", e)\n",
        "        return _make_ohe()\n",
        "\n",
        "def _split_cols_by_type(df):\n",
        "    return (df.select_dtypes(include=np.number).columns.tolist(),\n",
        "            df.select_dtypes(exclude=np.number).columns.tolist())\n",
        "\n",
        "def _simple_na_fill(df):\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype.kind in \"biufc\":\n",
        "            df.loc[:, c] = df[c].fillna(df[c].median())\n",
        "        else:\n",
        "            df.loc[:, c] = df[c].fillna(\"Unknown\")\n",
        "    return df\n",
        "\n",
        "def build_preproc(X, high_card_cutoff=25):\n",
        "    nums, cats = _split_cols_by_type(X)\n",
        "    hi = [c for c in cats if X[c].nunique(dropna=False) > high_card_cutoff]\n",
        "    lo = [c for c in cats if c not in hi]\n",
        "    steps=[]\n",
        "    if nums: steps.append((\"num\", StandardScaler(), nums))\n",
        "    if lo:   steps.append((\"ohe\", _make_ohe(), lo))\n",
        "    if hi:   steps.append((\"hi_cat\", _make_safe_target_encoder(hi), hi))\n",
        "    return ColumnTransformer(steps, remainder=\"drop\", verbose_feature_names_out=False)\n",
        "\n",
        "HOTEL_CSV = os.environ.get(\"HOTEL_CSV\", \"/hotel_bookings.csv\")\n",
        "ACCIDENTS_CSV = os.environ.get(\"ACCIDENTS_CSV\", \"/kaggle/input/us-accidents/US_Accidents_March23.csv\")\n",
        "\n",
        "def load_hotel_raw():\n",
        "    df = pd.read_csv(HOTEL_CSV, low_memory=False)\n",
        "    for c in [\"reservation_status\", \"reservation_status_date\"]:\n",
        "        if c in df: df = df.drop(columns=[c])\n",
        "    df.loc[:, \"agent\"] = df.get(\"agent\", pd.Series(index=df.index)).fillna(0)\n",
        "    df.loc[:, \"company\"] = df.get(\"company\", pd.Series(index=df.index)).fillna(0)\n",
        "    if \"country\" in df:\n",
        "        df.loc[:, \"country\"] = df[\"country\"].fillna(df[\"country\"].mode(dropna=True).iloc[0])\n",
        "    if \"children\" in df:\n",
        "        df.loc[:, \"children\"] = df[\"children\"].fillna(0)\n",
        "    y = df[\"is_canceled\"].astype(int)\n",
        "    X = _simple_na_fill(df.drop(columns=[\"is_canceled\"]))\n",
        "    return X, y\n",
        "\n",
        "def load_accidents_raw():\n",
        "    df = pd.read_csv(ACCIDENTS_CSV, low_memory=True)\n",
        "    df[\"Start_Time\"] = pd.to_datetime(df[\"Start_Time\"], errors=\"coerce\")\n",
        "    df[\"End_Time\"]   = pd.to_datetime(df[\"End_Time\"], errors=\"coerce\")\n",
        "    df[\"Duration\"]   = (df[\"End_Time\"] - df[\"Start_Time\"]).dt.total_seconds()/60.0\n",
        "    df = df.dropna(subset=[\"Duration\"]).loc[lambda d: d[\"Duration\"]>0]\n",
        "    if \"Description\" in df: df = df.drop(columns=[\"Description\"])\n",
        "    y = df[\"Duration\"].astype(np.float32)\n",
        "    X = _simple_na_fill(df.drop(columns=[\"Duration\",\"Start_Time\",\"End_Time\"]))\n",
        "    return X, y\n",
        "\n",
        "def _np1d(v):\n",
        "    import pandas as _pd\n",
        "    return v.values if isinstance(v, (_pd.Series, _pd.DataFrame)) else v\n",
        "\n",
        "# Build arrays if SL didn’t export them\n",
        "if not all(n in globals() for n in [\"Xh_tr_f\",\"Xh_va_f\",\"Xh_te_f\",\"yh_tr\",\"yh_va\",\"yh_te\"]):\n",
        "    Xh, yh = load_hotel_raw()\n",
        "    pre_h = build_preproc(Xh)\n",
        "    Xh_tr, Xh_tmp, yh_tr, yh_tmp = train_test_split(Xh, yh, test_size=0.3, random_state=SEED, stratify=yh)\n",
        "    Xh_va, Xh_te, yh_va, yh_te   = train_test_split(Xh_tmp, yh_tmp, test_size=0.5, random_state=SEED, stratify=yh_tmp)\n",
        "    Xh_tr_f = pre_h.fit_transform(Xh_tr, yh_tr); Xh_va_f = pre_h.transform(Xh_va); Xh_te_f = pre_h.transform(Xh_te)\n",
        "\n",
        "if FAST_MODE:\n",
        "    RUN_ACCIDENTS = False  # ensure\n",
        "if RUN_ACCIDENTS and not all(n in globals() for n in [\"Xa_tr_f\",\"Xa_va_f\",\"Xa_te_f\",\"ya_tr\",\"ya_va\",\"ya_te\"]):\n",
        "    Xa, ya = load_accidents_raw()\n",
        "    pre_a = build_preproc(Xa)\n",
        "    Xa_tr, Xa_tmp, ya_tr, ya_tmp = train_test_split(Xa, ya, test_size=0.3, random_state=SEED)\n",
        "    Xa_va, Xa_te, ya_va, ya_te   = train_test_split(Xa_tmp, ya_tmp, test_size=0.5, random_state=SEED)\n",
        "    Xa_tr_f = pre_a.fit_transform(Xa_tr, ya_tr); Xa_va_f = pre_a.transform(Xa_va); Xa_te_f = pre_a.transform(Xa_te)\n",
        "\n",
        "ADAPTER_OK = all(n in globals() for n in [\n",
        "    \"Xh_tr_f\",\"Xh_va_f\",\"Xh_te_f\",\"yh_tr\",\"yh_va\",\"yh_te\"\n",
        "]) and (True if not RUN_ACCIDENTS else all(n in globals() for n in [\n",
        "    \"Xa_tr_f\",\"Xa_va_f\",\"Xa_te_f\",\"ya_tr\",\"ya_va\",\"ya_te\"\n",
        "]))\n",
        "print(\"[OL] Adapter bound? ->\", ADAPTER_OK)\n",
        "\n",
        "# ---------- (1) Data: arrays → DataLoaders ----------\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NPDataset(Dataset):\n",
        "    def __init__(self, X, y, task):\n",
        "        self.X = np.asarray(X, dtype=np.float32)\n",
        "        self.y = _np1d(y).astype(np.int64 if task==\"cls\" else np.float32)\n",
        "        self.task = task\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        return (torch.from_numpy(self.X[i]),\n",
        "                torch.tensor(self.y[i]))\n",
        "\n",
        "def make_loaders(Xtr, ytr, Xva, yva, Xte, yte, task, bs_train=None, bs_eval=None, num_workers=None):\n",
        "    bs_train = (bs_train or 256)\n",
        "    bs_eval  = (bs_eval  or 1024)\n",
        "    if num_workers is None:\n",
        "        num_workers = NUM_WORKERS\n",
        "    pin = True if IS_CUDA else False\n",
        "\n",
        "    dtr = NPDataset(Xtr, ytr, task)\n",
        "    dva = NPDataset(Xva, yva, task)\n",
        "    dte = NPDataset(Xte, yte, task)\n",
        "\n",
        "    tr = DataLoader(dtr, batch_size=bs_train, shuffle=True,  drop_last=False,\n",
        "                    num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
        "    va = DataLoader(dva, batch_size=bs_eval,  shuffle=False, drop_last=False,\n",
        "                    num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
        "    te = DataLoader(dte, batch_size=bs_eval,  shuffle=False, drop_last=False,\n",
        "                    num_workers=num_workers, pin_memory=pin, persistent_workers=False)\n",
        "    return tr, va, te\n",
        "\n",
        "\n",
        "if all(n in globals() for n in [\"Xh_tr_f\",\"Xh_va_f\",\"Xh_te_f\",\"yh_tr\",\"yh_va\",\"yh_te\"]):\n",
        "    H_in = np.asarray(Xh_tr_f).shape[1]\n",
        "    H_loaders = make_loaders(Xh_tr_f, yh_tr, Xh_va_f, yh_va, Xh_te_f, yh_te, task=\"cls\",\n",
        "                             bs_train=TRAIN_BS, bs_eval=EVAL_BS, num_workers=NUM_WORKERS)\n",
        "    log_parity({\"dataset\":\"Hotel\",\"Xtr\":list(np.asarray(Xh_tr_f).shape),\"Xva\":list(np.asarray(Xh_va_f).shape),\n",
        "                \"Xte\":list(np.asarray(Xh_te_f).shape),\"bs_train\":TRAIN_BS,\"bs_eval\":EVAL_BS})\n",
        "\n",
        "if RUN_ACCIDENTS and all(n in globals() for n in [\"Xa_tr_f\",\"Xa_va_f\",\"Xa_te_f\",\"ya_tr\",\"ya_va\",\"ya_te\"]):\n",
        "    A_in = np.asarray(Xa_tr_f).shape[1]\n",
        "    A_loaders = make_loaders(Xa_tr_f, ya_tr, Xa_va_f, ya_va, Xa_te_f, ya_te, task=\"reg\",\n",
        "                             bs_train=TRAIN_BS, bs_eval=EVAL_BS, num_workers=NUM_WORKERS)\n",
        "    log_parity({\"dataset\":\"Accidents\",\"Xtr\":list(np.asarray(Xa_tr_f).shape),\"Xva\":list(np.asarray(Xa_va_f).shape),\n",
        "                \"Xte\":list(np.asarray(Xa_te_f).shape),\"bs_train\":TRAIN_BS,\"bs_eval\":EVAL_BS})\n",
        "\n",
        "# ---------- (2) Model: mirror SL MLP with nn.Module ----------\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=(128,64), out_dim=2, dropout_p=0.0):\n",
        "        super().__init__()\n",
        "        layers=[]; dims=[in_dim]+list(hidden)\n",
        "        for a,b in zip(dims[:-1], dims[1:]):\n",
        "            layers += [nn.Linear(a,b), nn.ReLU()]\n",
        "            if dropout_p>0: layers += [nn.Dropout(dropout_p)]\n",
        "        layers += [nn.Linear(dims[-1] if hidden else in_dim, out_dim)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "if 'H_in' in globals():\n",
        "    m_cls = MLP(H_in, hidden=(128,64), out_dim=2, dropout_p=0.0).to(DEVICE)\n",
        "    print(\"[OL] Hotel MLP:\", m_cls)\n",
        "if RUN_ACCIDENTS and 'A_in' in globals():\n",
        "    m_reg = MLP(A_in, hidden=(512,256), out_dim=1, dropout_p=0.0).to(DEVICE)\n",
        "    print(\"[OL] Accidents MLP:\", m_reg)\n",
        "\n",
        "# ---------- (3) Freezing all but last k layers (RO) ----------\n",
        "def linear_layers(model): return [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "def freeze_all_but_last_k(model, k=1):\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    Ls = linear_layers(model)\n",
        "    for m in Ls[-k:]:\n",
        "        for p in m.parameters(): p.requires_grad = True\n",
        "    tot = sum(p.numel() for p in model.parameters())\n",
        "    trn = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"[OL] Params total={tot:,} | trainable(last {k})={trn:,}\")\n",
        "    return trn\n",
        "\n",
        "# ---------- (4) Losses & metrics ----------\n",
        "USE_BCE_FOR_BINARY = False\n",
        "\n",
        "def make_criterion(task=\"cls\", out_dim=2):\n",
        "    if task==\"cls\":\n",
        "        if USE_BCE_FOR_BINARY and out_dim==1: return nn.BCEWithLogitsLoss()\n",
        "        return nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        return nn.MSELoss()\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score, mean_absolute_error\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_metrics(model, loader, task, out_dim):\n",
        "    model.eval()\n",
        "    ys, ps, loss_sum, n = [], [], 0.0, 0\n",
        "    crit = make_criterion(task, out_dim)\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        if task==\"cls\":\n",
        "            if USE_BCE_FOR_BINARY and out_dim==1:\n",
        "                loss = crit(out.squeeze(1), yb.float())\n",
        "                prob = torch.sigmoid(out.squeeze(1)).cpu().numpy(); y = yb.cpu().numpy()\n",
        "            else:\n",
        "                loss = crit(out, yb)\n",
        "                prob = torch.softmax(out, dim=1)[:,1].cpu().numpy(); y = yb.cpu().numpy()\n",
        "            ys.append(y); ps.append(prob)\n",
        "        else:\n",
        "            loss = crit(out.squeeze(-1), yb)\n",
        "            ys.append(yb.cpu().numpy()); ps.append(out.squeeze(-1).cpu().numpy())\n",
        "        loss_sum += float(loss.item()) * xb.size(0); n += xb.size(0)\n",
        "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
        "    avg_loss = loss_sum / max(n,1)\n",
        "    if task==\"cls\":\n",
        "        auroc = roc_auc_score(y, p) if (len(np.unique(y))>1) else np.nan\n",
        "        ap    = average_precision_score(y, p) if (len(np.unique(y))>1) else np.nan\n",
        "        pred  = (p>=0.5).astype(int)\n",
        "        acc   = accuracy_score(y, pred); f1 = f1_score(y, pred)\n",
        "        return avg_loss, {\"auroc\":auroc, \"prauc\":ap, \"acc\":acc, \"f1\":f1}\n",
        "    else:\n",
        "        mae = mean_absolute_error(y, p)\n",
        "        return avg_loss, {\"mae\":mae}\n",
        "\n",
        "# ---------- (5) Training loop (counts grad evals) ----------\n",
        "def run_epoch(model, loader, optimizer=None, task=\"cls\", out_dim=2, l2_lambda=0.0, label_smooth=0.0, input_noise_std=0.0):\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "    crit = make_criterion(task, out_dim)\n",
        "    total_loss, n, grad_evals = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        if input_noise_std>0 and task==\"reg\" and is_train:\n",
        "            xb = xb + input_noise_std*torch.randn_like(xb)\n",
        "        if is_train: optimizer.zero_grad(set_to_none=True)\n",
        "        out = model(xb)\n",
        "        if task==\"cls\" and (label_smooth>0) and (not USE_BCE_FOR_BINARY or out_dim!=1):\n",
        "            ncls = out.size(1)\n",
        "            y_one = torch.zeros((yb.size(0), ncls), device=out.device).scatter_(1, yb.view(-1,1), 1.0)\n",
        "            y_s = (1.0 - label_smooth)*y_one + label_smooth/(ncls-1)*(1.0 - y_one)\n",
        "            loss = -(y_s * torch.log_softmax(out, dim=1)).sum(dim=1).mean()\n",
        "        else:\n",
        "            loss = (nn.BCEWithLogitsLoss()(out.squeeze(1), yb.float()) if (task==\"cls\" and USE_BCE_FOR_BINARY and out_dim==1)\n",
        "                    else crit(out if task==\"cls\" else out.squeeze(-1), yb))\n",
        "        if l2_lambda>0:\n",
        "            l2 = sum((p**2).sum() for p in model.parameters() if p.requires_grad)\n",
        "            loss = loss + l2_lambda*l2\n",
        "        if is_train:\n",
        "            loss.backward(); optimizer.step(); grad_evals += 1\n",
        "        total_loss += float(loss.item())*xb.size(0); n += xb.size(0)\n",
        "    return total_loss/max(n,1), grad_evals\n",
        "\n",
        "# ---------- (6) Optimizer ablations (identical budgets & threshold ℓ) ----------\n",
        "class AdamNoBias(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9,0.999), eps=1e-8, weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None: loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            lr = group[\"lr\"]; beta1, beta2 = group[\"betas\"]; eps = group[\"eps\"]; wd = group[\"weight_decay\"]\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                g = p.grad\n",
        "                st = self.state[p]\n",
        "                if len(st)==0:\n",
        "                    st[\"m\"] = torch.zeros_like(p); st[\"v\"]=torch.zeros_like(p)\n",
        "                m, v = st[\"m\"], st[\"v\"]\n",
        "                m.mul_(beta1).add_(g, alpha=1.0-beta1)\n",
        "                v.mul_(beta2).addcmul_(g, g, value=1.0-beta2)\n",
        "                step = m/(v.sqrt()+eps)  # no bias correction\n",
        "                if wd!=0.0: step = step + wd*p\n",
        "                p.add_(step, alpha=-lr)\n",
        "        return loss\n",
        "\n",
        "def make_optimizer(params, kind, lr, beta1=0.9, beta2=0.999, wd=0.0):\n",
        "    filt = [p for p in params if p.requires_grad]\n",
        "    if kind==\"sgd\":         return torch.optim.SGD(filt, lr=lr, momentum=0.0, nesterov=False)\n",
        "    if kind==\"momentum\":    return torch.optim.SGD(filt, lr=lr, momentum=0.9, nesterov=False)\n",
        "    if kind==\"nesterov\":    return torch.optim.SGD(filt, lr=lr, momentum=0.9, nesterov=True)\n",
        "    if kind==\"adam\":        return torch.optim.Adam(filt, lr=lr, betas=(beta1,beta2))\n",
        "    if kind==\"adam_nobias\": return AdamNoBias(filt, lr=lr, betas=(beta1,beta2))\n",
        "    if kind==\"adam_beta1zero\": return torch.optim.Adam(filt, lr=lr, betas=(0.0,beta2))\n",
        "    if kind==\"adamw\":       return torch.optim.AdamW(filt, lr=lr, betas=(beta1,beta2), weight_decay=wd)\n",
        "    raise ValueError(kind)\n",
        "\n",
        "def train_to_budget(model, train_loader, val_loader, task, out_dim,\n",
        "                    max_updates=200, L_threshold=None, tag=\"run\", opt_kind=\"adam\",\n",
        "                    lr=1e-3, beta1=0.9, beta2=0.999, wd=0.0, label_smooth=0.0, input_noise_std=0.0):\n",
        "    import copy, time as _time\n",
        "    m = copy.deepcopy(model).to(DEVICE)\n",
        "    for p in m.parameters(): p.requires_grad = True  # Part 2 trains full net\n",
        "    opt = make_optimizer(m.parameters(), opt_kind, lr, beta1, beta2, wd)\n",
        "    grad_total, best_val, t0, reached = 0, float(\"inf\"), _time.time(), None\n",
        "    hist=[]\n",
        "    while grad_total < max_updates:\n",
        "        trL, ge = run_epoch(m, train_loader, opt, task, out_dim, l2_lambda=0.0,\n",
        "                            label_smooth=label_smooth, input_noise_std=input_noise_std)\n",
        "        grad_total += ge\n",
        "        vaL, extra = eval_metrics(m, val_loader, task, out_dim)\n",
        "        best_val = min(best_val, vaL)\n",
        "        if (L_threshold is not None) and (reached is None) and (vaL <= L_threshold):\n",
        "            reached = {\"grad_evals\":grad_total, \"time_sec\": _time.time()-t0}\n",
        "        hist.append({\"grad_evals\":grad_total,\"val_loss\":vaL, **extra})\n",
        "    df = pd.DataFrame(hist); logf = os.path.join(LOG_DIR, f\"{tag}.csv\"); df.to_csv(logf, index=False)\n",
        "    return {\"best_val\":best_val, \"reached_L\":reached, \"log\":logf}\n",
        "\n",
        "L_HOTEL = 0.20\n",
        "L_ACC   = 500.0\n",
        "\n",
        "def part2_ablation(model, loaders, task, out_dim, base_lr, base_beta1=0.9, base_beta2=0.999, variants=None):\n",
        "    tr, va, _ = loaders\n",
        "    variants = variants or OPT_VARIANTS\n",
        "    results=[]\n",
        "    for name, kw in variants:\n",
        "        tag = f\"opt_{name}_{task}\"\n",
        "        Lth = L_HOTEL if task==\"cls\" else L_ACC\n",
        "        # ---- profile this optimizer run ----\n",
        "        (r, dt, peak) = profile_call(f\"Part2::{task}::{name}\",\n",
        "                                     train_to_budget, model, tr, va, task, out_dim,\n",
        "                                     BUDGET_UPDATES, L_threshold=Lth, tag=tag,\n",
        "                                     opt_kind=name, lr=base_lr, beta1=base_beta1, beta2=base_beta2, **kw)\n",
        "        r[\"variant\"]=name; r[\"time_sec\"]=dt; r[\"peak_mb\"]=peak\n",
        "        results.append(r)\n",
        "        print(f\"[OL/Part2] {name}: best_val={r['best_val']:.4f} reached_L={r['reached_L']} time={dt:.2f}s peakMB={peak}\")\n",
        "    # save summary with time & memory\n",
        "    pd.DataFrame([{k:v for k,v in r.items() if k!='log'} for r in results]).to_csv(\n",
        "        os.path.join(LOG_DIR, f\"ablation_summary_{task}.csv\"), index=False)\n",
        "    plot_loss_vs_compute(os.path.join(LOG_DIR, \"opt_adam_cls.csv\"),\n",
        "                     \"Hotel: Adam loss vs compute\",\n",
        "                     \"loss_vs_compute_hotel_adam.png\")\n",
        "    return results\n",
        "\n",
        "def sensitivity_heatmap(model, loaders, task, out_dim, alphas=(1e-4,3e-4,1e-3), betas=(0.5,0.9,0.99), which=\"b1\"):\n",
        "    if FAST_MODE:\n",
        "        print(\"[OL/Part2] Heatmaps disabled in FAST_MODE\");\n",
        "        return pd.DataFrame()\n",
        "    tr, va, _ = loaders\n",
        "    rec=[]\n",
        "    for a in alphas:\n",
        "        for b in betas:\n",
        "            b1, b2 = (b, 0.999) if which==\"b1\" else (0.9, b)\n",
        "            (r, _, _) = profile_call(f\"Part2::heatmap::{task}::{which}::a{a}_b{b}\",\n",
        "                                     train_to_budget, model, tr, va, task, out_dim,\n",
        "                                     max_updates=BUDGET_UPDATES//2, L_threshold=None,\n",
        "                                     tag=f\"heat_{task}_{which}_a{a}_b{b}\",\n",
        "                                     opt_kind=\"adam\", lr=a, beta1=b1, beta2=b2)\n",
        "            rec.append({\"alpha\":a,\"beta\":b,\"best\":r[\"best_val\"]})\n",
        "    df = pd.DataFrame(rec); piv = df.pivot(index=\"beta\", columns=\"alpha\", values=\"best\")\n",
        "    plt.figure(); plt.imshow(piv.values, aspect=\"auto\")\n",
        "    plt.xticks(range(len(piv.columns)), piv.columns); plt.yticks(range(len(piv.index)), piv.index)\n",
        "    plt.xlabel(\"alpha (lr)\"); plt.ylabel(\"beta\" + (\"1\" if which==\"b1\" else \"2\"))\n",
        "    plt.colorbar(label=\"validation loss\"); plt.title(f\"{task.upper()} Adam sensitivity ({which})\")\n",
        "    fn = os.path.join(FIG_DIR, f\"heatmap_{task}_{which}.png\"); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
        "    print(\"[OL/Part2] Saved heatmap ->\", fn)\n",
        "    return df\n",
        "\n",
        "def stability_band(model, loaders, task, out_dim, seeds=(0,1,2), opt_kind=\"adam\", lr=1e-3):\n",
        "    if FAST_MODE:\n",
        "        print(\"[OL/Part2] Stability bands disabled in FAST_MODE\");\n",
        "        return pd.DataFrame()\n",
        "    import copy\n",
        "    tr, va, _ = loaders\n",
        "    traj=[]\n",
        "    for sd in seeds:\n",
        "        set_seed(sd)\n",
        "        m = copy.deepcopy(model).to(DEVICE)\n",
        "        (r, dt, peak) = profile_call(f\"Part2::stability::{task}::{opt_kind}::seed{sd}\",\n",
        "                                     train_to_budget, m, tr, va, task, out_dim,\n",
        "                                     BUDGET_UPDATES, L_threshold=None,\n",
        "                                     tag=f\"stab_{task}_{opt_kind}_seed{sd}\", opt_kind=opt_kind, lr=lr)\n",
        "        df = pd.read_csv(r[\"log\"]); df[\"seed\"]=sd; traj.append(df)\n",
        "        print(f\"[OL/Part2] seed {sd} time={dt:.2f}s peakMB={peak}\")\n",
        "    D = pd.concat(traj, axis=0)\n",
        "    g = D.groupby(\"grad_evals\")[\"val_loss\"]\n",
        "    xs = g.median().index.values; med=g.median().values; q25=g.quantile(0.25).values; q75=g.quantile(0.75).values\n",
        "    plt.figure(); plt.plot(xs, med, label=\"median\"); plt.fill_between(xs, q25, q75, alpha=0.3, label=\"IQR\")\n",
        "    plt.xlabel(\"# grad evals\"); plt.ylabel(\"validation loss\"); plt.title(f\"{task.upper()} {opt_kind} stability\")\n",
        "    plt.legend(); fn = os.path.join(FIG_DIR, f\"stability_{task}_{opt_kind}.png\"); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
        "    print(\"[OL/Part2] Saved stability band ->\", fn)\n",
        "    return D\n",
        "\n",
        "# ---------- (7) RO hygiene (Part 1) ----------\n",
        "def last_linear(model):\n",
        "    Ls = linear_layers(model)\n",
        "    if not Ls: raise RuntimeError(\"No nn.Linear layers found.\")\n",
        "    return Ls[-1]\n",
        "\n",
        "def pack_last(last_layer):\n",
        "    W = last_layer.weight.detach().cpu().numpy().copy()\n",
        "    b = last_layer.bias.detach().cpu().numpy().copy()\n",
        "    return np.concatenate([W.ravel(), b.ravel()]), W.shape\n",
        "\n",
        "def unpack_to_last(vec, last_layer, Wshape):\n",
        "    out, in_ = Wshape\n",
        "    W = vec[:out*in_].reshape(out, in_)\n",
        "    b = vec[out*in_:]\n",
        "    last_layer.weight.data = torch.from_numpy(W).to(last_layer.weight.device).type_as(last_layer.weight)\n",
        "    last_layer.bias.data   = torch.from_numpy(b).to(last_layer.bias.device).type_as(last_layer.bias)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validation_objective(model, val_loader, task, out_dim):\n",
        "    model.eval()\n",
        "    vL, _ = eval_metrics(model, val_loader, task, out_dim)\n",
        "    return float(vL)  # one full val pass = one function evaluation\n",
        "\n",
        "def RHC(f, x0, step=0.05, restarts=5, budget=2000):\n",
        "    best_x, best_f = x0.copy(), f(x0); evals=1; hist=[best_f]\n",
        "    for r in range(restarts+1):\n",
        "        x = x0.copy() if r==0 else x0 + 0.1*np.random.randn(*x0.shape)\n",
        "        fx = f(x); evals += 1\n",
        "        improved=True\n",
        "        while evals<budget and improved:\n",
        "            improved=False\n",
        "            for _ in range(20):\n",
        "                c = x + step*np.random.randn(*x.shape); fc = f(c); evals += 1\n",
        "                if fc < fx: x, fx = c, fc; improved=True; hist.append(fx)\n",
        "                if evals>=budget: break\n",
        "        if fx < best_f: best_x, best_f = x, fx\n",
        "    return best_x, best_f, np.array(hist)\n",
        "\n",
        "def SA(f, x0, T0=1.0, cooling=0.997, step=0.05, budget=4000):\n",
        "    x, fx = x0.copy(), f(x0); evals=1; hist=[fx]; T=T0\n",
        "    while evals<budget:\n",
        "        c = x + step*np.random.randn(*x.shape); fc = f(c); evals += 1\n",
        "        if fc < fx or np.random.rand() < np.exp((fx-fc)/max(T,1e-8)): x, fx = c, fc\n",
        "        hist.append(fx); T *= cooling\n",
        "    return x, fx, np.array(hist)\n",
        "\n",
        "def GA(f, x0, pop=40, elites=2, cx=0.7, mut=0.2, step=0.05, budget=8000):\n",
        "    dim = x0.size; evals=0\n",
        "    P = x0 + 0.1*np.random.randn(pop, dim)\n",
        "    fit = np.array([f(ind) for ind in P]); evals += pop; best_hist=[fit.min()]\n",
        "    while evals<budget:\n",
        "        idx = np.argsort(fit); P = P[idx]; fit = fit[idx]\n",
        "        new = [P[i] for i in range(elites)]\n",
        "        while len(new) < pop:\n",
        "            if np.random.rand()<cx:\n",
        "                a,b = P[np.random.randint(0,pop//2,2)]; cut = np.random.randint(0, dim)\n",
        "                child = np.concatenate([a[:cut], b[cut:]])\n",
        "            else: child = P[np.random.randint(0,pop)]\n",
        "            if np.random.rand()<mut: child = child + step*np.random.randn(dim)\n",
        "            new.append(child)\n",
        "        P = np.array(new)\n",
        "        fit = np.array([f(ind) for ind in P]); evals += pop; best_hist.append(fit.min())\n",
        "    best = P[fit.argmin()]\n",
        "    return best, fit.min(), np.array(best_hist)\n",
        "\n",
        "def run_part1_RO(model, loaders, task, out_dim, base_tag=\"dataset\", k_unfrozen=1, ro_cfg=None):\n",
        "    import copy\n",
        "    ro_cfg = ro_cfg or RO_CFG\n",
        "    m = copy.deepcopy(model).to(DEVICE)\n",
        "    tparams = freeze_all_but_last_k(m, k=k_unfrozen)\n",
        "    assert tparams <= 50_000, f\"RO trainable params {tparams} exceed ~50k cap\"\n",
        "    m.eval()\n",
        "\n",
        "    last = last_linear(m)\n",
        "    x0, Wshape = pack_last(last)\n",
        "\n",
        "    def f(vec):\n",
        "        with torch.no_grad():\n",
        "            unpack_to_last(vec.astype(np.float32), last, Wshape)\n",
        "            return validation_objective(m, loaders[1], task, out_dim)\n",
        "\n",
        "    # Profile each RO algo\n",
        "    (ret_rhc, t_rhc, mb_rhc) = profile_call(f\"Part1::{base_tag}::RHC\",\n",
        "                                            RHC, f, x0, 0.05, (3 if FAST_MODE else 5), ro_cfg[\"RHC\"])\n",
        "    xr, fr, hr = ret_rhc\n",
        "    (ret_sa,  t_sa,  mb_sa ) = profile_call(f\"Part1::{base_tag}::SA\",\n",
        "                                            SA, f, x0, 1.0, 0.997, 0.05, ro_cfg[\"SA\"])\n",
        "    xs, fs, hs = ret_sa\n",
        "    (ret_ga,  t_ga,  mb_ga ) = profile_call(f\"Part1::{base_tag}::GA\",\n",
        "                                            GA, f, x0, ro_cfg[\"GA_pop\"], 2, 0.7, 0.2, 0.05, ro_cfg[\"GA\"])\n",
        "    xg, fg, hg = ret_ga\n",
        "\n",
        "    def summarize(hist, algo, Lth):\n",
        "        best = float(np.min(hist))\n",
        "        # first eval reaching threshold (if any)\n",
        "        cummin = np.minimum.accumulate(hist)\n",
        "        idx = int(np.where(cummin <= Lth)[0][0]) if np.any(cummin <= Lth) else None\n",
        "        return {\"algo\": algo, \"best_val\": best, \"reached_L\": idx is not None, \"evals_to_L\": (idx if idx is not None else None)}\n",
        "\n",
        "    Lth = L_HOTEL if task==\"cls\" else L_ACC\n",
        "    summary = pd.DataFrame([\n",
        "        summarize(hr, \"RHC\", Lth),\n",
        "        summarize(hs, \"SA\",  Lth),\n",
        "        summarize(hg, \"GA\",  Lth),\n",
        "    ])\n",
        "    summary_path = os.path.join(LOG_DIR, f\"ro_summary_{base_tag}.csv\")\n",
        "    summary.to_csv(summary_path, index=False)\n",
        "    print(\"[OL/Part1] Saved RO summary ->\", summary_path)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(len(hr)), np.minimum.accumulate(hr), label=f\"RHC ({t_rhc:.1f}s)\")\n",
        "    plt.plot(np.arange(len(hs)), np.minimum.accumulate(hs), label=f\"SA ({t_sa:.1f}s)\")\n",
        "    plt.plot(np.arange(len(hg)), np.minimum.accumulate(hg), label=f\"GA ({t_ga:.1f}s)\")\n",
        "    plt.xlabel(\"function evaluations\")\n",
        "    plt.ylabel(\"best-so-far validation loss\")\n",
        "    plt.title(f\"RO progress ({base_tag})\"); plt.legend(); plt.tight_layout()\n",
        "    fn = os.path.join(FIG_DIR, f\"ro_progress_{base_tag}.png\"); plt.savefig(fn, dpi=160); plt.close()\n",
        "    print(\"[OL/Part1] Saved RO curves ->\", fn)\n",
        "\n",
        "# ---------- (8) Regularization study (Adam only) ----------\n",
        "def part3_regularization(model, loaders, task, out_dim, adam_lr, adam_beta1=0.9, adam_beta2=0.999):\n",
        "    import copy\n",
        "    tr, va, _ = loaders\n",
        "    def train_cfg(tag, l2=0.0, es_patience=None, drop_p=None, label_smooth=0.0, input_noise=0.0):\n",
        "        m = copy.deepcopy(model).to(DEVICE)\n",
        "        if drop_p is not None:\n",
        "            Ls = [m for m in m.modules() if isinstance(m, nn.Linear)]\n",
        "            in_dim = Ls[0].in_features; outs=[l.out_features for l in Ls[:-1]]\n",
        "            out_dim_loc = Ls[-1].out_features\n",
        "            new_m = MLP(in_dim, hidden=tuple(outs), out_dim=out_dim_loc, dropout_p=drop_p).to(DEVICE)\n",
        "            new_m.load_state_dict(m.state_dict(), strict=False)\n",
        "            m = new_m\n",
        "        opt = make_optimizer(m.parameters(), \"adam\", adam_lr, adam_beta1, adam_beta2, wd=0.0)\n",
        "\n",
        "        best=float(\"inf\"); ge_total=0; hist=[]; patience_left=es_patience\n",
        "        while ge_total < BUDGET_UPDATES:\n",
        "            trL, ge = run_epoch(m, tr, opt, task, out_dim, l2_lambda=l2, label_smooth=label_smooth, input_noise_std=input_noise)\n",
        "            ge_total += ge\n",
        "            vaL, extra = eval_metrics(m, va, task, out_dim)\n",
        "            best = min(best, vaL); hist.append({\"grad_evals\":ge_total,\"val_loss\":vaL, **extra})\n",
        "            if es_patience is not None:\n",
        "                if len(hist)>1 and hist[-1][\"val_loss\"] > min(h[\"val_loss\"] for h in hist[:-1]):\n",
        "                    patience_left -= 1\n",
        "                    if patience_left <= 0: break\n",
        "        df = pd.DataFrame(hist); logf=os.path.join(LOG_DIR,f\"{tag}.csv\"); df.to_csv(logf, index=False)\n",
        "        return best\n",
        "\n",
        "    # run & profile each config\n",
        "    (baseline, t_base, mb_base) = profile_call(f\"Part3::{task}::baseline\",\n",
        "                                               train_cfg, \"reg_baseline_\"+task)\n",
        "    single = []\n",
        "    for wd in [1e-5, 1e-4, 5e-4]:\n",
        "        (bv, t, mb) = profile_call(f\"Part3::{task}::l2::{wd}\", train_cfg, f\"reg_l2_{wd}_{task}\", l2=wd)\n",
        "        single.append((\"l2\", wd, bv, t, mb))\n",
        "    (bv, t, mb) = profile_call(f\"Part3::{task}::earlystop::3\", train_cfg, f\"reg_es3_{task}\", es_patience=3)\n",
        "    single.append((\"earlystop\", 3, bv, t, mb))\n",
        "    for p in ([0.1, 0.3] if task==\"cls\" else [0.05, 0.2]):\n",
        "        (bv, t, mb) = profile_call(f\"Part3::{task}::dropout::{p}\", train_cfg, f\"reg_dropout_{p}_{task}\", drop_p=p)\n",
        "        single.append((\"dropout\", p, bv, t, mb))\n",
        "    if task==\"cls\":\n",
        "        for eps in [0.05, 0.10]:\n",
        "            (bv, t, mb) = profile_call(f\"Part3::{task}::label_smooth::{eps}\", train_cfg, f\"reg_ls_{eps}_{task}\", label_smooth=eps)\n",
        "            single.append((\"label_smooth\", eps, bv, t, mb))\n",
        "    else:\n",
        "        for s in [0.01, 0.05]:\n",
        "            (bv, t, mb) = profile_call(f\"Part3::{task}::input_noise::{s}\", train_cfg, f\"reg_inoise_{s}_{task}\", input_noise=s)\n",
        "            single.append((\"input_noise\", s, bv, t, mb))\n",
        "\n",
        "    sdf = pd.DataFrame(single, columns=[\"reg\",\"value\",\"best_val\",\"time_sec\",\"peak_mb\"]).sort_values(\"best_val\")\n",
        "    best_reg = sdf.iloc[0].to_dict()\n",
        "    combo_kwargs = {}\n",
        "    if best_reg[\"reg\"]==\"l2\": combo_kwargs[\"l2\"]=float(best_reg[\"value\"])\n",
        "    if best_reg[\"reg\"]==\"earlystop\": combo_kwargs[\"es_patience\"]=int(best_reg[\"value\"])\n",
        "    if best_reg[\"reg\"]==\"dropout\": combo_kwargs[\"drop_p\"]=float(best_reg[\"value\"])\n",
        "    if best_reg[\"reg\"]==\"label_smooth\": combo_kwargs[\"label_smooth\"]=float(best_reg[\"value\"])\n",
        "    if best_reg[\"reg\"]==\"input_noise\": combo_kwargs[\"input_noise\"]=float(best_reg[\"value\"])\n",
        "    if task==\"cls\" and \"label_smooth\" not in combo_kwargs: combo_kwargs[\"label_smooth\"]=0.05\n",
        "    if task==\"reg\" and \"input_noise\" not in combo_kwargs: combo_kwargs[\"input_noise\"]=0.01\n",
        "    (combo, t_combo, mb_combo) = profile_call(f\"Part3::{task}::combo\", train_cfg, \"reg_combo_\"+task, **combo_kwargs)\n",
        "\n",
        "    plt.figure()\n",
        "    names = [\"Baseline\",\"Best Single\",\"Best Combo\"]\n",
        "    vals  = [baseline, best_reg[\"best_val\"], combo]\n",
        "    plt.bar(names, vals); plt.ylabel(\"validation loss\"); plt.title(f\"{task.upper()} — regularization\")\n",
        "    fn = os.path.join(FIG_DIR, f\"regularization_{task}.png\"); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
        "    print(\"[OL/Part3] Saved regularization bar plot ->\", fn)\n",
        "    sdf.to_csv(os.path.join(LOG_DIR, f\"reg_single_{task}.csv\"), index=False)\n",
        "    # also log combo timing\n",
        "    with open(os.path.join(LOG_DIR, f\"reg_combo_{task}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"best_single\":best_reg, \"combo_val\":combo, \"combo_time_sec\":t_combo, \"combo_peak_mb\":mb_combo}, f, indent=2)\n",
        "    return sdf, {\"baseline\":baseline, \"best_single\":best_reg, \"combo\":combo}\n",
        "\n",
        "# ---------- (9) Reporting & accounting helpers ----------\n",
        "def plot_loss_vs_compute(csv_log, title, outfile):\n",
        "    if not os.path.exists(csv_log):\n",
        "        print(\"[OL] Missing log:\", csv_log); return\n",
        "    df = pd.read_csv(csv_log)\n",
        "    if \"grad_evals\" not in df or \"val_loss\" not in df:\n",
        "        print(\"[OL] Missing required columns in\", csv_log); return\n",
        "    plt.figure(); plt.plot(df[\"grad_evals\"], df[\"val_loss\"])\n",
        "    plt.xlabel(\"# gradient evaluations\"); plt.ylabel(\"validation loss\"); plt.title(title)\n",
        "    fn=os.path.join(FIG_DIR, outfile); plt.tight_layout(); plt.savefig(fn, dpi=160); plt.close()\n",
        "    print(\"[OL] Saved ->\", fn)\n",
        "\n",
        "# ============================================================\n",
        "# ============ PART 1 / PART 2 / PART 3 EXECUTION ============\n",
        "# ============================================================\n",
        "\n",
        "# PART 1 — Randomized Optimization (RHC/SA/GA)\n",
        "if 'H_loaders' in globals() and 'm_cls' in globals():\n",
        "    run_part1_RO(m_cls, H_loaders, task=\"cls\", out_dim=(1 if USE_BCE_FOR_BINARY else 2),\n",
        "                 base_tag=\"hotel\", k_unfrozen=1, ro_cfg=RO_CFG)\n",
        "if RUN_ACCIDENTS and 'A_loaders' in globals() and 'm_reg' in globals():\n",
        "    run_part1_RO(m_reg, A_loaders, task=\"reg\", out_dim=1,\n",
        "                 base_tag=\"accidents\", k_unfrozen=1, ro_cfg=RO_CFG)\n",
        "\n",
        "# PART 2 — Optimizer ablations (equal budgets; same splits)\n",
        "if 'H_loaders' in globals() and 'm_cls' in globals():\n",
        "    _ab_h = part2_ablation(m_cls, H_loaders, task=\"cls\", out_dim=(1 if USE_BCE_FOR_BINARY else 2), base_lr=1e-3)\n",
        "    if DO_HEATMAPS:\n",
        "        _ = sensitivity_heatmap(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2))\n",
        "        _ = sensitivity_heatmap(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2), which=\"b2\")\n",
        "    if DO_STABILITY:\n",
        "        _ = stability_band(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2), seeds=(0,1,2), opt_kind=\"adam\", lr=1e-3)\n",
        "\n",
        "if RUN_ACCIDENTS and 'A_loaders' in globals() and 'm_reg' in globals():\n",
        "    _ab_a = part2_ablation(m_reg, A_loaders, task=\"reg\", out_dim=1, base_lr=3e-4)\n",
        "    if DO_HEATMAPS:\n",
        "        _ = sensitivity_heatmap(m_reg, A_loaders, \"reg\", 1)\n",
        "        _ = sensitivity_heatmap(m_reg, A_loaders, \"reg\", 1, which=\"b2\")\n",
        "    if DO_STABILITY:\n",
        "        _ = stability_band(m_reg, A_loaders, \"reg\", 1, seeds=(0,1,2), opt_kind=\"adam\", lr=3e-4)\n",
        "\n",
        "# PART 3 — Regularization study (Adam only; reuse Part-2 Adam LR/betas)\n",
        "if 'H_loaders' in globals() and 'm_cls' in globals():\n",
        "    _s_h, _bars_h = part3_regularization(m_cls, H_loaders, \"cls\", (1 if USE_BCE_FOR_BINARY else 2), adam_lr=1e-3)\n",
        "if RUN_ACCIDENTS and 'A_loaders' in globals() and 'm_reg' in globals():\n",
        "    _s_a, _bars_a = part3_regularization(m_reg, A_loaders, \"reg\", 1, adam_lr=3e-4)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}