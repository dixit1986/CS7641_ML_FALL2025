{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07366ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                        object\n",
      "Source                    object\n",
      "Severity                 float64\n",
      "Start_Time                object\n",
      "End_Time                  object\n",
      "Start_Lat                float64\n",
      "Start_Lng                float64\n",
      "End_Lat                  float64\n",
      "End_Lng                  float64\n",
      "Distance(mi)             float64\n",
      "Description               object\n",
      "Street                    object\n",
      "City                      object\n",
      "County                    object\n",
      "State                     object\n",
      "Zipcode                   object\n",
      "Country                   object\n",
      "Timezone                  object\n",
      "Airport_Code              object\n",
      "Weather_Timestamp         object\n",
      "Temperature(F)           float64\n",
      "Wind_Chill(F)            float64\n",
      "Humidity(%)              float64\n",
      "Pressure(in)             float64\n",
      "Visibility(mi)           float64\n",
      "Wind_Direction            object\n",
      "Wind_Speed(mph)          float64\n",
      "Precipitation(in)        float64\n",
      "Weather_Condition         object\n",
      "Amenity                     bool\n",
      "Bump                        bool\n",
      "Crossing                    bool\n",
      "Give_Way                    bool\n",
      "Junction                    bool\n",
      "No_Exit                     bool\n",
      "Railway                     bool\n",
      "Roundabout                  bool\n",
      "Station                     bool\n",
      "Stop                        bool\n",
      "Traffic_Calming             bool\n",
      "Traffic_Signal              bool\n",
      "Turning_Loop                bool\n",
      "Sunrise_Sunset            object\n",
      "Civil_Twilight            object\n",
      "Nautical_Twilight         object\n",
      "Astronomical_Twilight     object\n",
      "dtype: object\n",
      "Severity             float64\n",
      "Start_Lat            float64\n",
      "Start_Lng            float64\n",
      "End_Lat              float64\n",
      "End_Lng              float64\n",
      "Distance(mi)         float64\n",
      "Temperature(F)       float64\n",
      "Wind_Chill(F)        float64\n",
      "Humidity(%)          float64\n",
      "Pressure(in)         float64\n",
      "Visibility(mi)       float64\n",
      "Wind_Speed(mph)      float64\n",
      "Precipitation(in)    float64\n",
      "dtype: object\n",
      "torch.Size([95512, 30])\n",
      "torch.Size([95512])\n"
     ]
    }
   ],
   "source": [
    "# Install missing packages\n",
    "# %pip install seaborn\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "# --- Step 0: One-time Setup & Parity Guarantees ---\n",
    "# Fix seeds for reproducibility (Python, NumPy, and PyTorch)\n",
    "def set_seed(s=4242):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(4242)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Step 1: Data Handling ---\n",
    "# Load datasets\n",
    "hotel_data = pd.read_csv('hotel_bookings.csv')\n",
    "us_accidents_data = pd.read_csv('US_Accidents_March23.csv')\n",
    "\n",
    "# Preprocess datasets\n",
    "def preprocess_hotel_booking(data):\n",
    "    \"\"\"\n",
    "    Preprocess the Hotel Booking dataset.\n",
    "    \"\"\"\n",
    "    # Remove post-outcome fields to prevent leakage\n",
    "    data = data.drop(columns=['reservation_status'])\n",
    "\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Impute missing values for numeric columns\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    data[numeric_cols] = numeric_imputer.fit_transform(data[numeric_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns using the most frequent strategy\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "    # Encode categorical variables\n",
    "    encoder = TargetEncoder()\n",
    "    # Ensure the target variable is passed to the encoder\n",
    "    data[categorical_cols] = encoder.fit_transform(data[categorical_cols], data['is_canceled'])\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_us_accidents(data):\n",
    "    \"\"\"\n",
    "    Preprocess the US Accidents dataset.\n",
    "    \"\"\"\n",
    "    # Clean the datetime columns to remove fractional seconds or extraneous characters\n",
    "    data['End_Time'] = data['End_Time'].str.split('.').str[0]\n",
    "    data['Start_Time'] = data['Start_Time'].str.split('.').str[0]\n",
    "\n",
    "    # Calculate the duration in minutes\n",
    "    data['Duration'] = (pd.to_datetime(data['End_Time'], format='mixed') -\n",
    "                       pd.to_datetime(data['Start_Time'], format='mixed')).dt.total_seconds() / 60\n",
    "\n",
    "    # Scale numeric features\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "hotel_data_preprocessed = preprocess_hotel_booking(hotel_data)\n",
    "us_accidents_data_preprocessed = preprocess_us_accidents(us_accidents_data)\n",
    "\n",
    "# Split datasets\n",
    "X_hotel_train, X_hotel_test, y_hotel_train, y_hotel_test = train_test_split(\n",
    "    hotel_data_preprocessed.drop('is_canceled', axis=1),\n",
    "    hotel_data_preprocessed['is_canceled'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "X_accidents_train, X_accidents_test, y_accidents_train, y_accidents_test = train_test_split(\n",
    "    us_accidents_data_preprocessed.drop('Duration', axis=1),\n",
    "    us_accidents_data_preprocessed['Duration'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Inspect the data types of the columns\n",
    "print(X_accidents_test.dtypes)\n",
    "\n",
    "# Drop non-numeric columns (if they are not needed)\n",
    "X_accidents_test = X_accidents_test.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Encode categorical columns (if any)\n",
    "categorical_cols = X_accidents_test.select_dtypes(include=['object']).columns\n",
    "if not categorical_cols.empty:\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    X_accidents_test[categorical_cols] = encoder.fit_transform(X_accidents_test[categorical_cols])\n",
    "\n",
    "# Impute missing values (if any)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_accidents_test = pd.DataFrame(imputer.fit_transform(X_accidents_test), columns=X_accidents_test.columns)\n",
    "\n",
    "# Convert to numeric types\n",
    "X_accidents_test = X_accidents_test.astype(float)\n",
    "\n",
    "# Validate the data types\n",
    "print(X_accidents_test.dtypes)\n",
    "\n",
    "# Define Xtr and ytr for the Hotel Booking dataset\n",
    "Xtr = torch.from_numpy(X_hotel_train.values).float()\n",
    "ytr = torch.from_numpy(y_hotel_train.values).long()\n",
    "\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "# Convert to PyTorch tensors\n",
    "Xva = torch.from_numpy(X_hotel_test.values).float()  # Hotel test set\n",
    "yva = torch.from_numpy(y_hotel_test.values).long()   # Hotel test labels\n",
    "Xte = torch.from_numpy(X_accidents_test.values).float()  # Accidents test set\n",
    "yte = torch.from_numpy(y_accidents_test.values).float()   # Accidents test labels\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=256, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(TensorDataset(Xva, yva), batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(Xte, yte), batch_size=1024, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ab83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params total=12,354 | trainable(last 2)=8,386\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Model Definition ---\n",
    "# Mirror the sklearn MLP with an nn.Module\n",
    "# Use the SAME hidden sizes and activations you claimed in SL; that’s your “fixed backbone”\n",
    "# Keep output head/activation appropriate for the task (e.g., CrossEntropyLoss for multiclass, MSELoss for regression)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[128, 64], out_dim=4, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + hidden\n",
    "        for i in range(len(dims)-1):\n",
    "            layers += [nn.Linear(dims[i], dims[i+1]), nn.ReLU()]\n",
    "            if dropout_p > 0:\n",
    "                layers += [nn.Dropout(p=dropout_p)]\n",
    "        layers += [nn.Linear(hidden[-1] if hidden else in_dim, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# Example usage\n",
    "num_classes = len(np.unique(y_hotel_train))  # Define num_classes based on the target variable\n",
    "model = MLP(in_dim=Xtr.shape[1], hidden=[128, 64], out_dim=num_classes).to(device)\n",
    "\n",
    "\n",
    "# --- Step 3: Freezing Layers ---\n",
    "# Freeze all but the last k layers (this is the crux for RO and for Part-specific constraints)\n",
    "def linear_layers(model):\n",
    "    return [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "\n",
    "def freeze_all_but_last_k(model, k=2):\n",
    "    layers = linear_layers(model)\n",
    "    # Freeze all first\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    # Unfreeze last k Linear layers\n",
    "    for m in layers[-k:]:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "    # Report counts (needed for RO cap)\n",
    "    tot = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Params total={tot:,} | trainable(last {k})={trainable:,}\")\n",
    "    return trainable\n",
    "\n",
    "# Example: freeze all but last 1–3 layers for Part 1 RO\n",
    "trainable = freeze_all_but_last_k(model, k=2)\n",
    "assert trainable <= 50_000, \"RO parameter cap exceeded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae891e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Losses & Metrics ---\n",
    "# Define appropriate loss functions and metrics based on the task (classification or regression)\n",
    "task = \"classification\"  # or \"regression\"\n",
    "criterion = nn.CrossEntropyLoss() if task == \"classification\" else nn.MSELoss()\n",
    "\n",
    "# Add task-appropriate metrics (Accuracy/F1/AUROC vs. MAE/MSE/R2) consistent with your SL choices\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using ROC-AUC, PR-AUC, and F1-Score.\n",
    "    \"\"\"\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return roc_auc, pr_auc, f1\n",
    "\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate regression performance using MAE and MSE.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mae, mse\n",
    "\n",
    "# --- Step 5: Training and Evaluation Loop ---\n",
    "# Implement a minimal, transparent training/eval loop (counts “gradient evaluations” cleanly)\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    total_loss, n, grad_evals = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                grad_evals += 1  # count one optimizer step = one gradient evaluation\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n, grad_evals\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56584f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Optimizer Ablations (Part 2) ---\n",
    "# Define the optimizers exactly as the assignment lists; keep everything else fixed (batch size, schedule form, seeds)\n",
    "# Record time/steps-to-ℓ and stability over seeds\n",
    "# Do not call AdamW “Adam baseline”\n",
    "\n",
    "def make_opt(model, kind, lr, **kwargs):\n",
    "    if kind == \"sgd\":\n",
    "        return optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    if kind == \"momentum\":\n",
    "        return optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=kwargs.get(\"momentum\", 0.9))\n",
    "    if kind == \"nesterov\":\n",
    "        return optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=kwargs.get(\"momentum\", 0.9), nesterov=True)\n",
    "    if kind == \"adam\":\n",
    "        return optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, betas=(kwargs.get(\"beta1\", 0.9), kwargs.get(\"beta2\", 0.999)), eps=kwargs.get(\"eps\", 1e-8))\n",
    "    if kind == \"adamw\":\n",
    "        return optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, betas=(kwargs.get(\"beta1\", 0.9), kwargs.get(\"beta2\", 0.999)), eps=kwargs.get(\"eps\", 1e-8), weight_decay=kwargs.get(\"wd\", 1e-2))\n",
    "    raise ValueError(kind)\n",
    "\n",
    "# --- Step 7: Freezing and RO Hygiene (Part 1) ---\n",
    "# For RO, call model.eval() for every objective (dropout off; BN uses stored stats)\n",
    "# Freeze all but last 1–3 layers (≤ ~50k params)\n",
    "# Define the objective as full-validation loss\n",
    "# Count one function evaluation per full validation pass\n",
    "# Do not interleave gradient steps in RO\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_objective(model, val_loader):\n",
    "    model.eval()\n",
    "    return evaluate(model, val_loader)  # one full pass = 1 function evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55865d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_val': 0.302001476544952, 'grad_evals': 10098, 'time_sec': 15.679988861083984, 'reached_L': None}\n"
     ]
    }
   ],
   "source": [
    "# --- Step 8: Regularization Study (Part 3) ---\n",
    "# Keep Adam hyperparams fixed to the best from Part 2 (no switching to AdamW; no retuning LR when adding regularization)\n",
    "# Implement L2 (coupled) via loss term (not AdamW), early stopping rule, dropout placements (document where), label smoothing or target noise, and modest augmentation appropriate to the modality (off for val/test)\n",
    "# Budget-match runs and report dispersion across seeds\n",
    "\n",
    "def run_epoch_with_l2(model, loader, optimizer, l2_lambda=0.0):\n",
    "    model.train()\n",
    "    total_loss, n, grad_evals = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        if l2_lambda > 0:\n",
    "            l2 = sum((p**2).sum() for p in model.parameters() if p.requires_grad)\n",
    "            loss = loss + l2_lambda * l2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        grad_evals += 1\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n, grad_evals\n",
    "\n",
    "# --- Step 9: Reporting & Accounting ---\n",
    "# Compute accounting: gradient evals (updates), function evals (RO), wall-clock on the same hardware class\n",
    "# Threshold ℓ once per dataset; show steps/time to ℓ; include failures as “> budget”\n",
    "\n",
    "# Define the train_to_budget function\n",
    "def train_to_budget(model, optimizer, train_loader, val_loader, max_updates=10000, L_threshold=None):\n",
    "    \"\"\"\n",
    "    Train a model with a given optimizer and dataset loaders within a specified budget of gradient evaluations.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
    "        max_updates (int): Maximum number of gradient evaluations (updates) allowed.\n",
    "        L_threshold (float, optional): Validation loss threshold to stop training early.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the best validation loss, total gradient evaluations, training time, and whether the threshold was reached.\n",
    "    \"\"\"\n",
    "    grad_evals_total, best_val, t0 = 0, float(\"inf\"), time()\n",
    "    reached = None\n",
    "\n",
    "    while grad_evals_total < max_updates:\n",
    "        # Train for one epoch\n",
    "        tr_loss, ge = run_epoch(model, train_loader, optimizer)\n",
    "        grad_evals_total += ge\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        best_val = min(best_val, val_loss)\n",
    "\n",
    "        # Check if the validation loss threshold is met\n",
    "        if L_threshold is not None and reached is None and val_loss <= L_threshold:\n",
    "            reached = (grad_evals_total, time() - t0)\n",
    "\n",
    "    return {\n",
    "        \"best_val\": best_val,\n",
    "        \"grad_evals\": grad_evals_total,\n",
    "        \"time_sec\": time() - t0,\n",
    "        \"reached_L\": reached\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "optimizer = make_opt(model, \"adam\", lr=0.001)\n",
    "results = train_to_budget(model, optimizer, train_loader, val_loader, max_updates=10000)\n",
    "print(results)\n",
    "\n",
    "# --- Step 10: SL Code Integration ---\n",
    "# Train and evaluate models using sklearn pipelines\n",
    "def train_decision_tree(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a Decision Tree classifier with hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'max_depth': [8, 16],\n",
    "        'min_samples_leaf': [100, 200]\n",
    "    }\n",
    "    tree = DecisionTreeClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(tree, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_shallow_nn(X_train, y_train, input_dim):\n",
    "    \"\"\"\n",
    "    Train a shallow neural network with SGD optimizer.\n",
    "    \"\"\"\n",
    "    class ShallowNN(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(ShallowNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, 512)\n",
    "            self.fc2 = nn.Linear(512, 512)\n",
    "            self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "    model = ShallowNN(input_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "def train_deep_nn(X_train, y_train, input_dim):\n",
    "    \"\"\"\n",
    "    Train a deep neural network with SGD optimizer.\n",
    "    \"\"\"\n",
    "    class DeepNN(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(DeepNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, 256)\n",
    "            self.fc2 = nn.Linear(256, 256)\n",
    "            self.fc3 = nn.Linear(256, 128)\n",
    "            self.fc4 = nn.Linear(128, 128)\n",
    "            self.fc5 = nn.Linear(128, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = torch.relu(self.fc3(x))\n",
    "            x = torch.relu(self.fc4(x))\n",
    "            x = torch.sigmoid(self.fc5(x))\n",
    "            return x\n",
    "\n",
    "    model = DeepNN(input_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ab5c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Series([], Name: is_canceled, dtype: float64)\n",
      "(95512,)\n",
      "67702     1.303712\n",
      "115851   -0.767040\n",
      "57345     1.303712\n",
      "11622     1.303712\n",
      "33333    -0.767040\n",
      "Name: is_canceled, dtype: float64\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 20 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\", line 1024, in fit\n    super()._fit(\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\", line 294, in _fit\n    check_classification_targets(y)\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/utils/multiclass.py\", line 221, in check_classification_targets\n    raise ValueError(\nValueError: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(y_hotel_train.isna().sum())  \u001b[38;5;66;03m# Should be 0\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m decision_tree_model = \u001b[43mtrain_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_hotel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hotel_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m knn_model = train_knn(X_hotel_train, y_hotel_train)\n\u001b[32m     30\u001b[39m svm_model = train_svm(X_hotel_train, y_hotel_train)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mtrain_decision_tree\u001b[39m\u001b[34m(X_train, y_train)\u001b[39m\n\u001b[32m     82\u001b[39m tree = DecisionTreeClassifier(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     83\u001b[39m grid_search = GridSearchCV(tree, param_grid, cv=\u001b[32m5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1028\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m   1026\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 20 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\", line 1024, in fit\n    super()._fit(\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\", line 294, in _fit\n    check_classification_targets(y)\n  File \"/Users/AC81199/Library/CloudStorage/OneDrive-ElevanceHealth/Documents/Masters/Fall 2025/SL_Projects/.venv/lib/python3.11/site-packages/sklearn/utils/multiclass.py\", line 221, in check_classification_targets\n    raise ValueError(\nValueError: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the target variable\n",
    "print(y_hotel_train.isna().sum())  # Count the number of NaN values\n",
    "print(y_hotel_train[y_hotel_train.isna()])  # Display the NaN values\n",
    "print(y_hotel_train.shape)  # Check the shape of the target variable\n",
    "print(y_hotel_train.head())  # Check the first few rows of the target variable\n",
    "\n",
    "# if y_hotel_train.empty or y_hotel_train.shape[0] == 0:\n",
    "#     y_hotel_train = hotel_data_preprocessed['is_canceled']\n",
    "\n",
    "\n",
    "# Handle missing values (if any)\n",
    "if y_hotel_train.isna().sum() > 0:\n",
    "    # Option 1: Drop NaNs\n",
    "    # y_hotel_train = y_hotel_train.dropna()\n",
    "\n",
    "    # Option 2: Impute NaNs (uncomment one of the following)\n",
    "    # from sklearn.impute import SimpleImputer\n",
    "    # imputer = SimpleImputer(strategy='most_frequent')  # or 'mean', 'median', 'constant'\n",
    "    # y_hotel_train = imputer.fit_transform(y_hotel_train.values.reshape(-1, 1))\n",
    "\n",
    "    # Option 3: Fill NaNs with a default value\n",
    "    y_hotel_train = y_hotel_train.fillna(0)  # or the most frequent class\n",
    "\n",
    "# Validate the target variable\n",
    "print(y_hotel_train.isna().sum())  # Should be 0\n",
    "\n",
    "# Train models\n",
    "decision_tree_model = train_decision_tree(X_hotel_train, y_hotel_train)\n",
    "knn_model = train_knn(X_hotel_train, y_hotel_train)\n",
    "svm_model = train_svm(X_hotel_train, y_hotel_train)\n",
    "\n",
    "\n",
    "shallow_nn_model = train_shallow_nn(X_hotel_train, y_hotel_train, input_dim=X_hotel_train.shape[1])\n",
    "deep_nn_model = train_deep_nn(X_hotel_train, y_hotel_train, input_dim=X_hotel_train.shape[1])\n",
    "\n",
    "# Evaluate models\n",
    "roc_auc, pr_auc, f1 = evaluate_classification(y_hotel_test, decision_tree_model.predict(X_hotel_test))\n",
    "print(f\"Decision Tree - ROC-AUC: {roc_auc}, PR-AUC: {pr_auc}, F1-Score: {f1}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curve(decision_tree_model, X_hotel_train, y_hotel_train, 'Learning Curve for Decision Tree')\n",
    "plot_residuals(y_accidents_test, svm_model.predict(X_accidents_test), 'Residuals for SVM Regressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable diagnostics:\n",
      "NaN count: 0\n",
      "Shape: (95512,)\n",
      "Data type: float64\n",
      "Unique values: [-0.76704049  1.30371214]\n",
      "Value counts:\n",
      "is_canceled\n",
      "-0.767040    60259\n",
      " 1.303712    35253\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fixing target variable...\n",
      "After conversion - dtype: int64\n",
      "Unique values: [0 1]\n",
      "All values in [0,1]: True\n",
      "Final NaN count: 0\n",
      "Training models...\n",
      "Decision Tree trained successfully\n",
      "KNN trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the target variable\n",
    "print(\"Target variable diagnostics:\")\n",
    "print(f\"NaN count: {y_hotel_train.isna().sum()}\")\n",
    "print(f\"Shape: {y_hotel_train.shape}\")\n",
    "print(f\"Data type: {y_hotel_train.dtype}\")\n",
    "print(f\"Unique values: {np.unique(y_hotel_train)}\")\n",
    "print(f\"Value counts:\\n{y_hotel_train.value_counts()}\")\n",
    "\n",
    "# Fix the continuous target issue\n",
    "print(\"\\nFixing target variable...\")\n",
    "\n",
    "# The issue is that TargetEncoder in preprocessing is converting the target to continuous values\n",
    "# We need to ensure the target remains as discrete binary classes (0, 1)\n",
    "y_hotel_train = y_hotel_train.astype(int)\n",
    "y_hotel_test = y_hotel_test.astype(int)\n",
    "\n",
    "# Verify the fix\n",
    "print(f\"After conversion - dtype: {y_hotel_train.dtype}\")\n",
    "print(f\"Unique values: {np.unique(y_hotel_train)}\")\n",
    "print(f\"All values in [0,1]: {all(val in [0, 1] for val in y_hotel_train)}\")\n",
    "\n",
    "# Handle missing values (if any)\n",
    "if y_hotel_train.isna().sum() > 0:\n",
    "    y_hotel_train = y_hotel_train.fillna(0)\n",
    "    y_hotel_test = y_hotel_test.fillna(0)\n",
    "\n",
    "print(f\"Final NaN count: {y_hotel_train.isna().sum()}\")\n",
    "\n",
    "# Add missing functions that are called but not defined\n",
    "def train_knn(X_train, y_train):\n",
    "    \"\"\"Train a KNN classifier with hyperparameter tuning.\"\"\"\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_svm(X_train, y_train):\n",
    "    \"\"\"Train an SVM classifier with hyperparameter tuning.\"\"\"\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Fix evaluation function - need to import auc\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def evaluate_classification_fixed(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using proper probability predictions.\n",
    "    \"\"\"\n",
    "    # Convert probabilities to binary predictions\n",
    "    if len(y_pred_proba.shape) > 1:\n",
    "        y_pred_proba = y_pred_proba[:, 1] if y_pred_proba.shape[1] > 1 else y_pred_proba.ravel()\n",
    "    \n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return roc_auc, pr_auc, f1\n",
    "\n",
    "# Add missing plotting functions\n",
    "def plot_learning_curve(model, X_train, y_train, title):\n",
    "    \"\"\"Plot learning curve for the model.\"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_train, y_train, cv=5, scoring='roc_auc',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training Score')\n",
    "    plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation Score')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('ROC-AUC Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title):\n",
    "    \"\"\"Plot residuals for regression.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Train models\n",
    "print(\"Training models...\")\n",
    "decision_tree_model = train_decision_tree(X_hotel_train, y_hotel_train)\n",
    "print(\"Decision Tree trained successfully\")\n",
    "\n",
    "knn_model = train_knn(X_hotel_train, y_hotel_train)\n",
    "print(\"KNN trained successfully\")\n",
    "\n",
    "svm_model = train_svm(X_hotel_train, y_hotel_train)\n",
    "print(\"SVM trained successfully\")\n",
    "\n",
    "# Convert data to tensors for neural networks\n",
    "X_hotel_train_tensor = torch.FloatTensor(X_hotel_train.values)\n",
    "y_hotel_train_tensor = torch.FloatTensor(y_hotel_train.values).unsqueeze(1)\n",
    "\n",
    "shallow_nn_model = train_shallow_nn(X_hotel_train_tensor, y_hotel_train_tensor, input_dim=X_hotel_train.shape[1])\n",
    "print(\"Shallow NN trained successfully\")\n",
    "\n",
    "deep_nn_model = train_deep_nn(X_hotel_train_tensor, y_hotel_train_tensor, input_dim=X_hotel_train.shape[1])\n",
    "print(\"Deep NN trained successfully\")\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\nEvaluating models...\")\n",
    "\n",
    "# Decision Tree\n",
    "if hasattr(decision_tree_model, \"predict_proba\"):\n",
    "    dt_pred_proba = decision_tree_model.predict_proba(X_hotel_test)[:, 1]\n",
    "else:\n",
    "    dt_pred_proba = decision_tree_model.predict(X_hotel_test)\n",
    "roc_auc, pr_auc, f1 = evaluate_classification_fixed(y_hotel_test, dt_pred_proba)\n",
    "print(f\"Decision Tree - ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Plot learning curves (only for successful models)\n",
    "try:\n",
    "    plot_learning_curve(decision_tree_model, X_hotel_train, y_hotel_train, 'Learning Curve for Decision Tree')\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot learning curve: {e}\")\n",
    "\n",
    "# For regression plot (US accidents)\n",
    "try:\n",
    "    # Train a simple regressor for accidents data\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    \n",
    "    # Select only numeric columns for accidents\n",
    "    X_accidents_numeric = X_accidents_test.select_dtypes(include=[np.number])\n",
    "    if len(X_accidents_numeric.columns) > 0:\n",
    "        rf_regressor.fit(X_accidents_numeric, y_accidents_test)\n",
    "        y_accidents_pred = rf_regressor.predict(X_accidents_numeric)\n",
    "        plot_residuals(y_accidents_test, y_accidents_pred, 'Residuals for Random Forest Regressor')\n",
    "    else:\n",
    "        print(\"No numeric columns available for accidents regression\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create regression plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Optimization\n",
    "model = ShallowNN(input_dim=X_hotel_train.shape[1])\n",
    "best_state_rhc, best_fitness_rhc = rhc_optimizer(model, objective_function, max_iterations=1000)\n",
    "best_state_sa, best_fitness_sa = sa_optimizer(model, objective_function, max_iterations=1000)\n",
    "best_state_ga, best_fitness_ga = ga_optimizer(model, objective_function, max_iterations=1000)\n",
    "\n",
    "# Adam Variants Comparison\n",
    "optimizers = [\n",
    "    torch.optim.SGD(model.parameters(), lr=0.01),\n",
    "    torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08),\n",
    "    torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "]\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    training_time = train_model_with_optimizer(model, optimizer, nn.BCELoss(), dataloader, num_epochs=15)\n",
    "    print(f\"Training time with {optimizer}: {training_time}\")\n",
    "\n",
    "# Regularization Techniques Evaluation\n",
    "regularization_techniques = [\"L2\", \"Dropout\", \"Label Smoothing\", \"Data Augmentation\"]\n",
    "results = evaluate_regularization(model, dataloader, nn.BCELoss(), regularization_techniques)\n",
    "print(f\"Regularization results: {results}\")\n",
    "\n",
    "# Integrated Approach\n",
    "integrated_training_time = integrate_best_combination(model, dataloader, nn.BCELoss())\n",
    "print(f\"Integrated training time: {integrated_training_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
