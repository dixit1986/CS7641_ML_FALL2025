{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS7641 Supervised Learning — Hotel (classification) + US Accidents (regression)\n",
    "v3.1 — Aligns with Fall 2025 v6 dataset rules; no external encoders; RBF-γ grid; Hotel NN epoch curve; Linear SVM accuracy.\n",
    "\n",
    "Major points:\n",
    " - Encoders: SafeTargetEncoder (smoothed target mean) + SafeFreqEncoder (frequency), sklearn-compatible (no extra deps)\n",
    " - Global: StandardScaler, float32 features, int labels, fixed seeds, runtime & peak RAM logging, hardware note\n",
    " - CV: StratifiedShuffleSplit (classification) / ShuffleSplit (regression) with fixed n_splits\n",
    " - Hotel (~100k): DT, kNN (exact/brute/euclidean), LinearSVM(+calibration), RBF-SVM (γ ∈ {scale, 1/d, 2/d}), NN (SGD-only; ≤15 epochs; param-cap)\n",
    " - Accidents (~7M): DT & Linear SVR ≥1M rows; RBF SVR ≤100k; kNN ≤250k train/≤25k test; NN within param-cap\n",
    " - Plots: Learning curves (all), Validation (complexity) curves, ROC/PR, calibration & CM (Hotel),\n",
    "          residuals plot (Accidents), DT importances, SVM diagnostics (margin histogram & SV fraction),\n",
    "          NN epoch curve (Hotel+Accidents) & width-sweep within param cap\n",
    " - Extra credit: NN activation study (Hotel). Skips gracefully if torch not installed.\n",
    "\"\"\"\n",
    "\n",
    "import os, time, math, platform, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "FIG_DIR = \"sl_outputs/figs\"; LOG_DIR = \"sl_outputs/logs\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True); os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE); random.seed(RANDOM_STATE)\n",
    "\n",
    "# --------- RAM/time helpers ---------\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None\n",
    "import resource\n",
    "\n",
    "def now_mb():\n",
    "    try:\n",
    "        if psutil: return psutil.Process().memory_info().rss / (1024**2)\n",
    "        return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.0\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def hw_note():\n",
    "    cpu = platform.processor() or \"Unknown CPU\"\n",
    "    sys = f\"{platform.system()} {platform.release()}\"\n",
    "    print(f\"[Hardware] {sys}; CPU: {cpu}; Python {platform.python_version()}\")\n",
    "\n",
    "def time_fit_predict(model, Xtr, ytr, Xte, yte, fit_fn=\"fit\", predict_fn=\"predict\"):\n",
    "    start_ram = now_mb(); t0 = time.time()\n",
    "    getattr(model, fit_fn)(Xtr, ytr)\n",
    "    t1 = time.time()\n",
    "    yhat = getattr(model, predict_fn)(Xte)\n",
    "    t2 = time.time()\n",
    "    peak_ram = max(now_mb(), start_ram)\n",
    "    return (t1 - t0), (t2 - t1), peak_ram, yhat\n",
    "\n",
    "# --------- Plot helpers ---------\n",
    "def savefig(prefix, title):\n",
    "    fname = f\"{prefix}_{title.replace(' ', '_').replace('/', '-')}.png\"\n",
    "    path = os.path.join(FIG_DIR, fname)\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=160); plt.close()\n",
    "    print(f\"[saved] {path}\")\n",
    "\n",
    "def protocol_card(name, X, y=None, scoring=None, cv=None):\n",
    "    if y is not None and set(np.unique(y)) <= {0,1}:\n",
    "        pos = float(np.mean(y))\n",
    "        print(f\"[{name}] n={len(X)} scoring={scoring} cv={cv} prevalence={pos:.3f}\")\n",
    "    else:\n",
    "        print(f\"[{name}] n={len(X)} scoring={scoring} cv={cv}\")\n",
    "\n",
    "# --------- sklearn imports ---------\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, learning_curve, validation_curve,\n",
    "    StratifiedShuffleSplit, ShuffleSplit, GridSearchCV\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, auc, f1_score, confusion_matrix,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay, CalibrationDisplay, accuracy_score,\n",
    "    mean_absolute_error, mean_squared_error, median_absolute_error\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --------- OHE helper (version-robust) ---------\n",
    "def make_ohe():\n",
    "    try:\n",
    "        # sklearn ≥1.2\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32)\n",
    "    except TypeError:\n",
    "        # older versions\n",
    "        return OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=np.float32)\n",
    "\n",
    "# ===============================\n",
    "# Safe encoders (no ext deps)\n",
    "# ===============================\n",
    "class SafeTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Smoothed target mean encoding, sklearn-compatible.\n",
    "    Handles binary classification (y in {0,1}) and regression (float).\n",
    "    \"\"\"\n",
    "    def __init__(self, cols=None, smoothing=20.0):\n",
    "        self.cols = cols\n",
    "        self.smoothing = float(smoothing)\n",
    "        self.maps_ = {}\n",
    "        self.global_ = None\n",
    "        self._cols_in_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"SafeTargetEncoder requires target y\")\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        if self.cols is None:\n",
    "            self._cols_in_ = X.columns.tolist()\n",
    "        else:\n",
    "            self._cols_in_ = list(self.cols)\n",
    "        y = pd.Series(y)\n",
    "        self.maps_.clear()\n",
    "        self.global_ = float(np.nanmean(y))\n",
    "        for c in self._cols_in_:\n",
    "            vc = X[c].astype(\"category\")\n",
    "            df = pd.DataFrame({\"cat\": vc, \"y\": y})\n",
    "            grp = df.groupby(\"cat\", observed=True)[\"y\"].agg([\"mean\",\"count\"])\n",
    "            m = self.smoothing\n",
    "            enc = (grp[\"count\"] * grp[\"mean\"] + m * self.global_) / (grp[\"count\"] + m)\n",
    "            self.maps_[c] = enc.to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        out = []\n",
    "        for c in self._cols_in_:\n",
    "            mp = self.maps_[c]\n",
    "            v = X[c].map(mp).astype(np.float32)\n",
    "            v = v.fillna(np.float32(self.global_))\n",
    "            out.append(v.values.reshape(-1,1))\n",
    "        return np.hstack(out).astype(np.float32)\n",
    "\n",
    "class SafeFreqEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Normalized frequency (count) encoding, sklearn-compatible.\"\"\"\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "        self.maps_ = {}\n",
    "        self._cols_in_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        if self.cols is None:\n",
    "            self._cols_in_ = X.columns.tolist()\n",
    "        else:\n",
    "            self._cols_in_ = list(self.cols)\n",
    "        self.maps_.clear()\n",
    "        for c in self._cols_in_:\n",
    "            vc = X[c].value_counts(dropna=False, normalize=True)\n",
    "            self.maps_[c] = vc.to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        out = []\n",
    "        for c in self._cols_in_:\n",
    "            mp = self.maps_[c]\n",
    "            v = X[c].map(mp).astype(np.float32)\n",
    "            v = v.fillna(np.float32(0.0))\n",
    "            out.append(v.values.reshape(-1,1))\n",
    "        return np.hstack(out).astype(np.float32)\n",
    "\n",
    "# --------- Metrics helpers ---------\n",
    "def pr_auc(y_true, scores):\n",
    "    p, r, _ = precision_recall_curve(y_true, scores)\n",
    "    return auc(r, p)\n",
    "\n",
    "def f1_opt_threshold(y_true, scores):\n",
    "    p, r, t = precision_recall_curve(y_true, scores)\n",
    "    f1 = np.where((p+r) > 0, 2*p*r/(p+r), 0)\n",
    "    j = int(np.argmax(f1))\n",
    "    thr = t[j-1] if 0 < j < len(t) else 0.5\n",
    "    return float(thr), float(f1[j])\n",
    "\n",
    "# --------- Curves ---------\n",
    "def plot_learning(est, title, X, y, scoring, cv, prefix=\"Hotel\"):\n",
    "    protocol_card(title, X, y, scoring, cv)\n",
    "    sizes = np.linspace(0.1, 1.0, 5)\n",
    "    tr_sizes, tr_s, te_s = learning_curve(\n",
    "        est, X, y, train_sizes=sizes, cv=cv, scoring=scoring,\n",
    "        n_jobs=-1, shuffle=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.title(f\"{prefix} — Learning Curve: {title}\")\n",
    "    plt.xlabel(\"Training examples\"); plt.ylabel(scoring.upper()); plt.grid(alpha=.3)\n",
    "    plt.plot(tr_sizes, tr_s.mean(1), 'o-', label=\"Training\")\n",
    "    plt.plot(tr_sizes, te_s.mean(1), 'o-', label=\"Cross-val\"); plt.legend()\n",
    "    savefig(prefix, f\"LC_{title}\")\n",
    "\n",
    "def plot_validation(est, X, y, pname, prange, scoring, title, cv, prefix=\"Hotel\"):\n",
    "    protocol_card(title, X, y, scoring, cv)\n",
    "    tr_s, te_s = validation_curve(est, X, y, param_name=pname, param_range=prange, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.title(f\"{prefix} — Validation Curve: {title}\")\n",
    "    plt.xlabel(pname); plt.ylabel(scoring.upper()); plt.grid(alpha=.3)\n",
    "    plt.plot(prange, tr_s.mean(1), 'o-', label=\"Training\")\n",
    "    plt.plot(prange, te_s.mean(1), 'o-', label=\"Cross-val\"); plt.legend()\n",
    "    savefig(prefix, f\"MC_{title}\")\n",
    "\n",
    "# =========================\n",
    "# Data loaders & preprocess\n",
    "# =========================\n",
    "def load_hotel():\n",
    "    df = pd.read_csv(\"hotel_bookings.csv\")\n",
    "    # leakage controls\n",
    "    drop_leak = [c for c in ['reservation_status','reservation_status_date'] if c in df.columns]\n",
    "    df = df.drop(columns=drop_leak)\n",
    "    for c in ['agent','company','children']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(0).astype(int)\n",
    "    if 'country' in df.columns:\n",
    "        df['country'] = df['country'].fillna(df['country'].mode()[0])\n",
    "    y = df['is_canceled'].astype(np.int64)\n",
    "    X = df.drop(columns=['is_canceled'])\n",
    "    print(f\"[Hotel] rows raw={len(df)}  target=is_canceled\")\n",
    "    return X, y\n",
    "\n",
    "def hotel_preprocessor(X):\n",
    "    num = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    te_cols = [c for c in ['country','agent','company'] if c in X.columns]  # high-card\n",
    "    cat = [c for c in X.select_dtypes(exclude=np.number).columns if c not in te_cols]\n",
    "    pre = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num),\n",
    "        ('ohe', make_ohe(), cat),\n",
    "        ('tenc', SafeTargetEncoder(cols=te_cols, smoothing=20.0), te_cols)\n",
    "    ], remainder='drop')\n",
    "    to32 = ('to32', FunctionTransformer(lambda Z: Z.astype(np.float32)))\n",
    "    return Pipeline([('pre', pre), to32])\n",
    "\n",
    "def load_accidents(path=\"US_Accidents_March23.csv\"):\n",
    "    df = pd.read_csv(path)\n",
    "    df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "    df['End_Time']   = pd.to_datetime(df['End_Time'],   errors='coerce')\n",
    "    df['Duration'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60.0\n",
    "    df = df.drop(columns=['Start_Time','End_Time'])\n",
    "    df = df[df['Duration'] > 0]\n",
    "    if 'Description' in df.columns: df = df.drop(columns=['Description'])\n",
    "    for c in df.select_dtypes(include='object').columns: df[c] = df[c].fillna('Unknown')\n",
    "    for c in df.select_dtypes(include='bool').columns:   df[c] = df[c].astype(int)\n",
    "    df = df.dropna()\n",
    "    for c in df.select_dtypes(include=np.number).columns: df[c] = df[c].astype(np.float32)\n",
    "    y = df['Duration'].astype(np.float32)\n",
    "    X = df.drop(columns=['Duration','Severity']) if 'Severity' in df.columns else df.drop(columns=['Duration'])\n",
    "    print(f\"[Accidents] rows raw={len(df)}  target=Duration(min)\")\n",
    "    return X, y\n",
    "\n",
    "def accidents_preprocessor(X, high_card_threshold=20):\n",
    "    cat_all = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    num = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    low_card = [c for c in cat_all if X[c].nunique(dropna=False) <= high_card_threshold]\n",
    "    high_card = [c for c in cat_all if c not in low_card]\n",
    "    pre = ColumnTransformer([\n",
    "        ('num', StandardScaler(), num),\n",
    "        ('ohe', make_ohe(), low_card),\n",
    "        ('freq', SafeFreqEncoder(cols=high_card), high_card)  # no wide OHE for high-card\n",
    "    ], remainder='drop')\n",
    "    to32 = ('to32', FunctionTransformer(lambda Z: Z.astype(np.float32)))\n",
    "    return Pipeline([('pre', pre), to32])\n",
    "\n",
    "# =========================\n",
    "# NN helpers\n",
    "# =========================\n",
    "def mlp_params_count(input_dim, hidden, out_dim):\n",
    "    sizes = [input_dim] + list(hidden) + [out_dim]\n",
    "    params = 0\n",
    "    for i in range(len(sizes)-1):\n",
    "        params += sizes[i]*sizes[i+1] + sizes[i+1]\n",
    "    return params\n",
    "\n",
    "def choose_widths(input_dim, target_dim, shallow=True, cap_low=2e5, cap_high=1e6):\n",
    "    candidates = [128, 256, 512]\n",
    "    for w in candidates[::-1]:\n",
    "        hidden = (w, w) if shallow else (w//2, w//2, max(64,w//4), max(64,w//4))\n",
    "        pcount = mlp_params_count(input_dim, hidden, target_dim)\n",
    "        if cap_low <= pcount <= cap_high:\n",
    "            return list(hidden)\n",
    "    return [128, 128] if shallow else [128, 128, 64, 64]\n",
    "\n",
    "def nn_width_sweep(prefix, X, y, pre, task=\"cls\", widths=(128,256,512), cv=None):\n",
    "    scores = []\n",
    "    for w in widths:\n",
    "        hidden = (w, w)\n",
    "        pcount = mlp_params_count(pre.transform(X[:5]).shape[1], hidden, 1 if task==\"reg\" else 2)\n",
    "        if not (2e5 <= pcount <= 1e6):\n",
    "            print(f\"[NN width sweep] skip w={w} (params≈{int(pcount):,})\")\n",
    "            continue\n",
    "        if task==\"cls\":\n",
    "            est = Pipeline([('prep', pre), ('post', StandardScaler(with_mean=False)),\n",
    "                            ('clf', MLPClassifier(hidden_layer_sizes=hidden, solver='sgd', momentum=0.0,\n",
    "                                                  batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                                                  alpha=1e-4, early_stopping=True, n_iter_no_change=3, max_iter=15,\n",
    "                                                  random_state=RANDOM_STATE))])\n",
    "            sc = 'roc_auc'\n",
    "        else:\n",
    "            est = Pipeline([('prep', pre), ('post', StandardScaler(with_mean=False)),\n",
    "                            ('reg', MLPRegressor(hidden_layer_sizes=hidden, solver='sgd', momentum=0.0,\n",
    "                                                 batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                                                 alpha=1e-4, early_stopping=True, n_iter_no_change=3, max_iter=15,\n",
    "                                                 random_state=RANDOM_STATE))])\n",
    "            sc = 'neg_mean_absolute_error'\n",
    "        tr, te = learning_curve(est, X, y, cv=cv, scoring=sc, train_sizes=np.linspace(0.25,1.0,4),\n",
    "                                n_jobs=-1, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scores.append((w, te.mean(axis=1)[-1]))\n",
    "    if scores:\n",
    "        ws, s = zip(*scores)\n",
    "        plt.figure(figsize=(6.2,4.6))\n",
    "        plt.plot(ws, s, 'o-'); plt.grid(alpha=.3)\n",
    "        plt.xlabel(\"width (per layer)\"); plt.ylabel((\"ROC-AUC\" if task==\"cls\" else \"−MAE\"))\n",
    "        plt.title(f\"{prefix} — NN Model-Complexity (width sweep; param-cap enforced)\")\n",
    "        savefig(prefix, \"NN_Width_Sweep\")\n",
    "\n",
    "# =========================\n",
    "# HOTEL — Classification\n",
    "# =========================\n",
    "def run_hotel():\n",
    "    print(\"\\n== HOTEL (CLASSIFICATION: is_canceled) ==\")\n",
    "    X, y = load_hotel()\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    pre = hotel_preprocessor(Xtr)\n",
    "    cv_cls = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    post = ('post', StandardScaler(with_mean=False))\n",
    "\n",
    "    dt_clf  = Pipeline([('prep', pre), ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "    knn_clf = Pipeline([('prep', pre), post, ('clf', KNeighborsClassifier(n_neighbors=11, algorithm=\"brute\", metric=\"euclidean\", n_jobs=-1))])\n",
    "    linSVC  = Pipeline([('prep', pre), post, ('clf', LinearSVC(C=1.0, class_weight='balanced', max_iter=15000, random_state=RANDOM_STATE))])\n",
    "    rbfSVC  = Pipeline([('prep', pre), post, ('clf', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "\n",
    "    # Fit pre to inspect input dim for NN sizing & gamma grid\n",
    "    pre.fit(Xtr, ytr)\n",
    "    input_dim = pre.transform(Xtr[:5]).shape[1]\n",
    "    d_hotel = input_dim\n",
    "    gamma_grid_hotel = [\"scale\", 1.0/d_hotel, 2.0/d_hotel]\n",
    "\n",
    "    shallow = choose_widths(input_dim, 2, shallow=True)\n",
    "    nn_sgd = Pipeline([\n",
    "        ('prep', pre), post,\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                              batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                              alpha=1e-4, early_stopping=True, n_iter_no_change=3,\n",
    "                              max_iter=15, shuffle=True, momentum=0.0, nesterovs_momentum=False,\n",
    "                              random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "    # Validation curves\n",
    "    plot_validation(Pipeline([('prep', pre), ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=RANDOM_STATE))]),\n",
    "                    Xtr, ytr, \"clf__max_depth\", [6,10,14,18], \"roc_auc\", \"DT vs depth\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', KNeighborsClassifier(algorithm=\"brute\", metric=\"euclidean\"))]),\n",
    "                    Xtr, ytr, \"clf__n_neighbors\", [3,5,11,21], \"roc_auc\", \"kNN vs k\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', LinearSVC(class_weight='balanced', max_iter=15000, random_state=RANDOM_STATE))]),\n",
    "                    Xtr, ytr, \"clf__C\", np.logspace(-2,2,5), \"roc_auc\", \"LinearSVM vs C\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    # Note: we vary C on the curve; γ choices included in GS below.\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE))]),\n",
    "                    Xtr, ytr, \"clf__C\", [0.5,2,8], \"roc_auc\", \"RBF-SVM vs C (γ∈{scale,1/d,2/d})\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('clf', MLPClassifier(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                                                                        momentum=0.0, batch_size=1024, max_iter=15,\n",
    "                                                                        random_state=RANDOM_STATE))]),\n",
    "                    Xtr, ytr, \"clf__alpha\", [1e-4,5e-4,1e-3], \"roc_auc\", \"NN vs L2(alpha)\", cv=cv_cls, prefix=\"Hotel\")\n",
    "\n",
    "    # Learning curves\n",
    "    plot_learning(dt_clf,   \"Decision Tree\",      Xtr, ytr, \"roc_auc\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_learning(knn_clf,  \"kNN\",                Xtr, ytr, \"roc_auc\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_learning(linSVC,   \"Linear SVM\",         Xtr, ytr, \"roc_auc\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_learning(rbfSVC,   \"RBF SVM\",            Xtr, ytr, \"roc_auc\", cv=cv_cls, prefix=\"Hotel\")\n",
    "    plot_learning(nn_sgd,   \"Neural Net (SGD)\",   Xtr, ytr, \"roc_auc\", cv=cv_cls, prefix=\"Hotel\")\n",
    "\n",
    "    # Tuning (coarse grids per spec)\n",
    "    dt_grid  = {\"clf__max_depth\":[6,10,14,18],\n",
    "                \"clf__min_samples_leaf\":[50,100,200],\n",
    "                \"clf__min_samples_split\":[100,200,400],\n",
    "                \"clf__max_features\":[\"sqrt\",\"log2\",0.5],\n",
    "                \"clf__ccp_alpha\":[0.0,1e-4,5e-4,1e-3]}\n",
    "    dt_tuned  = GridSearchCV(dt_clf,  dt_grid, cv=cv_cls, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr, ytr).best_estimator_\n",
    "    knn_tuned = GridSearchCV(knn_clf, {\"clf__n_neighbors\":[5,11,21]}, cv=cv_cls, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr, ytr).best_estimator_\n",
    "    lin_tuned = GridSearchCV(linSVC,   {\"clf__C\":[0.1,1,10]}, cv=cv_cls, scoring=\"roc_auc\", n_jobs=-1).fit(Xtr, ytr).best_estimator_\n",
    "    rbf_tuned = GridSearchCV(\n",
    "        rbfSVC, {\"clf__C\":[0.5,2,8], \"clf__gamma\": gamma_grid_hotel},\n",
    "        cv=StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=RANDOM_STATE),\n",
    "        scoring=\"roc_auc\", n_jobs=-1\n",
    "    ).fit(Xtr, ytr).best_estimator_\n",
    "    nn_tuned  = GridSearchCV(nn_sgd,   {\"clf__alpha\":[1e-4,5e-4,1e-3]},\n",
    "                             cv=StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=RANDOM_STATE),\n",
    "                             scoring=\"roc_auc\", n_jobs=-1).fit(Xtr, ytr).best_estimator_\n",
    "\n",
    "    # DT stats & importances\n",
    "    dt_inner = dt_tuned.named_steps['clf']\n",
    "    print(f\"[DT] depth={dt_inner.get_depth()} leaves={dt_inner.get_n_leaves()} nodes={dt_inner.tree_.node_count}\")\n",
    "\n",
    "    def feature_names_from_pre(prep, X_sample):\n",
    "        trans = prep.named_steps['pre']\n",
    "        names = []\n",
    "        for name, trans_obj, cols in trans.transformers_:\n",
    "            if name == 'num':\n",
    "                names += list(cols)\n",
    "            elif name == 'ohe':\n",
    "                ohe = trans_obj\n",
    "                try:\n",
    "                    names += list(ohe.get_feature_names_out(cols))\n",
    "                except Exception:\n",
    "                    # fallback: join with underscores\n",
    "                    names += [f\"{c}_{i}\" for c in cols for i in range(len(np.unique(X_sample[c])))]\n",
    "            else:\n",
    "                names += list(cols)  # target/freq enc: one col per src col\n",
    "        return names\n",
    "\n",
    "    fn = feature_names_from_pre(dt_tuned.named_steps['prep'], Xtr)\n",
    "    imp = pd.Series(dt_inner.feature_importances_, index=fn).sort_values(ascending=False).head(10)\n",
    "    plt.figure(figsize=(6.8,4.6))\n",
    "    imp[::-1].plot(kind='barh'); plt.title(\"Hotel — DT Top-10 Gini Importances\"); plt.grid(alpha=.3, axis='x')\n",
    "    savefig(\"Hotel\",\"DT_Top10_Importances\")\n",
    "\n",
    "    # Calibrate LinearSVC\n",
    "    lin_cal = CalibratedClassifierCV(lin_tuned, method=\"sigmoid\", cv=3).fit(Xtr, ytr)\n",
    "    models = {\n",
    "        \"DT\": dt_tuned, \"kNN\": knn_tuned,\n",
    "        \"Linear SVM (calibrated)\": lin_cal, \"RBF SVM\": rbf_tuned, \"NN (SGD)\": nn_tuned\n",
    "    }\n",
    "\n",
    "    # Threshold tuning on internal val\n",
    "    Xs, Xv, ys, yv = train_test_split(Xtr, ytr, test_size=0.2, random_state=RANDOM_STATE, stratify=ytr)\n",
    "    tuned_thr = {}\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xv)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xv)\n",
    "        thr, _ = f1_opt_threshold(yv, s); tuned_thr[name] = thr\n",
    "\n",
    "    # ROC/PR\n",
    "    plt.figure(figsize=(6.6,5)); ax = plt.gca()\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        RocCurveDisplay.from_predictions(yte, s, name=name, ax=ax)\n",
    "    ax.set_title(\"Hotel — ROC curves\"); ax.grid(alpha=.3); savefig(\"Hotel\",\"ROC_All\")\n",
    "\n",
    "    plt.figure(figsize=(6.6,5)); ax = plt.gca()\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        PrecisionRecallDisplay.from_predictions(yte, s, name=name, ax=ax)\n",
    "    ax.set_title(\"Hotel — PR curves\"); ax.grid(alpha=.3); savefig(\"Hotel\",\"PR_All\")\n",
    "\n",
    "    # Calibration (top-2 by PR-AUC)\n",
    "    scored = []\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        scored.append((name, pr_auc(yte, s), s))\n",
    "    top2 = sorted(scored, key=lambda x: x[1], reverse=True)[:2]\n",
    "    plt.figure(figsize=(6.2,4.6)); ax = plt.gca()\n",
    "    for name, _, s in top2:\n",
    "        CalibrationDisplay.from_predictions(yte, s, name=name, ax=ax)\n",
    "    ax.set_title(\"Hotel — Calibration (Reliability)\"); ax.grid(alpha=.3)\n",
    "    savefig(\"Hotel\",\"Calibration_Top2\")\n",
    "\n",
    "    # Confusion matrix at tuned threshold for best model\n",
    "    best_name, _, best_scores = top2[0]\n",
    "    thr = tuned_thr[best_name]\n",
    "    yhat = (best_scores >= thr).astype(int)\n",
    "    cm = confusion_matrix(yte, yhat)\n",
    "    plt.figure(figsize=(4.8,4.4))\n",
    "    plt.imshow(cm, cmap='Blues'); plt.title(f\"Hotel — Confusion Matrix @{thr:.3f}\\n{best_name}\")\n",
    "    for (i,j),v in np.ndenumerate(cm): plt.text(j, i, int(v), ha='center', va='center')\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); savefig(\"Hotel\",\"CM_best\")\n",
    "\n",
    "    # SVM diagnostics\n",
    "    try:\n",
    "        margins = lin_tuned.decision_function(Xte)\n",
    "    except Exception:\n",
    "        margins = models[\"Linear SVM (calibrated)\"].decision_function(Xte)\n",
    "    plt.figure(figsize=(6.2,4.2))\n",
    "    plt.hist(margins, bins=40, alpha=0.85)\n",
    "    plt.title(\"Hotel — Linear SVM Margin Distribution\"); plt.xlabel(\"margin\"); plt.ylabel(\"count\"); plt.grid(alpha=.3)\n",
    "    savefig(\"Hotel\",\"LinearSVM_Margin_Hist\")\n",
    "\n",
    "    sv = rbf_tuned.named_steps['clf'].n_support_.sum()\n",
    "    frac = sv / len(ytr)\n",
    "    print(f\"[RBF-SVM] support vectors: {sv} ({frac:.3f} of train)\")\n",
    "\n",
    "    # Final test metrics (add ACC)\n",
    "    prev = float(np.mean(yte))\n",
    "    print(f\"\\n[Hotel Test] Prevalence (PR-AUC baseline) = {prev:.4f}\")\n",
    "    for name, m in models.items():\n",
    "        s = m.predict_proba(Xte)[:,1] if hasattr(m,\"predict_proba\") else m.decision_function(Xte)\n",
    "        roc = roc_auc_score(yte, s); pr = pr_auc(yte, s)\n",
    "        thr = tuned_thr[name]\n",
    "        ybin = (s >= thr).astype(int)\n",
    "        f1 = f1_score(yte, ybin)\n",
    "        acc = accuracy_score(yte, ybin)\n",
    "        print(f\"{name:24s} ROC-AUC={roc:.4f} PR-AUC={pr:.4f} F1@{thr:.3f}={f1:.4f} ACC@{thr:.3f}={acc:.4f}\")\n",
    "\n",
    "    # --- Hotel: NN epoch curve (SGD; early-stopping-style loop) ---\n",
    "    Xtr_es, Xval_es, ytr_es, yval_es = train_test_split(Xtr, ytr, test_size=0.2, random_state=RANDOM_STATE, stratify=ytr)\n",
    "    nn_es = MLPClassifier(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                          batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                          alpha=1e-4, warm_start=True, max_iter=1, shuffle=True,\n",
    "                          momentum=0.0, nesterovs_momentum=False, random_state=RANDOM_STATE)\n",
    "    pipe_es = Pipeline([('prep', pre), ('post', StandardScaler(with_mean=False)), ('clf', nn_es)])\n",
    "    best_pr = -1.0; best_epoch = 0; patience = 3; bad = 0; pr_hist=[]\n",
    "    for epoch in range(15):\n",
    "        pipe_es.fit(Xtr_es, ytr_es)\n",
    "        s = pipe_es.predict_proba(Xval_es)[:,1]\n",
    "        prv = pr_auc(yval_es, s)\n",
    "        pr_hist.append(prv)\n",
    "        if prv > best_pr + 1e-6:\n",
    "            best_pr = prv; best_epoch = epoch; bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience: break\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.plot(range(1, len(pr_hist)+1), pr_hist, 'o-'); plt.axvline(best_epoch+1, ls='--')\n",
    "    plt.title(f\"Hotel — NN Epoch Curve (best@{best_epoch+1}, PR-AUC={best_pr:.3f})\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"PR-AUC\"); plt.grid(alpha=.3)\n",
    "    savefig(\"Hotel\", \"NN_Epoch_Curve\")\n",
    "\n",
    "    # NN width-sweep (within cap)\n",
    "    nn_width_sweep(\"Hotel\", Xtr, ytr, pre, task=\"cls\", widths=(128,256,512), cv=cv_cls)\n",
    "\n",
    "    # Runtimes\n",
    "    runtimes = []\n",
    "    for name, m in models.items():\n",
    "        tfit, tpred, peak, _ = time_fit_predict(m, Xtr, ytr, Xte, yte, fit_fn=\"fit\", predict_fn=\"predict\")\n",
    "        runtimes.append([\"Hotel\", name, len(Xtr), len(Xte), tfit, tpred, peak])\n",
    "\n",
    "    return models, (Xtr, Xte, ytr, yte), runtimes, pre, shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_note()\n",
    "hotel_models, hotel_data, hotel_runtime, hotel_pre, hotel_nn_width = run_hotel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ACCIDENTS — Regression\n",
    "# =========================\n",
    "def run_accidents(path=\"US_Accidents_March23.csv\"):\n",
    "    print(\"\\n== US ACCIDENTS (REGRESSION: Duration minutes) ==\")\n",
    "    X, y = load_accidents(path)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    pre = accidents_preprocessor(Xtr); pre.fit(Xtr, ytr)\n",
    "    ntr = len(Xtr); nte = len(Xte)\n",
    "    print(f\"[Accidents] cleaned rows={ntr+nte} → train={ntr} test={nte}\")\n",
    "\n",
    "    # Size rules\n",
    "    n_linear = min(ntr, 1_500_000)  # target ≥1M if available\n",
    "    n_rbf   = min(ntr,   100_000)\n",
    "    n_knn   = min(ntr,   250_000); n_knn_test = min(nte, 25_000)\n",
    "\n",
    "    sel = np.random.RandomState(RANDOM_STATE).permutation(ntr)\n",
    "    Xtr_linear, ytr_linear = Xtr.iloc[sel[:n_linear]], ytr.iloc[sel[:n_linear]]\n",
    "    Xtr_rbf,    ytr_rbf    = Xtr.iloc[sel[:n_rbf]],    ytr.iloc[sel[:n_rbf]]\n",
    "    Xtr_knn,    ytr_knn    = Xtr.iloc[sel[:n_knn]],    ytr.iloc[sel[:n_knn]]\n",
    "    Xte_knn,    yte_knn    = Xte.iloc[:n_knn_test],    yte.iloc[:n_knn_test]\n",
    "\n",
    "    post = ('post', StandardScaler(with_mean=False))\n",
    "    dt   = Pipeline([('prep', pre), ('reg', DecisionTreeRegressor(random_state=RANDOM_STATE))])\n",
    "    knn  = Pipeline([('prep', pre), post, ('reg', KNeighborsRegressor(n_neighbors=11, algorithm=\"brute\", metric=\"euclidean\", n_jobs=-1))])\n",
    "    lsvr = Pipeline([('prep', pre), post, ('reg', LinearSVR(C=1.0, random_state=RANDOM_STATE, max_iter=20000))])\n",
    "    rsvr = Pipeline([('prep', pre), post, ('reg', SVR(kernel='rbf'))])\n",
    "\n",
    "    cv_reg = ShuffleSplit(n_splits=3, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Gamma grid for RBF-SVR\n",
    "    d_acc = pre.transform(Xtr[:5]).shape[1]\n",
    "    gamma_grid_acc = [\"scale\", 1.0/d_acc, 2.0/d_acc]\n",
    "\n",
    "    # Validation curves\n",
    "    plot_validation(Pipeline([('prep', pre), ('reg', DecisionTreeRegressor(random_state=RANDOM_STATE))]),\n",
    "                    Xtr_linear, ytr_linear, \"reg__max_depth\", [6,10,14,18], \"neg_mean_absolute_error\", \"DT vs depth\", cv=cv_reg, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', KNeighborsRegressor(algorithm=\"brute\", metric=\"euclidean\"))]),\n",
    "                    Xtr_knn, ytr_knn, \"reg__n_neighbors\", [3,5,11,21], \"neg_mean_absolute_error\", \"kNN vs k\", cv=cv_reg, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', LinearSVR(max_iter=20000, random_state=RANDOM_STATE))]),\n",
    "                    Xtr_linear, ytr_linear, \"reg__C\", np.logspace(-2,2,5), \"neg_mean_absolute_error\", \"LinearSVR vs C\", cv=cv_reg, prefix=\"Accidents\")\n",
    "    # Note: we vary C on the curve; γ choices will be used in GS.\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', SVR(kernel='rbf'))]),\n",
    "                    Xtr_rbf, ytr_rbf, \"reg__C\", [1,10], \"neg_mean_absolute_error\", \"RBF SVR vs C (γ∈{scale,1/d,2/d})\", cv=cv_reg, prefix=\"Accidents\")\n",
    "    plot_validation(Pipeline([('prep', pre), post, ('reg', MLPRegressor(solver='sgd', momentum=0.0, batch_size=1024, max_iter=15, random_state=RANDOM_STATE))]),\n",
    "                    Xtr_linear, ytr_linear, \"reg__alpha\", [1e-4,5e-4,1e-3], \"neg_mean_absolute_error\", \"NN vs L2(alpha)\", cv=cv_reg, prefix=\"Accidents\")\n",
    "\n",
    "    # Learning curves\n",
    "    def LC(est, title, Xlc, ylc): plot_learning(est, title, Xlc, ylc, \"neg_mean_absolute_error\", cv=cv_reg, prefix=\"Accidents\")\n",
    "    LC(dt,   \"Decision Tree\",      Xtr_linear, ytr_linear)\n",
    "    LC(knn,  \"kNN\",                Xtr_knn,    ytr_knn)\n",
    "    LC(lsvr, \"Linear SVR\",         Xtr_linear, ytr_linear)\n",
    "    LC(rsvr, \"RBF SVR\",            Xtr_rbf,    ytr_rbf)\n",
    "    # NN sizing\n",
    "    input_dim = pre.transform(Xtr[:5]).shape[1]\n",
    "    shallow = choose_widths(input_dim, 1, shallow=True)\n",
    "    nn  = Pipeline([('prep', pre), post,\n",
    "                    ('reg', MLPRegressor(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                                         batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                                         alpha=1e-4, early_stopping=True, n_iter_no_change=3,\n",
    "                                         max_iter=15, shuffle=True, momentum=0.0,\n",
    "                                         random_state=RANDOM_STATE))])\n",
    "    LC(nn,   \"Neural Net (SGD)\",   Xtr_linear, ytr_linear)\n",
    "\n",
    "    # Tuning\n",
    "    dt_grid  = {\"reg__max_depth\":[6,10,14,18],\n",
    "                \"reg__min_samples_leaf\":[100,200],\n",
    "                \"reg__min_samples_split\":[200,400],\n",
    "                \"reg__max_features\":[\"sqrt\",\"log2\",0.5],\n",
    "                \"reg__ccp_alpha\":[0.0,1e-4,5e-4,1e-3]}\n",
    "    dt_g  = GridSearchCV(dt,  dt_grid, cv=cv_reg, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_linear, ytr_linear).best_estimator_\n",
    "    knn_g = GridSearchCV(knn, {\"reg__n_neighbors\":[5,11,21]}, cv=cv_reg, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_knn, ytr_knn).best_estimator_\n",
    "    lsvr_g= GridSearchCV(lsvr,{\"reg__C\":[0.1,1,10]}, cv=cv_reg, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_linear, ytr_linear).best_estimator_\n",
    "    rsvr_g= GridSearchCV(\n",
    "        rsvr, {\"reg__C\":[1,10], \"reg__gamma\": gamma_grid_acc},\n",
    "        cv=cv_reg, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
    "    ).fit(Xtr_rbf, ytr_rbf).best_estimator_\n",
    "    nn_g  = GridSearchCV(nn,  {\"reg__alpha\":[1e-4,5e-4,1e-3]}, cv=cv_reg, scoring=\"neg_mean_absolute_error\", n_jobs=-1).fit(Xtr_linear, ytr_linear).best_estimator_\n",
    "\n",
    "    models = {\"DT\": dt_g, \"kNN\": knn_g, \"Linear SVR\": lsvr_g, \"RBF SVR\": rsvr_g, \"NN (SGD)\": nn_g}\n",
    "\n",
    "    # Test metrics & residuals\n",
    "    print(\"\\n[Accidents Test] MAE / MedAE / RMSE / predict-time (s)\")\n",
    "    runtimes = []\n",
    "    for name, m in models.items():\n",
    "        Xtr_fit = Xtr_linear if name!=\"RBF SVR\" else Xtr_rbf\n",
    "        ytr_fit = ytr_linear if name!=\"RBF SVR\" else ytr_rbf\n",
    "        Xte_eval= Xte if name!=\"kNN\" else Xte_knn\n",
    "        yte_eval= yte if name!=\"kNN\" else yte_knn\n",
    "        tfit, tpred, peak, pred = time_fit_predict(m, Xtr_fit, ytr_fit, Xte_eval, yte_eval)\n",
    "        mae = mean_absolute_error(yte_eval, pred)\n",
    "        med = median_absolute_error(yte_eval, pred)\n",
    "        rmse= math.sqrt(mean_squared_error(yte_eval, pred))\n",
    "        print(f\"{name:12s} MAE={mae:.2f} MedAE={med:.2f} RMSE={rmse:.2f}  fit={tfit:.1f}s pred={tpred:.2f}s RAM~{peak:.0f}MB\")\n",
    "        runtimes.append([\"Accidents\", name, len(Xtr_fit), len(Xte_eval), tfit, tpred, peak])\n",
    "\n",
    "    # Residuals for best MAE\n",
    "    maes = {name: mean_absolute_error(yte if name!=\"kNN\" else yte_knn,\n",
    "                                      models[name].predict(Xte if name!=\"kNN\" else Xte_knn))\n",
    "            for name in models}\n",
    "    best = min(maes, key=maes.get)\n",
    "    ypred = models[best].predict(Xte)\n",
    "    resid = (yte - ypred)\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.scatter(ypred, resid, s=6, alpha=.35)\n",
    "    plt.axhline(0, ls='--'); plt.xlabel(\"Predicted duration (min)\"); plt.ylabel(\"Residual (true - pred)\")\n",
    "    plt.title(f\"Accidents — Residuals vs Prediction ({best})\")\n",
    "    savefig(\"Accidents\",\"Residuals_vs_Pred\")\n",
    "\n",
    "    # NN width-sweep (within cap)\n",
    "    nn_width_sweep(\"Accidents\", Xtr_linear, ytr_linear, pre, task=\"reg\", widths=(128,256,512), cv=cv_reg)\n",
    "\n",
    "    # NN epoch curve (early-stopping marker)\n",
    "    Xtr_es, Xval_es, ytr_es, yval_es = train_test_split(Xtr_linear, ytr_linear, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    nn_es = MLPRegressor(hidden_layer_sizes=tuple(shallow), solver='sgd',\n",
    "                         batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                         alpha=1e-4, warm_start=True, max_iter=1, shuffle=True,\n",
    "                         momentum=0.0, random_state=RANDOM_STATE)\n",
    "    pipe_es = Pipeline([('prep', pre), ('post', StandardScaler(with_mean=False)), ('reg', nn_es)])\n",
    "    best_mae = np.inf; best_epoch = 0; patience = 3; bad = 0; hist=[]\n",
    "    for epoch in range(15):\n",
    "        pipe_es.fit(Xtr_es, ytr_es)\n",
    "        pred = pipe_es.predict(Xval_es)\n",
    "        mae = mean_absolute_error(yval_es, pred)\n",
    "        hist.append(mae)\n",
    "        if mae < best_mae - 1e-6:\n",
    "            best_mae = mae; best_epoch = epoch; bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "        if bad >= patience: break\n",
    "    plt.figure(figsize=(6.2,4.6))\n",
    "    plt.plot(range(1, len(hist)+1), hist, 'o-'); plt.axvline(best_epoch+1, ls='--')\n",
    "    plt.title(f\"Accidents — NN Epoch Curve (best@{best_epoch+1}, MAE={best_mae:.2f})\"); plt.xlabel(\"epoch\"); plt.ylabel(\"MAE\"); plt.grid(alpha=.3)\n",
    "    savefig(\"Accidents\", \"NN_Epoch_Curve\")\n",
    "\n",
    "    # DT stats & importances\n",
    "    dt_inner = dt_g.named_steps['reg']\n",
    "    print(f\"[DT-Reg] depth={dt_inner.get_depth()} leaves={dt_inner.get_n_leaves()} nodes={dt_inner.tree_.node_count}\")\n",
    "    # names for Accidents\n",
    "    trans = dt_g.named_steps['prep'].named_steps['pre']\n",
    "    names = []\n",
    "    for nm, tr, cols in trans.transformers_:\n",
    "        if nm == 'num':\n",
    "            names += list(cols)\n",
    "        elif nm == 'ohe':\n",
    "            ohe = tr\n",
    "            try:\n",
    "                names += list(ohe.get_feature_names_out(cols))\n",
    "            except Exception:\n",
    "                names += [f\"{c}_{i}\" for c in cols for i in range(2)]\n",
    "        else:\n",
    "            names += list(cols)\n",
    "    imp = pd.Series(dt_inner.feature_importances_, index=names).sort_values(ascending=False).head(10)\n",
    "    plt.figure(figsize=(6.8,4.6))\n",
    "    imp[::-1].plot(kind='barh'); plt.title(\"Accidents — DT Top-10 Importances\"); plt.grid(alpha=.3, axis='x')\n",
    "    savefig(\"Accidents\",\"DT_Top10_Importances\")\n",
    "\n",
    "    return models, (Xtr, Xte, ytr, yte), runtimes, pre\n",
    "\n",
    "acc_models, acc_data, acc_runtime, acc_pre = run_accidents(\"US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"dataset\",\"model\",\"n_train\",\"n_test\",\"fit_sec\",\"pred_sec\",\"peak_ram_mb\"]\n",
    "rt = pd.DataFrame(hotel_runtime + acc_runtime, columns=cols)\n",
    "rt.to_csv(os.path.join(LOG_DIR, \"runtime_table.csv\"), index=False)\n",
    "print(f\"[saved] {os.path.join(LOG_DIR,'runtime_table.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15801ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Extra Credit: Activation study (Hotel)\n",
    "# =========================\n",
    "def activation_study_hotel(pre_fitted, Xtr, Xte, ytr, yte, width_tuple):\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "    except Exception as e:\n",
    "        print(f\"[ExtraCredit] PyTorch not available ({e}); skipping GELU/SiLU. Running ReLU/tanh fallback.\")\n",
    "        results = []\n",
    "        for act in [\"relu\", \"tanh\"]:\n",
    "            clf = MLPClassifier(hidden_layer_sizes=width_tuple, solver='sgd', momentum=0.0, nesterovs_momentum=False,\n",
    "                                batch_size=1024, learning_rate='constant', learning_rate_init=0.05,\n",
    "                                alpha=1e-4, max_iter=15, early_stopping=True, n_iter_no_change=3,\n",
    "                                random_state=RANDOM_STATE, activation=act)\n",
    "            pipe = Pipeline([('prep', pre_fitted), ('post', StandardScaler(with_mean=False)), ('clf', clf)])\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            s = pipe.predict_proba(Xte)[:,1]\n",
    "            roc = roc_auc_score(yte, s); pr = pr_auc(yte, s)\n",
    "            results.append([act, roc, pr])\n",
    "        pd.DataFrame(results, columns=[\"activation\",\"ROC\",\"PR\"]).to_csv(os.path.join(LOG_DIR,\"extra_credit_activation_summary.csv\"), index=False)\n",
    "        return\n",
    "\n",
    "    torch.manual_seed(RANDOM_STATE); np.random.seed(RANDOM_STATE)\n",
    "    Xtr_t = pre_fitted.transform(Xtr).astype(np.float32)\n",
    "    Xte_t = pre_fitted.transform(Xte).astype(np.float32)\n",
    "    d = Xtr_t.shape[1]\n",
    "\n",
    "    def make_loader(X, y, bs=1024, shuffle=True):\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y.values.astype(np.int64)))\n",
    "        return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "    train_loader = make_loader(Xtr_t, ytr)\n",
    "    val_loader   = make_loader(Xte_t, yte, shuffle=False)\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, act_name):\n",
    "            super().__init__()\n",
    "            acts = {\"relu\": nn.ReLU(), \"gelu\": nn.GELU(), \"silu\": nn.SiLU(), \"tanh\": nn.Tanh()}\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(d, width_tuple[0]), acts[act_name],\n",
    "                nn.Linear(width_tuple[0], width_tuple[1]), acts[act_name],\n",
    "                nn.Linear(width_tuple[1], 1)\n",
    "            )\n",
    "        def forward(self, x): return self.net(x)\n",
    "\n",
    "    activations = [\"relu\", \"gelu\", \"silu\", \"tanh\"]\n",
    "    results = []\n",
    "    bce = torch.nn.BCEWithLogitsLoss()\n",
    "    for act in activations:\n",
    "        model = MLP(act)\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.0, weight_decay=1e-4)\n",
    "        best_val = -1; best_epoch = 0; patience=3; bad=0\n",
    "        pr_hist=[]; roc_hist=[]\n",
    "        for epoch in range(15):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                opt.zero_grad(); logits = model(xb).squeeze(1)\n",
    "                loss = bce(logits, yb.float()); loss.backward(); opt.step()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = []\n",
    "                for xb, _ in val_loader:\n",
    "                    logits.append(model(xb).squeeze(1))\n",
    "                logits = torch.cat(logits).detach().numpy()\n",
    "                roc = roc_auc_score(yte, logits); pr = pr_auc(yte, logits)\n",
    "                roc_hist.append(roc); pr_hist.append(pr)\n",
    "            if pr > best_val: best_val = pr; best_epoch = epoch; bad=0\n",
    "            else: bad += 1\n",
    "            if bad >= patience: break\n",
    "        plt.figure(figsize=(6.2,4.6))\n",
    "        plt.plot(range(1, len(pr_hist)+1), pr_hist, 'o-'); plt.grid(alpha=.3)\n",
    "        plt.title(f\"Hotel — NN Activation ({act.upper()}) PR-AUC vs epoch (best@{best_epoch+1})\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"PR-AUC\"); savefig(\"Hotel\", f\"NN_Activation_{act.upper()}_EpochCurve\")\n",
    "        results.append([act, pr_hist[-1], roc_hist[-1], best_epoch+1])\n",
    "    pd.DataFrame(results, columns=[\"activation\",\"PR_AUC\",\"ROC_AUC\",\"best_epoch\"]).to_csv(\n",
    "        os.path.join(LOG_DIR,\"extra_credit_activation_summary.csv\"), index=False)\n",
    "    print(\"[ExtraCredit] Wrote sl_outputs/logs/extra_credit_activation_summary.csv\")\n",
    "\n",
    "# Extra credit (optional; will skip if torch not installed)\n",
    "activation_study_hotel(hotel_pre, hotel_data[0], hotel_data[1], hotel_data[2], hotel_data[3], tuple(hotel_nn_width))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
